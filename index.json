[{"content":"In the fierce competition of Web3 infrastructure, an increasing number of projects claim to be building \u0026ldquo;the TCP/IP of Web3.\u0026rdquo; Among these, Zero-Knowledge (ZK) proof technology has gained significant attention due to its powerful verification capabilities, with many Layer 0 projects positioning ZK proofs as their core competitive advantage. However, we need to think deeply: Are ZK proofs truly the essence of Layer 0?\nLet us approach this question from a more fundamental perspective.\nReimagining the Nature of Layer 0 When we talk about Layer 0, what are we actually discussing? It is not an execution engine, nor is it merely a consensus mechanism. The true value of Layer 0 lies in trust abstraction — providing a dependable trust foundation for upper-layer applications while encapsulating complex cross-chain operations, state synchronization, and verification mechanisms within the protocol layer.\nMuch like the Internet\u0026rsquo;s TCP/IP protocol stack, Layer 0 needs to solve a core problem: how to establish a reliable communication and state transfer mechanism in a network environment filled with uncertainty. The key concepts here are \u0026ldquo;abstraction\u0026rdquo; and \u0026ldquo;trust,\u0026rdquo; rather than specific technical implementations.\nThe Limitations of ZK Proofs ZK proofs indeed possess powerful verification capabilities, enabling the proof of computational correctness without revealing information. However, when we consider them as the sole technical path for Layer 0, we encounter several practical challenges.\nFirst is the computational cost. ZK proof protocols are computationally intensive, requiring substantial processing power and time, which may be impractical for real-time, high-throughput applications. Every state transition requires generating ZK proofs, creating serious performance bottlenecks in scenarios involving high-frequency trading or numerous state updates. While mitigation strategies include efficient implementation techniques, optimized cryptographic algorithms, and advances in hardware acceleration, the computationally intensive nature of ZK proofs means they cannot serve as a universal solution for all scenarios.\nSecond is the flexibility constraint. Pure ZK systems often require predefined rules and circuits for state transitions, which poses a serious limitation for Layer 0 systems that need rapid iteration and adaptation to different application scenarios.\nMost importantly, ZK proofs solve verification problems, not trust problems. Even if we can perfectly prove the correctness of every state transition, we still need to address more fundamental questions: how to enable different chains and applications to trust each other and collaborate effectively.\nUnderstanding the Limitations of TCP/IP and the Complexity of Web3 When we discuss learning design philosophy from TCP/IP, we need to recognize an important reality: the traditional TCP/IP protocol stack was born in the 1970s, primarily solving connectivity problems — how to enable different computer networks to communicate reliably with each other. TCP/IP\u0026rsquo;s design assumptions were based on a relatively trustworthy environment where network nodes were fundamentally trustworthy or at least had clear administrative authority.\nHowever, Web3 faces entirely different challenges. Web3 must not only solve connectivity problems but also address trust issues in a completely decentralized environment without authoritative institutions. This is like being in a room full of strangers where you need not only to communicate but also to verify identities, securely exchange goods, collaborate on tasks, and maintain fair payment mechanisms.\nIf TCP/IP solved the problem of \u0026ldquo;how to build roads,\u0026rdquo; then Web3 needs to solve the problem of \u0026ldquo;how to establish a complete socio-economic system without government.\u0026rdquo; This system requires coordination across multiple dimensions including identity authentication, property rights, contract execution, and dispute resolution.\nThe Multi-dimensional Challenges of Web3 Protocol Stack Let us examine the complexity of Web3 infrastructure from a more comprehensive perspective:\nModule Objective Existing Projects Maturity Level Connectivity Gossip networks, P2P, secure routing libp2p, waku, quic ✅ Relatively mature Decentralized Identity No reliance on CA, no reliance on PKI ENS, DID, Ethereum Name Service ⚠️ Early stage Decentralized Storage Content addressing, censorship resistance, verifiability IPFS, Arweave, Ceramic ⚠️ Semi-mature, high cost Decentralized Computing Verifiable execution, off-chain computation, ZK verification RISC Zero, zkWASM, Aleo ⚠️ Very early stage Decentralized Payments State transitions + settlement + zkPay Lightning, StarkNet, Aztec ✅ Initial framework Proof Protocol Layer (ZK) Composable, recursive, security layer Plonky2, Halo2, Nova ✅ Mature but heavy This table reveals that genuine Web3 infrastructure faces multi-dimensional challenges. The connectivity layer is already relatively mature because it most closely resembles the problems traditional TCP/IP aimed to solve. The decentralized identity layer remains in early stages, reflecting the fundamental challenge of establishing trustworthy identity systems without central authority. The decentralized storage layer is semi-mature but costly, indicating that technical feasibility has been proven but economic models require optimization. The decentralized computing layer is very early stage because verifiable computation involves complex cryptographic proofs. The proof protocol layer is mature but heavy, which perfectly validates our point about ZK proofs — the technology itself is relatively mature, but computational costs remain high.\nThe Hybrid Model of Trust Abstraction Based on our understanding of Web3\u0026rsquo;s complexity, a genuine Layer 0 should adopt a hybrid trust model, providing different trust guarantee mechanisms according to various application scenarios and security requirements. This design approach can draw inspiration from LayerZero V2\u0026rsquo;s architecture.\nLayerZero\u0026rsquo;s architecture is modular at the verification level while remaining static at the transport level. This design achieves a crucial balance between current performance and future-oriented design. Any entity capable of verifying cross-chain data packets can join LayerZero as a Decentralized Verification Network (DVN), thereby avoiding vendor lock-in at the security level.\nThe core principle of this design philosophy is: different messages and state transitions can choose different verification mechanisms. For high-value cross-chain asset transfers, ZK proofs can provide the strongest security guarantees. For low-risk data synchronization or state queries, optimistic verification or multi-signature mechanisms can be used to reduce costs. For applications requiring rapid response, instant confirmation mechanisms can be provided with asynchronous verification running in the background.\nIt is worth noting that practical solutions are being explored to address ZK proof computational costs. For example, Polyhedra\u0026rsquo;s zkLightClient directly addresses LayerZero V2\u0026rsquo;s challenge through proof batching. Polyhedra\u0026rsquo;s zkLightClient significantly reduces on-chain verification costs and latency by compressing multiple transaction verifications into a single ZKP. This batching technique demonstrates how to maintain ZK proof security while solving performance bottlenecks through engineering optimization.\nConcrete Implementation of the Hybrid Model Based on this approach, a genuine Layer 0 system should comprise several layers:\nCore State Layer: For account balances, critical state transitions, and other core data, ZK proofs ensure absolute security. This data has relatively low update frequency but requires the highest security standards.\nExtended Interaction Layer: For inter-application message passing, governance voting, data publishing, and other operations, configurable verification mechanisms are provided. Applications can choose ZK proofs, optimistic verification, or hybrid modes based on their specific needs.\nRouting Coordination Layer: Responsible for coordination between different verification mechanisms, ensuring system-wide consistency and reliability. This layer\u0026rsquo;s design resembles the routing layer in network protocol stacks, handling load balancing and fault recovery.\nDesign Proposal for Hybrid Model When considering a hybrid model, we can reference real hybrid chain design experiences and adopt the following structure:\nZK Core + Optimistic Side Path Architecture: This architecture\u0026rsquo;s core concept is maintaining the strongest security guarantees while providing optimization paths for high-frequency interaction scenarios. The default path enters the state engine through signature verification, account nonce checking, and snapshot mechanisms, enabling rapid response to most transaction requests. Simultaneously, the system asynchronously generates ZK proofs, marking them as \u0026ldquo;strong security state blocks\u0026rdquo; to ensure absolute safety of critical states.\nMore importantly, this architecture provides a flexible query interface through the query_proof(hash) mechanism, allowing users or other chains to verify the correctness of any state transition on demand. This \u0026ldquo;proof on demand\u0026rdquo; design philosophy solves a crucial problem: not all state transitions require real-time ZK proofs, but all state transitions should be verifiable.\nThis design\u0026rsquo;s advantage lies in finding the optimal balance between performance and security. If Layer 0 is positioned as \u0026ldquo;the final trust anchor for consensus, state, and transfers,\u0026rdquo; then more ZK proofs are indeed better because they provide the strongest security guarantees. However, if Layer 0 needs to support high-frequency interactions and low-latency experiences, then hybrid strategies become crucial. By introducing the \u0026ldquo;proof on demand + optimistic path\u0026rdquo; combination mechanism, the system can maintain the highest security standards while meeting real-world application performance requirements.\nUnderstanding Trust Abstraction Evolution Through LayerZero In LayerZero V2, oracles and relayers have been replaced by Decentralized Verification Networks (DVNs) and permissionless executors. DVNs are responsible for verifying message accuracy before message delivery and correct execution on target chains.\nThis evolution embodies an important design philosophy: separating verification from execution. LayerZero addresses the resource-intensive nature of developing and updating external security code by separating verification from execution. Any code not critical to security is moved to separate components — executors — which can run permissionlessly and remain isolated from verification logic.\nThis separation not only improves system flexibility but, more importantly, provides applications with choice. LayerZero endpoints provide stable application-facing interfaces, lossless network channel abstractions, exactly-once delivery guarantees, and manage OApp security stacks. Endpoint immutability ensures long-term channel validity through enforced update isolation, configuration ownership, and channel integrity.\nTrade-offs in Practical Applications Let us return to practical application scenarios to understand the value of this design. Suppose we are building a cross-chain DeFi application:\nFor large-value asset transfers, we need the highest level of security guarantees, making ZK proofs the optimal choice. Even with high computational costs, this is acceptable relative to asset value.\nFor price information synchronization, we need speed and timeliness, allowing us to use optimistic verification mechanisms and initiate ZK proof processes only when disputes arise.\nFor governance voting, we need transparency and tamper-resistance, allowing us to use commit-reveal mechanisms combined with delayed ZK verification.\nFor user behavior data, we need privacy protection and batch processing, allowing us to use recursive ZK proofs to amortize computational costs.\nDesign Philosophy of Modular Protocol Ecosystem The success of traditional TCP/IP protocol stacks lies in providing a layered, modular, and scalable architecture where each layer has clearly defined responsibility boundaries, upper layers need not concern themselves with lower-layer implementations, and lower layers provide abstract service interfaces for upper layers.\nHowever, Web3\u0026rsquo;s complexity far exceeds traditional internet requirements. Web3\u0026rsquo;s trust abstraction encompasses not only transaction verification but also identity authentication, data storage, computational execution, payment settlement, and multiple other dimensions. Each dimension has its own maturity level and challenges requiring coordinated development. Therefore, Web3\u0026rsquo;s Layer 0 needs to adopt modular composition thinking, not merely layered abstraction.\nAs we can see from our protocol stack analysis, the true Web3 TCP/IP is not just \u0026ldquo;one protocol\u0026rdquo; but rather a modular, composable protocol ecosystem. Different application scenarios require different trust guarantees, storage methods, computational models, and payment mechanisms.\nSimilarly, Web3\u0026rsquo;s Layer 0 should follow this design philosophy. LayerZero enables developers to create and configure unified applications, tokens, and data primitives — regardless of chain — through arbitrary message transmission, much like TCP/IP standardized internet communication.\nThe key to this standardization lies not in forcing the use of specific verification mechanisms but in providing unified interfaces and abstractions. Developers can focus on business logic while delegating complex cross-chain communication, state synchronization, and security verification to the protocol layer.\nTechnical Implementation of Hybrid Model From a technical implementation perspective, a hybrid model Layer 0 system needs to address several key issues:\nState Management: How to maintain state consistency across different verification mechanisms? This requires designing a unified state mechanism capable of handling different types of state updates and verification results.\nRouting Mechanism: How to automatically select appropriate verification paths based on message types and security requirements? This requires implementing intelligent routing algorithms that can balance security, cost, and performance.\nDispute Resolution: How to arbitrate and resolve conflicts when different verification mechanisms produce contradictory results? This requires designing layered dispute resolution mechanisms that ultimately fall back to the most secure verification methods.\nPerformance Optimization: How to maximize system throughput and response speed while ensuring security? This requires implementing various optimization techniques such as batching, parallel verification, and precomputation.\nConclusion: The Future of Trust Abstraction ZK proofs are an important technology providing Web3 with powerful verification capabilities. However, treating them as the sole technical path for Layer 0 represents limited thinking. A genuine Layer 0 should be a trust abstraction layercapable of providing the most appropriate trust guarantee mechanisms according to different application needs and scenarios.\nThis hybrid model design can not only fully leverage ZK proof advantages but also provide greater flexibility and scalability for the entire Web3 ecosystem. Just as TCP/IP protocol stacks achieved internet prosperity through layered and modular design, Web3\u0026rsquo;s Layer 0 should also establish a solid foundation for next-generation decentralized applications through trust abstraction and hybrid models.\nIn this process, we need to shift from technical implementation perspectives to protocol design perspectives, from single solutions to ecosystem construction, and from technology-first to user experience-first approaches. Only in this way can we truly build the \u0026ldquo;TCP/IP\u0026rdquo; of the Web3 world.\nComprehensively speaking, building genuine Web3 infrastructure requires us to grasp several key points:\nRecognize that Web3 complexity far exceeds traditional internet requirements: Traditional TCP/IP only solved connectivity problems, while Web3 needs to solve multi-dimensional trust problems including identity, storage, computation, and payments in a completely decentralized environment.\nZK proofs are not the essence of Layer 0; trust abstraction is: True value lies in providing the most appropriate trust guarantee mechanisms according to different application needs, rather than forcing the use of a single technical path.\nBuild a modular, composable protocol ecosystem: Web3\u0026rsquo;s Layer 0 should be a protocol ecosystem capable of flexibly combining different trust mechanisms, storage solutions, computational models, and payment methods.\nLearn from LayerZero V2\u0026rsquo;s practical experience: Through decentralized verification networks and verification-execution separation, a good balance has been achieved between modular security and protocol stability.\nOnly by deeply understanding these complexities can we truly build infrastructure suitable for the Web3 era, establishing a solid foundation for the prosperity of decentralized applications.\n","permalink":"https://sidereushu.github.io/posts/zk-proofs-are-not-the-essence-of-layer-0-trust-abstraction-is/","summary":"\u003cp\u003eIn the fierce competition of Web3 infrastructure, an increasing number of projects claim to be building \u0026ldquo;the TCP/IP of Web3.\u0026rdquo; Among these, Zero-Knowledge (ZK) proof technology has gained significant attention due to its powerful verification capabilities, with many Layer 0 projects positioning ZK proofs as their core competitive advantage. However, we need to think deeply: Are ZK proofs truly the essence of Layer 0?\u003c/p\u003e\n\u003cp\u003eLet us approach this question from a more fundamental perspective.\u003c/p\u003e","title":"ZK Proofs Are Not the Essence of Layer 0, Trust Abstraction Is"},{"content":" \u0026ldquo;The internet was designed to be open, but the platforms built on top of it are not.\u0026rdquo;\n——Chris Dixon, Rebooting the Internet\n0. From Open Web1 to Centralized Web2: The Legacy of Missing Trust The Web1 era began with openness. Born out of academic and military collaboration, the TCP/IP protocol stack laid the foundation for global connectivity. TCP/IP was — and remains — an open and permissionless stack: any device following the protocol can join the network. This property of permissionless connectivity created the early decentralized flavor of the Internet.\nBut as the Internet commercialized, new demands emerged: identity, content, value transfer. We soon realized that TCP/IP solved the problem of connectivity, but not the problem of trust.\nFor example:\nHow do users authenticate their identities? Who owns or controls the data? How can cross-site coordination be trusted? How do we prevent double-spending or denial in financial interactions? Because trust was never embedded into the network stack, applications were forced to implement it themselves. This gap was filled by centralized platforms. Thus, Web2 emerged — an architecture where platforms manage identities, content, and transaction integrity, becoming central custodians of user data and trust.\n1. TCP/IP: The Backbone of the Open Internet TCP/IP is the foundational communication protocol suite of the Internet. It is organized as a layered architecture:\nTCP/IP Layer OSI Equivalent Key Protocols Role Application Application HTTP, DNS High-level application logic Transport Transport TCP, UDP End-to-end transmission Network Network IP, ICMP Routing and addressing Link Data/Physical Ethernet, Wi-Fi Physical transmission The stack’s key advantage is abstraction and decoupling: each layer only depends on the layer below. This modularity enabled the Internet’s rapid expansion.\nHowever, the TCP/IP philosophy is based on best-effort delivery:\nThe lower layers do not guarantee delivery; higher layers are responsible for reliability.\nFor example, the IP layer may drop or reorder packets; TCP adds retransmission and reordering to give the illusion of reliability. In Web2, this is acceptable because platforms have centralized control. But Web3 does not have that luxury.\n2. Web3 Demands More Than “Best Effort” — It Demands Cryptographic Guarantees Web3 aims to build not an Internet of information, but an Internet of value. That means communication isn\u0026rsquo;t just about moving bytes, but about moving state, value, commitments — all in a verifiable, tamper-resistant, trust-minimized way.\nWhy TCP/IP Falls Short for Web3 Core Web3 Requirement Can TCP/IP Provide It? Verifiable state transitions ❌ Stateless Decentralized identity ❌ No identity layer Cross-chain communication ❌ No context awareness Zero-knowledge proof transport ❌ No crypto semantics Permissionless composability ❌ No execution model Censorship-resistant P2P messaging ❌ No built-in resistance In short, TCP/IP may be the physical substrate, but it is not Web3-native. Today, much of Web3 still relies on centralized infrastructure for critical operations:\nWallets interact with chains via centralized RPC providers (e.g., Infura) IPFS content often gets routed through centralized gateways Smart contract calls are initiated by centralized frontends Web3 today is still a Web3 emulator running on Web2 protocols.\n3. A Comparative Stack: Web2 vs Web3 Protocol Layers To reframe Web3’s architecture, we can draw an analogy with TCP/IP:\nInformation Internet Protocol Layer Value Internet (Web3) Protocol Layer IP addresses IP (Network Layer) Blockchain accounts Layer 1 state machines Packet transport TCP / UDP (Transport) Rollups / Channels Layer 2 state extensions Website access HTTP / DNS (App Layer) DApps, NFTs, DAOs Application protocols (ERC-20/721) What’s missing from Web3 is not just more dApps — it’s a native backbone layer, a \u0026ldquo;TCP/IP-equivalent\u0026rdquo; for trust, identity, and value. We need a Layer 0 purpose-built for Web3.\n4. Why Didn’t TCP/IP Include This? When TCP/IP was developed in the 1970s, it was designed to solve the problem of reliable connectivity, not trust. Many technologies needed for decentralized trust simply didn’t exist yet:\nCapability Missing in TCP/IP Era Web2 Replacement Decentralized identity No public-key addresses / DID Email + password (OAuth) Decentralized storage No IPFS / Filecoin AWS / GCP Decentralized compute No zkVM / decentralized VMs Serverless / Cloud Decentralized payments No cryptocurrencies / smart contracts Credit card / PayPal So although TCP/IP was open and permissionless, the application layer had no choice but to rely on centralized platforms to provide closed-loop services. The result was Web2 — an ecosystem where:\nUsers generate data Platforms own data Algorithms control users This model has been called data feudalism — users become digital serfs working the land owned by platforms.\n5. Web3 Is a Return to TCP/IP’s Spirit — Not a Rejection Web3 is not trying to overthrow the Internet — it’s trying to complete what TCP/IP never finished: embedding trust into the stack.\nWe can envision a new decentralized protocol stack:\nLayer Web2 Implementation Web3 Equivalent Communication TCP/IP TCP/IP (shared) Structure JSON / SQL Merkle Trees / SMTs Identity OAuth (Google, Facebook) Public-key addresses / DIDs Storage AWS / Cloud IPFS / Arweave / Filecoin Compute Cloud Functions EVM / zkVM / WASM + ZK Consensus Centralized databases Blockchain (PoW / PoS / BFT) Application Web2 platforms DApps / DAOs / DeFi / NFTs Web3’s mission is to use cryptography + distributed systems to build native, verifiable protocols for trust and computation.\n6. So What Should Web3’s TCP/IP Look Like? We need a Web3-native Layer 0 stack that transmits not just packets, but:\nVerifiable state snapshots Censorship-resistant messages Zero-knowledge proofs and public-key identities Deployable, composable contracts Cross-chain state interoperability Projects like Celestia, Eigenlayer, and IBC are exploring this territory — building the \u0026ldquo;TCP/IP for the Internet of Value\u0026rdquo;.\n7. Web3’s “New TCP/IP” Isn’t a Rebellion — It’s a Foundation Web1 gave us protocol freedom\nWeb2 gave us platform convenience\nWeb3 seeks to reclaim the protocol layer — without relying on the platform\nWeb3 doesn’t aim to replace TCP/IP — it aims to augment it with a native, trust-aware state layer.\nThis new \u0026ldquo;connectivity\u0026rdquo; layer won’t just route data packets — it will route state, trust, value, and cryptographic commitments. We are no longer content with “best-effort”. What we demand is proof-driven delivery.\nAnd that is the foundational infrastructure Web3 is still missing.\n","permalink":"https://sidereushu.github.io/posts/web3s-missing-foundation--why-it-needs-a-new-tcp-ip/","summary":"\u003cblockquote\u003e\n\u003cp\u003e\u003cstrong\u003e\u0026ldquo;The internet was designed to be open, but the platforms built on top of it are not.\u0026rdquo;\u003c/strong\u003e\u003cbr\u003e\n——Chris Dixon, \u003cem\u003eRebooting the Internet\u003c/em\u003e\u003c/p\u003e\u003c/blockquote\u003e\n\u003chr\u003e\n\u003ch3 id=\"0-from-open-web1-to-centralized-web2-the-legacy-of-missing-trust\"\u003e0. From Open Web1 to Centralized Web2: The Legacy of Missing Trust\u003c/h3\u003e\n\u003cp\u003eThe Web1 era began with openness. Born out of academic and military collaboration, the TCP/IP protocol stack laid the foundation for global connectivity. TCP/IP was — and remains — an \u003cstrong\u003eopen and permissionless stack\u003c/strong\u003e: any device following the protocol can join the network. This property of \u003cem\u003epermissionless connectivity\u003c/em\u003e created the early decentralized flavor of the Internet.\u003c/p\u003e","title":"Web3’s Missing Foundation - Why It Needs a New TCP/IP"},{"content":"\nTimeline for Post-Quantum Migration According to analysis by Chaincode Labs, Bitcoin’s transition to post-quantum cryptography (PQC) can follow two main strategies: a short-term contingency plan (cf. Figure 1) and a long-term comprehensive path (cf. Figure 2).\nThe short-term strategy focuses on deploying a basic quantum-resistant option within 1 to 2 years, offering a fallback mechanism in case cryptographically relevant quantum computers (CRQCs) emerge sooner than expected. This involves proposing a minimal PQC signature scheme through a BIP, implementing it in Bitcoin Core, and enabling voluntary migration of vulnerable UTXOs. While not optimized for all use cases, it provides immediate protection for at-risk users and critical institutions. Success depends on close coordination across the technical community and early involvement from major Bitcoin holders.\nThe long-term strategy envisions a broader research and development process to design Bitcoin’s optimal quantum-resistant future. This includes evaluating different PQC algorithms, mitigating their performance trade-offs, and developing robust UTXO migration mechanisms. Drawing from historical upgrades like SegWit and Taproot, which took several years to reach wide adoption, this path could span up to a decade.\nChaincode emphasizes that these two strategies are not mutually exclusive. The short-term plan offers immediate protection and preparedness, while the long-term plan focuses on sustainability, optimization, and ecosystem-wide adoption in the face of evolving quantum threats.\nFigure 1. Timeline of estimate to establish short-term contingency measures.\nFigure 2. Timeline for comprehensive quantum resistance path.\nGovernment Post-Quantum Plans and Timelines Governments worldwide are actively responding to the potential massive disruption posed by quantum computers to existing cryptographic systems by formulating strategies for migration to post-quantum cryptography (PQC).\nAs of mid-2025, more than 15 countries and regions have issued official guidance on PQC transition. Most follow the NIST (U.S. National Institute of Standards and Technology) standardization process, while some develop indigenous algorithms independently. The general timeline for transition completion spans 2027 to 2035, with 2030 considered a critical milestone.\nUnited States The U.S. has established the most comprehensive plan, including:\nNational Security Memorandum 10 (May 2022): Signed by President Biden, titled “Advancing America’s Leadership in Quantum Computing While Mitigating Risks to Vulnerable Cryptographic Systems”.\nThe goal is to mitigate quantum risks as much as possible by 2035.\nNIST Transition Deadlines:\nPhasing out RSA-2048 and ECC-256 by 2030, with complete prohibition by 2035, while allowing hybrid classical-PQC schemes during the transition.\nNSA’s CNSA 2.0 Suite:\nRequires software and network equipment upgrades by 2030, with browsers and operating systems completing upgrades by 2033.\nPresidential Executive Order 14144 (January 2025):\n“Strengthening National Cybersecurity Innovation” emphasizes collaboration with other nations and industry to encourage adoption of NIST’s PQC standards.\nUnited Kingdom The UK’s National Cyber Security Centre (NCSC), part of GCHQ intelligence agency, published guidance in March 2025 outlining a three-phase transition plan:\nBy 2028: Identify cryptographic services needing upgrades and develop migration plans.\n2028–2031: Prioritize upgrading critical systems and adapt according to PQC progress.\n2031–2035: Complete full migration of systems and products to PQC.\nThe UK plan explicitly bases on NIST standards and aligns with IETF protocol standardization efforts.\nEuropean Union The EU drives its PQC roadmap through the European Telecommunications Standards Institute (ETSI) Quantum-Safe Cryptography (QSC) working group, established in 2015:\nPublished quantum-safe migration guidelines [ETS20] and key establishment standards [ETS25];\nProvides technical reports endorsing NIST standards as well.\nChina China pursues a more independent and autonomous PQC development path.\nIn February 2025, the Commercial Cryptography Institute launched the “Next Generation Commercial Cryptography Algorithms Program” (NGCC) 1, developing domestic PQC algorithms aligned with national security requirements.\nChina emphasizes technological self-reliance and has not publicly released a detailed transition timeline.\nIndustry Progress in Post-Quantum Cryptography (PQC) Several well-known companies relying on cryptography in their products or services have already started migrating to post-quantum cryptography (PQC) solutions.\nCloudflare Date: October 2022\nCloudflare announced that all websites and APIs served through Cloudflare support a post-quantum hybrid key exchange mechanism.\nTechnical Details: This mechanism combines classical elliptic curve cryptography (ECC) with the post-quantum Kyber algorithm to form a hybrid scheme.\nSecurity Advantage: Attackers must break both ECC and Kyber algorithms simultaneously to compromise keys, significantly enhancing security.\nGoogle Date: August 2023\nGoogle announced that its Chrome browser started supporting a post-quantum hybrid key generation mechanism similar to Cloudflare’s.\nMotivation and Background: Google emphasized the urgency of updating TLS (Transport Layer Security) to use quantum-resistant session keys to prevent “Harvest Now, Decrypt Later” attacks, i.e.,\nAttackers collect encrypted data now and decrypt it later once universal quantum computers become available.\nSignal Date: September 2023\nSignal, a leading privacy-focused encrypted messaging app, announced adding post-quantum security layers to its communication encryption.\nImplementation: Introduced CRYSTALS-Kyber post-quantum key encapsulation mechanism (KEM) atop its original ECC-based encryption to enhance security.\nApple Date: February 2024\nApple announced a new encryption protocol PQ3 designed for iMessage.\nPQ3 Features:\nUses PQC for initial key agreement and ongoing message exchange;\nSupports automatic recovery mechanisms, enabling rapid restoration of security even if a key is compromised, thereby enhancing overall resilience.\nIndustry Trend Summary With PQC algorithms continuously standardized and maturing, an increasing number of companies will deploy PQC mechanisms, marking a shift from “experimental defense” to “mainstream security requirement.”\nWhy Choose a “Hybrid Encryption” Strategy? No single PQC algorithm is yet proven 100% secure;\nHybrid schemes allow a gradual transition without interrupting existing security guarantees;\nThey prevent a “single point of failure” — even if one algorithm is broken in the future, the other may still provide protection.\nWhy Kyber? Kyber is the first post-quantum key encapsulation algorithm (KEM) standardized by NIST;\nBased on lattice cryptography, currently regarded as highly resilient to quantum attacks;\nHas relatively small key and ciphertext sizes, making it suitable for performance-sensitive applications such as TLS and mobile communications.\nDrivers for Industry Action Clear government timelines (e.g., NIST’s 2030/2035 plans) create regulatory pressure;\nQuantum threats may not be immediate, but data harvesting attacks are already ongoing, requiring early preparation to protect future privacy;\nCompanies gain a first-mover advantage in security and can strengthen market trust.\nPhilosophical Debate If a universal, breakable quantum computer eventually becomes reality, the Bitcoin community will face a difficult decision:\nShould it take action to handle currently exposed public-key UTXOs that are vulnerable to quantum attacks?\nThis question fundamentally touches Bitcoin’s core values, such as property rights, censorship resistance, forward compatibility, and conservatism.\nThe “Burn” Position Core Arguments By “burning” quantum-vulnerable UTXOs, these coins become unusable, thereby protecting property rights and network integrity.\nBitcoin developer Jameson Lopp2 , in his article [“Against Allowing Quantum Recovery of Bitcoin” (Lop25a)] and BDML talks, argues:\nAllowing quantum computers to seize these bitcoins results in a wealth transfer—from those who lost keysto winners of the quantum arms race.\nThe “burn” strategy treats quantum vulnerability as a protocol-level bug that should be conservatively fixed, like previous vulnerabilities.\nEconomic Effects “Burning” reduces total Bitcoin supply, thereby increasing the value of remaining coins.\nCoordinated “burn” events can reduce market uncertainty and volatility.\nThe “Steal” Position Core Arguments Opponents of “burning” argue:\nIt violates user property rights and amounts to confiscation.\nBitcoin’s design intent is complete user sovereignty over their assets, allowing spending at any time.\nSome users may be unaware of quantum risks or unable to migrate assets promptly.\nMoral and Technical Dilemmas Protocol changes that render some UTXOs permanently unspendable introduce third-party control, violating Bitcoin’s decentralization principles.\nWithout intervention, the “steal” path becomes the default option—once quantum computers mature, early attackers could seize all vulnerable assets, causing serious wealth redistribution.\nMigration Pathways Overview Migrating Bitcoin to quantum-resistant signatures is one of the most historically significant undertakings in the Bitcoin ecosystem. Even if a quantum-safe signature scheme gains technical consensus and integrates into Bitcoin Core, migrating millions of UTXOs to quantum-safe scripts requires unprecedented network coordination. This section discusses practical migration pathways, on-chain resource requirements, activation methods, user education, and coordination mechanisms3.\nQuantum-resistant migration is not only a technical challenge but also a network-wide collaboration and governance issue.\nIt involves block space, soft forks, user key management, activation mechanisms, and more.\nAll Bitcoin holders need basic awareness of migration pathways to develop effective risk mitigation strategies against quantum threats.\nUTXO Migration As of May 2025, Bitcoin’s UTXO set contains approximately 190 million UTXOs. Only a portion exposes public keys and is thus vulnerable to long-range attacks. However, all UTXOs are potentially susceptible to short-range attacksduring spending and transaction confirmation windows, as described earlier in “On-Spend Vulnerable Script Types.”\nIdeally, all UTXOs should migrate to quantum-resistant scripts by spending old UTXOs in a transaction and creating new UTXOs secured by post-quantum signatures. In practice, some UTXOs are inaccessible (e.g., lost keys), creating the “philosophical dilemma: burn or steal.”\n2024 research estimates:\nTheoretical fastest migration time: Using 100% block space for migration transactions, migrating all UTXOs would take approximately 76–142 days (model-dependent).\nRealistic scenario: Using only 25% block space, it would take 305–568 days.\nIn reality, only an urgent migration of exposed public-key UTXOs is necessary.\nKey points:\nUTXO migration is resource-intensive and costly.\nWithout on-chain scaling, existing block size and block interval severely limit migration throughput.\nSignature aggregation and batching transactions are potential optimizations.\nMigration Mechanisms Several proposed mechanisms balance user participation, consensus rule changes, and technical complexity.\nCommit-Delay-Reveal Protocol (CDR) and Variants Background:\nInitially proposed in 2018, with variants by Adam Back, Tim Ruffing, and others.\nDesigned to securely migrate old UTXOs to post-quantum scripts even after ECC compromise.\nThree-phase process:\nCommit phase:\nUser creates a commitment transaction hashing the old ECC and new post-quantum public keys.\nThe hash is committed on-chain via OP_RETURN.\nTransaction valid under current consensus rules.\nSoft fork needed to enforce that the UTXO can only be spent by knowledge of both private keys.\nDelay phase:\nIntroduces a mandatory delay period (e.g., 6 months) to prevent reorg attacks by ensuring chain finality.\nFunds are locked and unusable during this period.\nReveal phase:\nUser submits a reveal transaction disclosing the original ECC and new post-quantum keys.\nThe transaction is signed with the post-quantum signature.\nVerifies that the reveal matches the commit hash.\nAdvantages:\nHigh security under CRQC presence; allows safe migration of unexposed UTXOs post-ECC compromise.\nSupports migration after quantum attacks occur.\nDisadvantages:\nCannot migrate UTXOs with already exposed public keys (e.g., P2PK).\nRequires user participation in two transactions.\nRequires users to hold post-quantum coins for fees, creating a bootstrapping problem.\nAt least one, more likely two soft forks needed (one for PQ signatures, one for CDR semantics).\nBlock space constraints remain.\nImproved Scheme: Lifted FawkesCoin Protocol Improvements:\nResolves the “user must already hold PQ coins” problem in original CDR.\nEmploys zero-knowledge proofs (ZKPs) using the PICNIC PQ signature to prove knowledge of a private key without revealing the public key.\nCircumvents the paradox of needing to reveal a public key to commit.\nAdditional capabilities:\nHD wallet users can prove derivation relationships using seeds.\nPartial private key loss is recoverable via master seed.\nCan protect the majority of Bitcoin accumulated since HD wallets became standard.\nCosts and challenges:\nRequires integrating ZKPs into Bitcoin protocol, a more disruptive change than single PQ signature adoption.\nPICNIC and related ZK systems are complex and not yet widely deployed.\nCommentary \u0026amp; Opinion Bitcoin’s quantum-resistant migration is a gradual but urgent process:\nComprehensiveness vs Reality:\nIdeally migrate all UTXOs.\nPractically prioritize active, spendable, high-risk UTXOs.\nInaccessible funds (lost keys) remain a “philosophical problem.”\nTechnical vs Social Coordination:\nTechnical solutions (CDR, ZKP lifting) are early-stage and require extensive validation.\nReal challenges lie in activation, education, consensus coordination, and incentive design.\nFuture research directions:\nDesign quantum-resistant coin creation mechanisms to solve bootstrapping.\nSimplify CDR workflows (e.g., automated wallet support).\nExplore soft-fork-free or incremental transition mechanisms.\nAnalyze Grover’s algorithm effectiveness on various Bitcoin script structures to inform prioritization.\nReferences Institute of Commercial Cryptography Standards, China. Announcement on Launching the Next-generation Commercial Cryptographic Algorithms Program (NGCC). February 5, 2025.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nJ. Lopp. Against Allowing Quantum Recovery of Bitcoin. Cypherpunk Cogitations. March 16, 2025.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nChaincode Labs. Bitcoin and Quantum Computing: Current Status and Future Directions.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://sidereushu.github.io/posts/post-quantum-readiness-in-blockchain---threats-roadmaps-and-migration-strategy-iii/","summary":"\u003cbr\u003e\n\u003ch2 id=\"timeline-for-post-quantum-migration\"\u003eTimeline for Post-Quantum Migration\u003c/h2\u003e\n\u003cp\u003eAccording to analysis by Chaincode Labs, Bitcoin’s transition to post-quantum cryptography (PQC) can follow two main strategies: a \u003cstrong\u003eshort-term contingency plan\u003c/strong\u003e (cf. Figure 1) and a \u003cstrong\u003elong-term comprehensive path\u003c/strong\u003e (cf. Figure 2).\u003c/p\u003e\n\u003cp\u003eThe \u003cstrong\u003eshort-term strategy\u003c/strong\u003e focuses on deploying a basic quantum-resistant option within \u003cstrong\u003e1 to 2 years\u003c/strong\u003e, offering a fallback mechanism in case cryptographically relevant quantum computers (CRQCs) emerge sooner than expected. This involves proposing a minimal PQC signature scheme through a BIP, implementing it in Bitcoin Core, and enabling voluntary migration of vulnerable UTXOs. While not optimized for all use cases, it provides immediate protection for at-risk users and critical institutions. Success depends on close coordination across the technical community and early involvement from major Bitcoin holders.\u003c/p\u003e","title":"Post-Quantum Readiness in Blockchain: Threats, Roadmaps, and Migration Strategy III"},{"content":"\nPost-Quantum Cryptography (PQC) Post-Quantum Cryptography has become a critical solution to counter the threat posed by scalable, controllable quantum computers to current cryptographic systems.\nUrgency of PQC: Originates from Peter Shor’s 1995 algorithm, which can factor integers and compute discrete logarithms in polynomial time, effectively breaking mainstream schemes like RSA, DH, and ECC.\nPQC is not a single algorithm, but a set of parallel technical approaches, including:\nLattice-based cryptography: The most promising category with well-established theoretical foundations;\nHash-based signatures: Such as SPHINCS+, offering strong security but large signature sizes;\nCode-based cryptography: Examples include BIKE and McEliece, characterized by very large keys;\nIsogeny-based cryptography: Such as SIKE, featuring compact structures but recently subject to severe cryptanalytic attacks;\nMultivariate polynomial cryptography: Like Rainbow, which is vulnerable to algebraic attacks.\nThese schemes differ greatly in practical terms, especially regarding:\nKey and signature sizes;\nSigning and verification efficiency;\nMaturity of design and security evaluation;\nSensitivity to block size and verification efficiency in applications like Bitcoin. PQC Algorithm Analysis in the Bitcoin Context To evaluate the suitability of PQC algorithms for Bitcoin, we compare their public key/signature sizes and computational efficiency against the current ECDSA and Schnorr signature schemes[^ChainCodeReport].\nBased on data aggregated from BIP-360 1and the PQ Signatures Zoo, the summary is as follows:\nHash-based signatures (e.g., SPHINCS+): Very small public keys but the largest signatures;\nLattice-based signatures (e.g., FALCON and CRYSTALS-Dilithium): Achieve a good balance between public key and signature sizes;\nIsogeny-based signatures (e.g., SQIsign): Smallest signatures and relatively small public keys, but slow and not yet widely vetted.\nThe overall trend indicates PQC technologies are still evolving, with room for improvement in signature size, public key size, and signing/verification efficiency.\nTherefore, locking in a specific scheme too early is suboptimal; a more rational strategy is to monitor developments continuously and maintain compatibility in design.\nMetric ECC (Schnorr) SPHINCS+ CRYSTALS-Dilithium FALCON SQIsign Signature Size ~64 bytes ~8–17 KB ~2–3 KB ~0.5–1 KB ~100–300 bytes Public Key Size ~32 bytes ~32 bytes ~1.3 KB ~800 bytes ~200 bytes Signing Time Fast Slow Medium Fast Slow Security Maturity High High High Medium Low Suitable for Bitcoin? ✅ ❌ ⚠️ ✅ ❌ NIST Post-Quantum Standardization Process To address the long-term threat posed by quantum computing to existing cryptographic standards, NIST launched the Post-Quantum Cryptography (PQC) standardization process in December 2016. This initiative builds on foundational research from PQCrypto conferences (since 2006) and multiple projects in the EU, Japan (e.g., SAFEcrypto, CREST) 2.\nThe process is globally open, collecting algorithm submissions, conducting public testing and academic review, with the goal to publish algorithms resistant to both classical and quantum attacks. As of March 2025, NIST has officially released the following standards:\nFIPS 203: Encryption algorithm standards (non-signature);\nFIPS 204: Module Lattice Signature Standard (ML-DSA) based on CRYSTALS-Dilithium;\nFIPS 205: Stateless Hash-Based Signature Standard (SLH-DSA) based on SPHINCS+;\nFIPS 206: NTRU-based Fast Fourier Lattice Signature (FN-DSA) based on FALCON, draft expected in 2025.\nAdditionally, another candidate, the LPN-based HQC key encapsulation algorithm, is expected to complete review by 2026 before draft release NIS25.\nNIST’s Main Signature Schemes (Current Status) FIPS # Name Underlying Algorithm Features Suitable for Bitcoin? FIPS 204 ML-DSA Dilithium Secure, balanced, easy to deploy ✅ FIPS 205 SLH-DSA SPHINCS+ Extremely secure but bulky ❌ FIPS 206 FN-DSA FALCON Compact, efficient but numerically unstable ✅ (requires floating-point stability improvements) Dilithium is currently the most balanced scheme and is widely adopted by Google and Cloudflare;\nFALCON is attractive for Bitcoin due to its compactness and speed, but its floating-point implementation complexity introduces side-channel risks;\nSPHINCS+ is mainly used for ultra-high security scenarios (e.g., backup signatures, critical firmware signing), and is unsuitable for high-frequency signing.\nReferences H. Beast. BIP-360: QuBit - Pay to Quantum Resistant Hash. Bitcoin Improvement Proposal (BIP) Pull Request 1670. September 2024.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nChaincode Labs. Bitcoin and Quantum Computing: Current Status and Future Directions.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://sidereushu.github.io/posts/post-quantum-readiness-in-blockchain---threats-roadmaps-and-migration-strategy-ii/","summary":"\u003cbr\u003e\n\u003ch2 id=\"post-quantum-cryptography-pqc\"\u003ePost-Quantum Cryptography (PQC)\u003c/h2\u003e\n\u003cp\u003ePost-Quantum Cryptography has become a critical solution to counter the threat posed by scalable, controllable quantum computers to current cryptographic systems.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eUrgency of PQC\u003c/strong\u003e: Originates from \u003cstrong\u003ePeter Shor’s 1995 algorithm\u003c/strong\u003e, which can factor integers and compute discrete logarithms in polynomial time, effectively breaking mainstream schemes like RSA, DH, and ECC.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003ePQC is \u003cstrong\u003enot a single algorithm\u003c/strong\u003e, but a set of \u003cstrong\u003eparallel technical approaches\u003c/strong\u003e, including:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eLattice-based cryptography\u003c/strong\u003e: The most promising category with well-established theoretical foundations;\u003c/p\u003e","title":"Post-Quantum Readiness in Blockchain: Threats, Roadmaps, and Migration Strategy II"},{"content":"\nBitcoin\u0026rsquo;s Security and the Threat from Quantum Computing Bitcoin\u0026rsquo;s security relies on a cryptographic assumption that has long been considered unbreakable under current technological conditions. However, the emergence of quantum computers could undermine this assumption within the next decade.\nAt the core of Bitcoin’s cryptographic foundation are:\nthe Elliptic Curve Digital Signature Algorithm (ECDSA);\nand, since 2021, the introduction of Schnorr signatures.\nBoth schemes are based on the Elliptic Curve Discrete Logarithm Problem (ECDLP), which is asymmetric in nature: deriving the public key from a private key is easy, but reversing the process is believed to require trillions of years even on today’s most powerful supercomputers.\nHowever, in the face of cryptographically relevant quantum computers (CRQCs), this asymmetry may collapse — deriving the private key from the public key could take only hours or days.\nInstitutions such as the U.S. National Institute of Standards and Technology (NIST), the U.S. government, and other international agencies have recommended phasing out elliptic curve cryptography by 2030 and fully deprecating it by 2035.\nPotential Impact of Quantum Computers on Bitcoin Once practical quantum computers become available, their impact on Bitcoin could be profound. According to an analysis by 1:\nApproximately 20% to 50% of all circulating Bitcoin (4 to 10 million BTC) could be at risk of theft;\nThis risk primarily stems from the ability to derive private keys from exposed public keys.\nThe most vulnerable assets include:\nUTXOs that use insecure script types;\nFunds associated with “address reuse”, where the public key has already been revealed;\nExchange and institutional addresses that frequently reuse addresses;\nEarly-era addresses from the Satoshi period, such as those using raw public key scripts;\n“Lost coins”, which remain unmoved and are locked under outdated or unsafe scripts.\nAmong these, high-value exchange and institutional accounts are expected to be top targets for quantum-enabled attackers.\nQuantum Threats to Bitcoin Mining and Ecosystem Security On the other hand, while quantum computing may also impact Bitcoin mining, its threat is considered limited in the foreseeable future:\nCurrent mining performance is primarily driven by clock frequency;\nQuantum miners would have to compete with highly optimized ASIC machines;\nEven with a quadratic speedup from algorithms like Grover’s, it would still be difficult to outperform classical miners;\nThe hardware performance gap remains significant.\nIf quantum miners were to dominate in the future, potential risks could include:\nDouble-spending attacks by small or solo miners;\nIncreased centralization due to disproportionate mining power.\nAlthough these scenarios remain distant, exploring post-quantum mining models still holds theoretical value and can help prepare for long-term contingencies.\nBeyond Signatures and Mining: Ecosystem-Level Risks In addition to the direct threats to digital signatures and mining, a broader set of ecosystem-level vulnerabilities must also be considered in the post-quantum era:\nFor example: SSL/TLS communication protocols, hardware wallets, and firmware update mechanisms may all be compromised by quantum-capable adversaries. Two Types of Quantum Attack Vectors (Long-Range Attack vs. Short-Range Attack)\nLong-Range Attack: Targets addresses whose public keys are already exposed on-chain, such as P2PK, P2MS, and P2TR. These addresses are permanently vulnerable, as their public keys are visible at all times.\nShort-Range Attack: Targets addresses that only reveal their public keys when spending, such as P2PKH. In this case, the attacker must act within a very short window — between the time a transaction is broadcast (and the public key is exposed) and when it gets confirmed on the blockchain.\nIn short, if the public key of a UTXO is known, quantum adversaries may derive the private key and steal the funds. The difference lies in how and when the public key is exposed:\nSome addresses are “natively exposed”, meaning their public keys are visible from the beginning, and thus remain under long-term attack risk.\nOthers (e.g., P2PKH) only reveal the public key when the coin is spent, offering a very narrow attack window, also known as a \u0026ldquo;preemptive\u0026rdquo; or \u0026ldquo;racing\u0026rdquo; attack.\nQuantum Vulnerabilities by Script Type The feasibility of quantum attacks depends not only on the existence of CRQCs (Cryptographically Relevant Quantum Computers), but also on whether the corresponding public key is accessible. Therefore, different script types present different levels of vulnerability. Below is a classification by script type:\n🔴 Immediate Vulnerability P2PK (Pay to Public Key): The earliest script type, which directly exposes the ECDSA public key in the locking script. Though it accounts for only ~0.025% of UTXOs, it secures ~8.68% of all BTC (~1.72 million BTC) — largely associated with early Satoshi-era wallets.\nP2MS (Pay to MultiSig): A script type that exposes multiple public keys. Now rarely used and secures a relatively small amount of BTC.\nP2TR (Pay to Taproot): Despite being a modern script, its key-path spending still reveals a tweaked public key, making it vulnerable to quantum attacks. P2TR makes up ~32.5% of UTXOs but secures only ~0.74% of BTC.\n🟡 On-Spend Vulnerable Script Types P2PK, P2MS, and P2TR are classified as “immediately vulnerable” because they expose public keys directly in the output script (scriptPubKey).\nHowever, some script types do not reveal the public key until the coin is spent. These are vulnerable only during the short window between transaction broadcast and confirmation.\n🟠 Address Reuse Vulnerability Script types such as P2PKH, P2SH, P2WPKH, and P2WSH only expose public keys at the time of spending, meaning the key is normally revealed once.\nBut if the same address (i.e., the same public key) is reused, the associated public key remains permanently exposed on-chain, turning a short-range attack surface into a long-range attack vector.\n🟣 Other Avenues for Public Key Exposure 1. Fork Chain Exposure If a user spends an unspent UTXO on a Bitcoin fork chain (e.g., Bitcoin Cash, Bitcoin Gold), the public key becomes exposed on the fork — even though the UTXO remains unspent on the Bitcoin mainnet.\nThus, the corresponding UTXO on the mainnet becomes a long-term quantum target.\n2. Leaked Extended Public Keys (xpubs) xpubs are used in Hierarchical Deterministic (HD) wallets, enabling address generation without revealing private keys.\nIf an xpub is leaked, all non-hardened child public keys derived from it can have their private keys recovered by quantum computers.\n3. Public Key Exposure in Multi-Sig \u0026amp; Lightning Network In multi-signature wallets and Lightning Network channels, participants must share their public keys to construct transactions or payment channels.\nAlthough these public keys are not broadly public, they are shared within limited parties, and thus may be exploited by insiders or malicious actors.\nEcosystem Vulnerabilities Once elliptic curve cryptography (ECC) is broken by quantum computers, the impact will go far beyond Bitcoin — potentially compromising the entire Internet cryptographic infrastructure.\nCore protocols like SSL/TLS could become insecure, leading to ecosystem-level attacks such as man-in-the-middle (MITM) attacks, where adversaries intercept and redirect user requests:\nMITM attacks: By compromising SSL/TLS, attackers can forge connections to exchanges and hijack funds.\nHardware wallet updates: Firmware updates may be tampered with, silently embedding backdoors.\nMining pool impersonation: Attackers may masquerade as miners to infiltrate mining pools.\nDNS attacks: Redirecting users to malicious nodes through DNS manipulation.\nAPI attacks: Tampering with third-party APIs widely used by wallets and exchanges.\nThese threats are stealthier than direct cryptographic breakage, potentially remaining undetected for extended periods due to their ecosystem-level obfuscation.\nBitcoin Hash Functions and Grover’s Algorithm Bitcoin relies on two hash functions: SHA-256 and RIPEMD-160, used for:\nMining (double SHA-256 on block headers)\nGenerating transaction IDs\nSigning messages\nDeriving addresses from public keys (Hash160)\nQuantum Resistance of Hash Functions The fundamental property of hash functions is irreversibility.\nEven the world’s most powerful supercomputers (e.g., El Capitan, as of Nov 2024) would require far more time to break them than to crack ECC. 2\nGrover’s algorithm offers a quadratic speedup for brute-forcing hash functions:\nClassical attack complexity: O(N)O(N)\nQuantum attack complexity: O(N)O(N​)\nFor example, finding a SHA-256 collision classically requires 22562256 operations.\nWith Grover’s algorithm, this is reduced to 21282128, which is still computationally infeasible without extremely powerful quantum computer.\nImpact on Bitcoin Mining The essence of Bitcoin mining is to find a nonce value that makes the block header\u0026rsquo;s hash lower than the network difficulty target.\nThis process involves repeated SHA-256 computations, currently performed almost exclusively by ASIC hardware.\n1. Limitations of Quantum Mining Grover’s algorithm provides sequential, not parallel, speedup, which leads to key limitations:\nIt cannot scale in parallel like classical mining does; adding more quantum hardware does not linearly increasehash power.\nCompeting with ASICs would require a large number of high-speed quantum machines, making deployment extremely costly.\n2. Network Difficulty Adjustment Bitcoin dynamically adjusts its mining difficulty every 2016 blocks to maintain a block interval of ~10 minutes.\nIf quantum mining is introduced, the protocol will increase the difficulty accordingly, which would further offset any advantage from Grover’s algorithm.\nForks and the 51% Attack 1. Quantum-Induced Forking Risk Research shows that quantum mining could increase the frequency of blockchain forks:\nWhen one miner finds a valid block, other quantum miners must decide whether to continue or restart their computation.\nSimultaneous quantum measurements by multiple miners increase the chance that multiple valid blocks are found at the same time, causing forks.\n2. Security Concerns In a high-fork environment:\nHonest miners\u0026rsquo; hash power gets split across multiple chains;\nAdversaries can concentrate their resources on attacking one chain, potentially gaining dominance without owning 51% of total hash power.\nThis reduces finality and consensus stability, and significantly increases the attack surface.\nComprehensive Quantum Vulnerability Assessment of Bitcoin Component Quantum Threat Resistance Level ECC Signatures Critical risk (Shor\u0026rsquo;s algorithm) Very Low Hash Functions Moderate risk (Grover\u0026rsquo;s algorithm) Relatively High Mining Mechanism Efficiency impacted but adjustable Medium Ecosystem Attacks High risk (SSL, DNS, APIs) Depends on external mitigation Fork Security Potentially severe (more frequent forks) Low References Chaincode Labs. Bitcoin and Quantum Computing: Current Status and Future Directions.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nE. Strohmaier, J. Dongarra, H. Simon, H. Meuer. TOP500 List - November 2024 (64th Edition). TOP500.org. November 19, 2024.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://sidereushu.github.io/posts/post-quantum-readiness-in-blockchain---threats-roadmaps-and-migration-strategy-i/","summary":"\u003cbr\u003e\n\u003ch3 id=\"bitcoins-security-and-the-threat-from-quantum-computing\"\u003eBitcoin\u0026rsquo;s Security and the Threat from Quantum Computing\u003c/h3\u003e\n\u003cp\u003eBitcoin\u0026rsquo;s security relies on a \u003cstrong\u003ecryptographic assumption that has long been considered unbreakable under current technological conditions\u003c/strong\u003e. However, the emergence of quantum computers \u003cstrong\u003ecould undermine this assumption within the next decade\u003c/strong\u003e.\u003c/p\u003e\n\u003cp\u003eAt the core of Bitcoin’s cryptographic foundation are:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003ethe \u003cstrong\u003eElliptic Curve Digital Signature Algorithm (ECDSA)\u003c/strong\u003e;\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eand, since 2021, the introduction of \u003cstrong\u003eSchnorr signatures\u003c/strong\u003e.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eBoth schemes are based on the \u003cstrong\u003eElliptic Curve Discrete Logarithm Problem (ECDLP)\u003c/strong\u003e, which is \u003cstrong\u003easymmetric\u003c/strong\u003e in nature: deriving the public key from a private key is easy, but reversing the process is believed to require trillions of years even on today’s most powerful supercomputers.\u003cbr\u003e\nHowever, in the face of \u003cstrong\u003ecryptographically relevant quantum computers (CRQCs)\u003c/strong\u003e, this asymmetry may collapse — \u003cstrong\u003ederiving the private key from the public key could take only hours or days\u003c/strong\u003e.\u003c/p\u003e","title":"Post-Quantum Readiness in Blockchain: Threats, Roadmaps, and Migration Strategy I"},{"content":"\n❓What is Quantum Computing? Quantum computing harnesses the quantum states of microscopic particles—such as photons, electrons, or atoms—to process information. It is fundamentally based on several key principles: superposition, entanglement, coherence, and the no-cloning theorem.\nAs early as 1982, Professor Richard Feynman famously stated:\n\u0026ldquo;Nature isn’t classical, damn it, and if you want to make a simulation of nature, you’d better make it quantum mechanical.\u0026rdquo;\nThis insight laid the conceptual foundation for building computational tools using quantum systems themselves, rather than relying on classical approximations of quantum behavior.\nWhy Classical Computers Are Not Enough Many fundamental systems in nature do not follow classical physics but are governed by quantum mechanics. Therefore, if we want to “simulate nature,” classical computation is not sufficient — we must use quantum systems themselves to build computational tools.\nMore specifically, simulating quantum systems with classical computers faces exponential difficulty. A quantum system composed of $n$ particles requires $2^n$ complex numbers to describe its state, which quickly exceeds the processing capability of classical machines. Furthermore, particles in a quantum system are often not independent — they form complex correlations known as quantum entanglement.\nQuantum computing, on the other hand, leverages the superposition of quantum states to perform parallel computation. For example, a 4-qubit quantum register can represent all 16 values from 0 to 15 simultaneously, and any operation on the register acts on all 16 values at once.\nThis is why Feynman proposed a groundbreaking idea: instead of simulating quantum systems with classical computers, we should use quantum systems themselves to compute. His insight was remarkably accurate — as we now know, there are computations that can only be carried out through quantum experiments and are beyond the reach of traditional computers.\nWhat Does Quantum Computing Mean for Mainstream Cryptosystems Like RSA and ECC? In 1994, Peter Shor proposed a quantum algorithm—Shor\u0026rsquo;s Algorithm—that can break mainstream cryptosystems such as RSA and ECC, drawing widespread attention. The foundation of Shor\u0026rsquo;s Algorithm is its ability to perform fast Fourier transforms (FFT) on state vectors of exponential size, making it easier to identify the period of a function.\nQuantum computing leverages the superposition effect of quantum states, allowing parallel computation across all possible states. For example, Schrödinger’s cat can be simultaneously ‘alive’ and ‘dead’—a superposition of two states. If there are 100 cats, each in a superposition of ‘alive’ and ‘dead’ there are ($2^{100}$) possible states. Each state has its own amplitude, positive or negative, and quantum computing can manipulate this exponentially large vector of states simultaneously.\nOn the surface, this exponential parallelism might seem to allow brute-force solutions to any computationally hard problem. However, the output of a quantum computation is itself a superposition, making it difficult to extract a definitive answer.\nCurrently, only Shor’s Algorithm can use quantum Fourier transforms to efficiently extract the period of a function, thereby solving the integer factorization and discrete logarithm problems upon which modern public-key cryptosystems RSA and ECC are based. For other hard problems that cannot be reduced to period finding, no polynomial-time quantum algorithms are known yet.\nHow Close Are We to Practical Quantum Computers? Yes and no. To break a 1024-bit RSA key, a universal quantum computer with millions of qubits would be needed—far beyond our current manufacturing capabilities (the largest quantum computers today have only a few hundred qubits):\nIn 2017, IBM introduced the world’s first 16-qubit quantum computer. The latest fault-tolerant universal quantum machines, such as Google’s Willow chip (105 qubits by the end of 2024), now achieve error correction threshold capabilities. The Oxford team has also created single-qubit operations with gate error rates as low as 0.000015%. Developing public-key cryptographic algorithms that can resist quantum attacks (post-quantum cryptography) has become an urgent task. To address this, the U.S. National Institute of Standards and Technology (NIST) launched the Post-Quantum Cryptography (PQC) standardization project back in 2016. After years of evaluation, the leading quantum-resistant public-key algorithms—such as CRYSTALS-Kyber, Dilithium, and Falcon—are now essentially standardized and are set to become the new benchmarks for federal and enterprise security infrastructure.\nD-Wave’s Progress The Canadian quantum computing company D-Wave has long provided quantum annealers, which are well-suited for solving optimization problems, rather than problems like period-finding as required by Shor’s algorithm.\nIn 2025, D-Wave released its Advantage2 system, featuring over 4,400+ qubits and a 20-node connectivity topology. It is currently offering cloud-based services to clients such as Mastercard, NTT, and the Jülich Supercomputing Centre.\nMeanwhile, D-Wave’s team published a paper in Science claiming “quantum advantage” for the first time: in simulating complex magnetic systems, their machine outperformed the world’s top classical supercomputers by what they claim would be millions of years.\nAlthough this result has sparked debates over classical simulators (such as MPS and t-VMC), it still suggests that quantum annealing is showing practical value in certain scientific and engineering domains.\nD-Wave has built and sold several quantum computers; however, these machines cannot run Shor’s algorithm and therefore do not pose a direct threat to cryptographic schemes based on factoring or discrete logarithms.\nStrictly speaking, D-Wave builds quantum annealers, which are machines that can only run quantum annealing algorithms. Their fundamental working principle differs from that of universal quantum computers, which rely on quantum superposition to achieve powerful computational capabilities. In contrast, quantum annealers rely on quantum tunneling to simulate the annealing process.\nCurrently, there is research underway that attempts to transform integer factorization into an annealing-compatible problem and solve it using D-Wave machines — but so far, it has only managed to factor integers up to 90 bits.\nEmerging Track: Topological Quantum Computing and Fault-Tolerance February 2025: Microsoft launched its first Majorana-based topological quantum processor, Majorana 1, featuring highly stable topological qubits and aiming for million-qubit scalability.\nMay 2025: Quantinuum announced that its H2 system had achieved a record-breaking quantum volume $2^{23}=8388608$, a key metric for assessing a system’s practical computational power.\nMay 2025: Oxford, Google’s Willow, and other teams continued advancing toward fault-tolerant quantum computing by optimizing single-qubit gate fidelities and improving two-qubit error correction schemes.\n","permalink":"https://sidereushu.github.io/posts/notes---what-is-quantum-computing-implications-for-rsa--ecc/","summary":"\u003cbr\u003e\n\u003ch3 id=\"what-is-quantum-computing\"\u003e❓What is Quantum Computing?\u003c/h3\u003e\n\u003cp\u003eQuantum computing harnesses the quantum states of microscopic particles—such as photons, electrons, or atoms—to process information. It is fundamentally based on several key principles: \u003cstrong\u003esuperposition\u003c/strong\u003e, \u003cstrong\u003eentanglement\u003c/strong\u003e, \u003cstrong\u003ecoherence\u003c/strong\u003e, and the \u003cstrong\u003eno-cloning theorem\u003c/strong\u003e.\u003c/p\u003e\n\u003cp\u003eAs early as \u003cstrong\u003e1982\u003c/strong\u003e, \u003cstrong\u003eProfessor Richard Feynman\u003c/strong\u003e famously stated:\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003e\u003cem\u003e\u0026ldquo;Nature isn’t classical, damn it, and if you want to make a simulation of nature, you’d better make it quantum mechanical.\u0026rdquo;\u003c/em\u003e\u003c/p\u003e\u003c/blockquote\u003e\n\u003cp\u003e\u003cimg alt=\"Feynman\" loading=\"lazy\" src=\"/images/screenshot-Feynman.png\"\u003e\u003c/p\u003e\n\u003cp\u003eThis insight laid the conceptual foundation for building computational tools using quantum systems themselves, rather than relying on classical approximations of quantum behavior.\u003c/p\u003e","title":"Notes | What is Quantum Computing? Implications for RSA \u0026 ECC"},{"content":"Throughout the evolution of human civilization, the pursuit of computational power has never ceased. From $\\underline{mechanical ~computation}$ to $\\underline{electronic ~computation}$, we now stand at the dawn of a new era — $\\mathbf{on ~the ~verge ~of ~quantum ~computing. }$\nThe core breakthrough of quantum computing does not lie in macro-level advancements such as “faster” transistors or “stronger” chips, but rather in a paradigm shift from macro to micro: it no longer relies on the classical binary of 0 and 1 but leverages the peculiar properties of quantum superposition and entanglement at the particle level to build $\\underline{an ~entirely ~new ~computational ~logic}$. This logic cannot be simulated by traditional electronic computers, much like nuclear weapons cannot be replicated with chemical explosives—their energies are simply not on the same scale.\nIn this age of exponentially increasing computational power, modern cryptography stands as the first field to be fundamentally challenged. Encryption algorithms that are rock-solid on classical computers—such as RSA and ECC—become vulnerable in the face of quantum algorithms like Shor’s algorithm.\nJust as electronic computers once marked the end of the mechanical cipher era, quantum computing is now poised to reshape the foundations of modern cryptography. It brings not only challenges, but also an opportunity to redefine what “security” truly means.\n","permalink":"https://sidereushu.github.io/posts/from-macro-to-micro---a-computational-revolution-reshaping-cryptography/","summary":"\u003cp\u003eThroughout the evolution of human civilization, the pursuit of computational power has never ceased. From $\\underline{mechanical ~computation}$ to $\\underline{electronic ~computation}$, we now stand at the dawn of a new era — $\\mathbf{on ~the ~verge ~of ~quantum ~computing. }$\u003c/p\u003e\n\u003cp\u003eThe core breakthrough of quantum computing does not lie in macro-level advancements such as “faster” transistors or “stronger” chips, but rather in \u003cstrong\u003ea paradigm shift from macro to micro\u003c/strong\u003e: it no longer relies on the classical binary of 0 and 1 but leverages the peculiar properties of quantum superposition and entanglement at the particle level to build $\\underline{an ~entirely ~new ~computational ~logic}$. This logic cannot be simulated by traditional electronic computers, much like nuclear weapons cannot be replicated with chemical explosives—their energies are simply not on the same scale.\u003c/p\u003e","title":"From Macro to Micro - A Computational Revolution Reshaping Cryptography"},{"content":" Dr. Sidereus M. Hu A crypto PhD building the future of trust. Research interests include: Post-Quantum Cryptography, Privacy-Preserving Computation, and Provable Security.\nFormer Founder, Crypto \u0026 InfoSec Team (H7US3C)\nHonors: SJTU Top 10 Outstanding Students of the Year Academic Star Award Outstanding Dissertation Award 📧 crypto.sidereus@gmail.com 📧 mxhu@teamtps.org ","permalink":"https://sidereushu.github.io/about/","summary":"\u003cstyle\u003e\n.card {\n  max-width: 960px;\n  margin: 2rem auto;\n  padding: 2rem;\n  display: flex;\n  align-items: flex-start;\n  gap: 2.5rem;\n  background: #f9f9f9;\n  border-radius: 16px;\n  box-shadow: 0 4px 24px rgba(0, 0, 0, 0.08);\n  font-family: -apple-system, BlinkMacSystemFont, \"Helvetica Neue\", \"PingFang SC\", \"Microsoft YaHei\", sans-serif;\n}\n.card img {\n  width: 320px;\n  height: auto;\n  object-fit: cover;\n  border-radius: 12px;\n  box-shadow: 0 4px 12px rgba(0, 0, 0, 0.1);\n  margin: 0;\n  display: block;\n}\n.card-content {\n  flex: 1;\n  margin: 0;\n  padding: 0;\n  display: flex;\n  flex-direction: column;\n  justify-content: space-between; /* 邮箱与图片底部对齐关键 */\n  min-height: 100%; /* 保证内容高度撑满父容器 */\n}\n.card-content-inner {\n  display: flex;\n  flex-direction: column;\n}\n.card-content h1 {\n  font-size: 2rem;\n  margin-top: 0;\n  margin-bottom: 0.4rem;\n  word-break: keep-all;\n  text-align: left;\n  white-space: pre;         /* 空格生效，确保左对齐 */\n  font-weight: bold;\n  padding-left: 0;\n}\n.card-content p {\n  margin: 0.35rem 0;\n  font-size: 1.05rem;\n  color: #333;\n  line-height: 1.75;\n}\n.card-content p.intro {\n  margin-bottom: 0.12rem;\n}\n.card-content .founder {\n  margin-bottom: 1.8rem;   /* 增加与Honors间距，邮箱能到底部 */\n}\n.card-content a {\n  color: #1a73e8;\n  text-decoration: none;\n}\n.honor-list {\n  list-style: disc;\n  padding-left: 1.25em;\n  margin: 0.2em 0 1em 0;\n}\n.honor-list li {\n  margin: 0.12em 0;\n  padding-left: 0;\n  text-indent: 0;\n  line-height: 1.3;\n}\n.email-list {\n  display: flex;\n  flex-direction: column;\n  gap: 0.13em;\n  margin-top: 0.18em;\n}\n.email-list a {\n  font-size: 1.03rem;\n  text-decoration: none;    /* 去掉下划线 */\n}\n\u003c/style\u003e\n\u003cdiv class=\"card\"\u003e\n  \u003cimg src=\"/images/me.jpg\" alt=\"我的头像\"\u003e\n  \u003cdiv class=\"card-content\"\u003e\n    \u003cdiv class=\"card-content-inner\"\u003e\n      \u003ch1\u003eDr. Sidereus M. Hu        \u003c/h1\u003e\n      \u003cp class=\"intro\"\u003eA crypto PhD building the future of trust. Research interests include: Post-Quantum Cryptography, Privacy-Preserving Computation, and Provable Security.\u003c/p\u003e","title":"About Sidereus"},{"content":"","permalink":"https://sidereushu.github.io/posts/images/short-term/","summary":"","title":""}]