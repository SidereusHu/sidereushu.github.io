[{"content":"\nCore Objective Core Purpose: The goal of the attack (by Byzantine validators) is not to break the system (such as double-spending), but rather to manipulate block publication and voting so that their \u0026ldquo;privately held\u0026rdquo; block (b₁) defeats an honest block (b₂), thereby becoming the main chain.\nUltimate Gain: By making the honest b₂ block become \u0026ldquo;orphaned,\u0026rdquo; Byzantine validators can \u0026ldquo;steal\u0026rdquo; the block rewards that should have belonged to the b₂ proposer, thus obtaining \u0026ldquo;higher rewards.\u0026rdquo;\nLegend normal block (white square): A normal, consensus-confirmed block. withheld block (dashed red square): A withheld block. This is the key to the attack—the block has been created and privately voted on, but not broadcast to the network. delayed block (solid red square): A delayed release block. This is the state when a \u0026ldquo;withheld block\u0026rdquo; is finally released to the network. orphaned block (square with diagonal lines): Orphaned block. A valid block that was not selected for the main chain. vote for a block (circle): A vote for a block. delay a block (arrow with circle): Combined action of Byzantine validators privately voting and deciding to withhold (delay) this block. Attack Flow Breakdown Starting Point (Time = T0): b₀ is a normal block (white square). This is the commonly recognized starting main chain.\nByzantine \u0026ldquo;Conspiracy\u0026rdquo; (Time = T1): After b₀, b₁ appears. Note that b₁ is drawn as a withheld block (dashed red square).\nThis means vᵢ proposed b₁, and all Byzantine validators privately voted for it (represented by the red circles vote for a block and the delay a block arrows in the diagram), but they withheld b₁ and did not broadcast it to other honest nodes. Honest Validators\u0026rsquo; \u0026ldquo;Open Play\u0026rdquo; (Time = T2): In the next slot:\nAn honest validator (who is unaware of b₁\u0026rsquo;s existence) proposes block b₂ based on b₀. b₂ is broadcast normally. At this moment, for honest nodes, the main chain is: b₀ → b₂ Attack Execution and Fork Competition (Time = T2 instant): Just as b₂ is proposed, Byzantine validators immediately release their privately held b₁ to the network: b₁\u0026rsquo;s status changes from withheld block (dashed) to delayed block (solid red square).\nResult: At this point, a fork appears in the network: Honest chain: b₀ → b₂ Byzantine chain: b₀ → b₁ Consensus Ruling (Time = T3):\nHonest chain b₀ → b₂ weight = b₀\u0026rsquo;s weight + b₂\u0026rsquo;s weight (possibly only the b₂ proposer and a few nodes that just received it voted). Byzantine chain b₀ → b₁ weight = b₀\u0026rsquo;s weight + b₁\u0026rsquo;s weight (b₁ proposer + all Byzantine validators\u0026rsquo; premeditated votes). Due to the Byzantine validators\u0026rsquo; \u0026ldquo;premeditated\u0026rdquo; votes, once the b₁ chain is released, its weight immediately exceeds the b₂ chain. Sandwich Reorg Attack The \u0026ldquo;Sandwich Reorg Attack\u0026rdquo; is a sophisticated attack against Ethereum\u0026rsquo;s consensus HLMD-GHOST fork choice rule. In Ethereum\u0026rsquo;s consensus (particularly Casper FFG + LMD-GHOST), LMD-GHOST (Latest Message Driven GHOST) is a voting weight-based fork choice rule:\nEach validator votes (attestation) for what they consider the \u0026ldquo;heaviest chain.\u0026rdquo; Each fork\u0026rsquo;s \u0026ldquo;weight\u0026rdquo; = the number of validator votes received (weighted). The chain head is selected as the branch with the highest cumulative weight. \u0026ldquo;Sandwich\u0026rdquo; Attack Concept The attacker inserts their own hidden chain between two honest blocks, causing the network\u0026rsquo;s weight judgment to reverse.\nIn Ethereum, each slot has a proposer. To reduce empty blocks and accelerate finality convergence, the system gives the current slot\u0026rsquo;s proposer a temporary weight boost, known as \u0026ldquo;proposer boosting.\u0026rdquo;\nThis boost is typically about 70% of the total weight. Meaning: The proposer\u0026rsquo;s own block\u0026rsquo;s branch receives temporary weighting to encourage the chain to advance toward the new block. This mechanism itself is beneficial—but attackers can abuse it, leading to reorganization.\nAttack Timeline Time Action Explanation t b₀ This is the common ancestor (starting point) of the chain. t+1 Attacker vᵢ proposes b₁ (red block) But the attacker temporarily does not broadcast b₁ (dashed border). t+2 Honest validator proposes b₂ (gray block) Since the attacker hasn\u0026rsquo;t published b₁, everyone believes b₂ is the legitimate newest chain inheriting from b₀. Thus, most validators vote for b₂, making the b₂ chain have higher weight at this time. t+3 Attacker vᵢ publishes b₁ and has accomplice vⱼ propose b₃ with b₁ as parent The key here: b₃ uses the proposer boosting mechanism, gaining a 70% weight bonus for the current slot. Since this weight exceeds honest validators\u0026rsquo; voting power (less than 70%), the branch b₀→b₁→b₃ is considered the \u0026ldquo;heavier chain\u0026rdquo;. Thus, the original b₂ chain is reorganized (reorg\u0026rsquo;d) out. Balancing Attack Attack Structure b₀ (white square) is the common ancestor/base chain head, existing at time t and accepted by honest nodes as the current head. In two consecutive slots t+1 and t+2, both proposers are Byzantine, vᵢ (at t+1) and vⱼ (at t+2) respectively. vᵢ generates block b₁ (red dashed box—withheld) at t+1, but does not immediately broadcast it. vⱼ at t+2 both publishes their own proposed b₂ and simultaneously releases the previously hidden b₁. Note that both b₁ and b₂ have b₀ as parent (binary fork, forming two parallel chains). Vote Splitting Strategy The attacker divides honest validators into two groups V₁ and V₂, with both groups equal in size (or weight).\nThe attacker prepares two sets of attestations:\na₁: Supporting the chain with b₁ as head (internally collected or disguised as \u0026ldquo;evidence that b₁ is heavier\u0026rdquo;), releasing a₁ only to V₁. a₂: Supporting the chain with b₂ as head, releasing a₂ only to V₂. Result: V₁ members, after seeing a₁, believe b₁ is heavier and only vote for b₁; V₂ members, after seeing a₂, only vote for b₂. Thus, at the end of t+2, the accumulated attestation weight of both chains is equal (or nearly equal), so neither can form more than 2/3 support to finalize (under the classic 2/3 finality setting), nor can either be clearly selected as the long-term main chain by HLMD-GHOST → resulting in the system\u0026rsquo;s long-term inability to achieve finality (liveness is compromised), causing system availability to collapse.\nWhy This Blocks Finality (Intuitive Reason) In voting weight-based fork-choice (HLMD-GHOST), finality depends on continuously appearing consecutive checkpoints that can obtain \u0026gt;2/3 weight support. Through careful \u0026ldquo;information distribution\u0026rdquo; (selective release of attestations), the attacker partitions the information view of honest validators into two groups, causing each fork to receive only about half of the honest votes. Since no branch can gain enough majority votes, no checkpoint can accumulate beyond the finality threshold (e.g., \u0026gt;2/3), so the chain cannot finalize—this is liveness being blocked (not safety being broken, but progress/availability being blocked). Attack Prerequisites To successfully and continuously launch a balancing attack, the attacker needs at least some or all of the following capabilities/conditions:\nControl of two consecutive slot proposers (both consecutive proposers are Byzantine)\nThis is the core assumption in the diagram: the attacker has block production rights in consecutive proposer slots, enabling them to create and delay/parallel publish forked blocks b₁, b₂. Selective information propagation (message partition/targeted gossip) capability:\nThe attacker must be able to control which honest validators see which attestations (or blocks) first, i.e., can send messages only to V₁ or V₂. This can be accomplished through network layer partitioning (actual network division or delay) or through selective forwarding of certain messages. Note: The attacker does not need to control honest validators\u0026rsquo; signing authority; only needs to control information visibility (who-sees-what). Sufficient stake/weight distribution conditions (weakened version):\nTo make both sides\u0026rsquo; weights exactly balanced, the attacker doesn\u0026rsquo;t need to hold a large proportion of stake, but needs to be able to divide honest validators into two groups with roughly equal weight (this can be achieved through social engineering, network delays, directed information propagation, etc.). If honest validator weights are very uneven or difficult to divide equally, the attack cost will be higher. Sustained repeated launch capability:\nTo keep the chain continuously unable to finalize, the attacker must repeat the above operations over multiple epoch/slot cycles (especially when checkpoint spans are large). If it\u0026rsquo;s only a one-time operation, the network may still recover finality subsequently. Summary: The attack heavily relies on network layer manipulation and proposer control. In environments where the network is partially controllable or the attacker can control several proposers, this is a realistic threat.\nSuccess Probability / Cost If proposer election is random and public, and the attacker cannot long-term control consecutive proposers, the attack success probability is low.\nIf the attacker can long-term control several proposers (or occupy a high proportion of proposer positions through Byzantine nodes), or can temporarily partition the network/selectively forward, the success probability significantly increases.\nCosts include: acquiring consecutive proposers (stake cost or election probability), constructing and maintaining network control costs for targeted gossip, bearing possible slashing/penalty risks (if detection and penalty mechanisms exist).\n","permalink":"https://sidereushu.github.io/posts/reorg-attacks-i/","summary":"\u003cbr\u003e\n\u003ch2 id=\"core-objective\"\u003eCore Objective\u003c/h2\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eCore Purpose\u003c/strong\u003e: The goal of the attack (by Byzantine validators) is \u003cstrong\u003enot\u003c/strong\u003e to break the system (such as double-spending), but rather to manipulate block publication and voting so that their \u0026ldquo;privately held\u0026rdquo; block (b₁) defeats an honest block (b₂), thereby becoming the main chain.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eUltimate Gain\u003c/strong\u003e: By making the honest b₂ block become \u0026ldquo;orphaned,\u0026rdquo; Byzantine validators can \u0026ldquo;steal\u0026rdquo; the block rewards that should have belonged to the b₂ proposer, thus obtaining \u0026ldquo;higher rewards.\u0026rdquo;\u003c/p\u003e","title":"Reorg Attacks I"},{"content":"\nPlonk\u0026rsquo;s Setup Phase In Plonk\u0026rsquo;s Setup Phase (SRS Generation Phase), we first obtain a Structured Reference String (SRS). The SRS contains powers of a secret exponent $τ$ (used for polynomial commitments):\n$$SRS={g^{τ^0},g^{τ^1},g^{τ^2},\\ldots,g^{τ^n}}$$This step is one-time and global (universal SRS).\nThen, for a specific circuit $C$, we derive the Proving Key (PK) and Verifying Key (VK):\n$$(PK,~VK)\\leftarrow \\mathrm{Plonk.Setup}(C, SRS)$$The PK tuple contains polynomials for each gate, permutation, and constraint in the circuit (commitments encoded with $\\tau$); the VK tuple contains public polynomial commitments + selector domain information + constant verification structures.\nImplementation of VK Uniqueness in Plonk Mathematical Expression of Circuits: In PLONK, the complete structure and constraints of a circuit $C$ are ultimately defined mathematically through a set of selector polynomials ${q_L,q_R,q_M,q_O,q_C}$ and permutation polynomials ${\\sigma_{S1},\\sigma_{S2},\\sigma_{S3}}$. These polynomials collectively encode what type each gate is (addition gate, multiplication gate, or custom gate) and the wiring relationships between gates.\nVK is a Commitment to the Circuit: The core components of VK are precisely the commitments to these polynomials that define the circuit structure. Specifically, it contains (but is not limited to) commitment values evaluated at the secret point $s$ in the SRS for these polynomials:\n$$vk={\\ldots,[q_L(s)]_1,[q_R(s)]_1,\\ldots,[\\sigma_{S1}(s)]_1,\\ldots}$$When the semantic substance of the statement differs, the circuit structure will be different.\nExample:\nUser A proves \u0026ldquo;I know the proving key PK_A corresponding to VK_A\u0026rdquo;\nUser B proves \u0026ldquo;I know the proving key PK_B corresponding to VK_B\u0026rdquo;\nIf the circuit logic corresponding to these two VKs is different (e.g., different number of arithmetic gates, different constraint matrices),\nthen their structures ${q_L,q_R,q_M,q_O,q_C}$ must be different.\nTherefore, the resulting VK (Verification Key) must also be different.\nThat is to say:\nIf \u0026ldquo;knowing PK_A\u0026rdquo; and \u0026ldquo;knowing PK_B\u0026rdquo; require implementing different logic in the circuit → the circuits are different;\nIf both call the same verifier function, just with different parameters → the circuit is the same.\nUsing Salt to Achieve VK Uniqueness For situations requiring \u0026ldquo;different VK for each user\u0026rdquo;, this can be achieved by:\nAdding user-specific constants in the circuit definition (such as public key, salt, session id);\nOr \u0026ldquo;compiling\u0026rdquo; these constants into the circuit when generating it;\nThus making the circuit structure $C_i$ different, and VK consequently different.\nFormally:\n$$C_i=C(logic,~user_{salt_i})\\quad \\Rightarrow \\quad vk_i=Setup(C_i,~SRS)$$Compared to modifying coefficients which can also make VK different, this approach means:\nThe circuit logic must change;\nThe circuit must be recompiled (requiring regeneration of the constraint system);\nCannot be universally reused.\nWhile the \u0026ldquo;salt\u0026rdquo; approach is:\nAdding a public input constant to the circuit;\nThis constant participates in some hash or gate constraint;\nLogically still the same, but the circuit instance (i.e., specific coefficient values) is different;\nTherefore VK is also uniquified, but the circuit template can still be universal.\nConcerns and Recommendations Regarding Salt in the Project Description The concerns mainly stem from understanding the traditional use of \u0026ldquo;salt\u0026rdquo; in cryptography, such as in hash functions or key derivation functions (KDF):\nTraditional Salt (e.g., password hashing):\nPurpose: Increase entropy to defend against precomputation attacks (such as rainbow tables).\nProperties: Must be random or at least unique.\nSecurity concerns: How to securely store salt, how to ensure its randomness.\nHowever, the role and security model of this \u0026ldquo;salt\u0026rdquo; used to achieve VK uniqueness in PLONK circuits is completely different. We can dispel readers\u0026rsquo; concerns by redefining the purpose of this \u0026ldquo;salt\u0026rdquo;.\nThe \u0026ldquo;Salt\u0026rdquo; Here is Not an Entropy Source, but a \u0026ldquo;Domain Separator\u0026rdquo; It would be more accurate to understand this \u0026ldquo;salt\u0026rdquo; as a public, deterministic identifier. Its purpose is not to add randomness, but to perform domain separation—that is, to cryptographically split a universal circuit template into multiple unrelated, unique instances.\n","permalink":"https://sidereushu.github.io/posts/research-on-vk-uniqueness-and-salt-in-plonk-setup-phase/","summary":"\u003cbr\u003e\n\u003ch2 id=\"plonks-setup-phase\"\u003ePlonk\u0026rsquo;s Setup Phase\u003c/h2\u003e\n\u003cp\u003eIn Plonk\u0026rsquo;s \u003cstrong\u003eSetup Phase (SRS Generation Phase)\u003c/strong\u003e, we first obtain a Structured Reference String (SRS). The SRS contains powers of a secret exponent $τ$ (used for polynomial commitments):\u003c/p\u003e\n$$SRS={g^{τ^0},g^{τ^1},g^{τ^2},\\ldots,g^{τ^n}}$$\u003cp\u003eThis step is \u003cstrong\u003eone-time and global (universal SRS)\u003c/strong\u003e.\u003c/p\u003e\n\u003cp\u003eThen, for a specific circuit $C$, we derive the Proving Key (PK) and Verifying Key (VK):\u003c/p\u003e\n$$(PK,~VK)\\leftarrow \\mathrm{Plonk.Setup}(C, SRS)$$\u003cp\u003eThe PK tuple contains polynomials for each gate, permutation, and constraint in the circuit (commitments encoded with $\\tau$); the VK tuple contains public polynomial commitments + selector domain information + constant verification structures.\u003c/p\u003e","title":"Research on VK Uniqueness and Salt in Plonk Setup Phase"},{"content":"\nIn recent years, \u0026ldquo;ZK-friendly\u0026rdquo; hash functions designed for zero-knowledge proof (ZK) scenarios have received widespread attention. They are typically based on carefully constructed algebraic structures to achieve lower constraint counts in arithmetic circuits, thereby enabling higher efficiency in proof systems. However, it is precisely this algebraic friendliness that exposes potential attack surfaces under certain analysis models. Particularly in recent years, algebraic analysis methods—such as Gröbner basis attacks and polynomial degree reduction techniques using subspace trails—have gained significant research interest in the cryptanalysis field and have gradually become one of the core tools for evaluating the security of such hash functions.\nThis article is based on a recent paper [GKR25] by the Poseidon designers, verifying the main conclusions related to the attack algorithms proposed in that work, making it our theoretical calculation tool.\nAnalysis Tool 1: Parameter Bounds Recommended by Poseidon Designers [GKR25] is the latest work proposed by the Poseidon designers. This work provides the minimum partial rounds required for the Poseidon2 hash function in Sponge mode to achieve 128-bit security in the figure below. The black line in the figure represents the parameter settings recommended by the Poseidon2 designers (where the dashed line includes a safety redundancy of +7.5% · rₚ), and the other colored lines are the current mainstream attack test data conducted by the authors.\nFrom the black line in the figure, we can see that the Partial rounds value rₚ recommended by the Poseidon authors is in the 52-53 range, and with safety redundancy, the rₚ value is in the 56-58 range. This data is calculated from the following theoretical lower bound formula:\n$$ r_F = 6 ; (r_f = 3, ; r_f' = 3) , , $$$$ r_P = \\left\\lceil \\max \\left\\{ \\underbrace{1 + \\frac{\\min\\{\\kappa, \\log_2(p)\\}}{\\log_2(\\alpha)} + \\log_{\\alpha}(t) - r_F}_{\\text{Term1}}, \\; r_{GB} \\right\\} \\right\\rceil , $$$$ r_{GB} = \\max \\left\\{ \\frac{\\min\\{\\log_2(p), \\kappa\\}}{\\log_2(\\alpha)} - r_F, \\; t - 1 + \\log_{\\alpha}(2) \\cdot \\min \\left\\{ \\frac{\\kappa}{t+1}, \\frac{\\log_2(p)}{2} \\right\\} - r_F' \\right\\} .$$Verification of Poseidon Authors\u0026rsquo; Conclusions: We first select a set of parameters to see if we can verify the authors\u0026rsquo; conclusions shown in the figure:\nParameters:\n$p \\approx 2^{256}$ (thus $\\log_2(p) = 256$) $\\alpha = 5$ $t = 8$ $\\kappa = 128$ $r_F = 6$ (we first don\u0026rsquo;t consider safety redundancy) Calculate intermediate values:\n$\\log_2(\\alpha) = \\log_2(5) \\approx 2.322$ $\\log_\\alpha(t) = \\log_5(8) = \\frac{\\log_2(8)}{\\log_2(5)} = \\frac{3}{2.322} \\approx 1.292$ Step 1: Calculate Term1 (Defense against interpolation attacks) $$ \\text{Term}1 = 1 + \\frac{\\min{\\{128, 256}\\}}{2.322} + 1.292 - 6 $$ $$ = 1 + \\frac{128}{2.322} + 1.292 - 6 $$ $$ \\approx 1 + 55.125 + 1.292 - 6 $$ $$ \\approx 51.417 $$Step 2: Calculate $r_{GB}$ The $r_{GB}$ formula also contains a max operation. We calculate its main components separately:\nFirst part of $r_{GB}$: $$ \\frac{\\min{\\{\\log_2(p), \\kappa}\\}}{\\log_2(\\alpha)} - r_F = \\frac{\\min{\\{256, 128}\\}}{2.322} - 6 \\approx 55.125 - 6 = 49.125 $$ Second part of $r_{GB}$:\n$\\log_{\\alpha}(2) = \\frac{\\log_2(2)}{\\log_2(5)} = \\frac{1}{2.322} \\approx 0.431$\n$\\min\\left\\{\\frac{\\kappa}{t+1}, \\frac{\\log_2(p)}{2}\\right\\} = \\min\\left\\{\\frac{128}{8+1}, \\frac{256}{2}\\right\\} = \\min\\{14.22, 128\\} = 14.22$\n$t - 1 + \\log_{\\alpha}(2) \\cdot (14.22) - r_F = 8 - 1 + 0.431 \\cdot 14.22 - 6 \\approx 7 + 6.13 - 6 = 7.13$\nTherefore, $r_{GB} = \\max{\\{49.125, 7.13\\}}$. We can see that $r_{GB}$ is at least $49.125$.\nStep 3: Final $r_P$ calculation $$ r_P = \\max{\\{\\text{Term1}, r_{GB}\\}} = \\max{\\{51.417, r_{GB}\\}} $$Since the value of Term1 (51.417) is greater than the calculated part of $r_{GB}$ (49.125), the final $r_P$ is determined by the defense requirements against interpolation attacks.\n$$ r_P \\approx 51.417 $$After rounding up, we get $r_P = 52$. This result matches the authors\u0026rsquo; recommended parameter range. If we add the 7.5% safety margin, then the final partial rounds would be:\n$$\\lceil 52+52\\times 7.5\\% \\rceil = \\lceil 55.9 \\rceil = 56$$Parameter Analysis: After verification, we can now use the above analysis method as an analysis tool to conduct security assessments for any set of analysis data. For example, below we perform a security assessment for the following set of parameters.\nParameters: $t=2$, $d=1$, $c=0$, $α=5$, $\\kappa=128$, $p=381$\nPre-calculated common values:\n$\\min{\\{\\kappa, \\log_2(p)\\}} = \\min{\\{128, 381\\}} = 128$ $\\log_2(\\alpha) = \\log_2(5) \\approx 2.322$ $\\log_\\alpha(t) = \\log_5(2) = \\log_2(2) / \\log_2(5) = 1 / 2.322 \\approx 0.4307$ $\\log_\\alpha(2) = \\log_5(2) \\approx 0.4307$ Step 1: Calculate $r_{GB}$ Calculation process:\nFirst subterm of $r_{GB}$: $$ \\frac{\\min{\\{\\log_2(p), \\kappa}\\}}{\\log_2(\\alpha)} - r_F $$ $$ = \\frac{128}{2.322} - 6 $$ $$ \\approx 55.125 - 6 = 49.125 $$ Second subterm of $r_{GB}$: $$t - 1 + \\log_{\\alpha}(2) \\cdot \\min\\left\\{ \\frac{\\kappa}{t+1}, \\frac{\\log_2(p)}{2} \\right\\} - r'_F$$First calculate the $\\min{\\ldots}$ part:\n$$\\min\\left\\{ \\frac{128}{2+1}, \\frac{381}{2} \\right\\} = \\min\\{42.67, 190.5\\} = 42.67$$Then substitute into the complete expression:\n$$ = 2 - 1 + 0.4307 \\cdot 42.67 - 3 $$ $$ \\approx 1 + 18.38 - 3 = 16.38 $$ Determine the value of $r_{GB}$: $$ r_{GB} = \\max{\\{49.125, 16.38\\}} = 49.125 $$Step 2: Calculate final $r_P$ Now, $r_P$ is determined by taking the maximum of the other two subterms and then rounding up.\nFirst subterm of $r_P$ (resistance against algebraic attacks): $$ 1 + \\frac{\\min{\\{\\kappa, \\log_2(p)}\\}}{\\log_2(\\alpha)} + \\log_\\alpha(t) - r_F $$ $$ \\approx 1 + \\frac{128}{2.322} + 0.4307 - 6 $$ $$ \\approx 1 + 55.125 + 0.4307 - 6 $$ $$ = 56.5557 - 6 = 50.556 $$ Second subterm of $r_P$ (resistance against GB attacks): This value is the $r_{GB}$ we calculated in the previous step. $$ r_{GB} = 49.125 $$ Determine the value of $r_P$: $$ r_P = \\lceil \\max{\\{50.556, 49.125\\}} \\rceil $$ $$ r_P = \\lceil 50.556 \\rceil $$ $$ r_P = 51 $$The result of $r_P = 51$ matches the authors\u0026rsquo; recommended parameter range. If we add the 7.5% safety margin, then the final partial rounds would be:\n$$\\lceil 51+51\\times 7.5\\% \\rceil = \\lceil 54.825 \\rceil = 55$$\nAnalysis Tool 2: Analysis of the Improved Attack Algorithm Proposed in [GKR25] The improved attack algorithm proposed by the [GKR25] authors is based on the original GB attack algorithm and proposes a forward GB attack algorithm. The attack model Model 1 (FW) it adopts is shown in the following screenshot from the original paper:\nVerification of Poseidon Authors\u0026rsquo; Conclusions: We first select a set of parameters from Table 3 to see if we can verify the corresponding conclusions in the table. Before this, we restate our goal: find the minimum integer $r_P$ such that the attack complexity $C_{GB} ≥ 2^{128}$.\nCorresponding inequality to be verified: $$\\left(1 + 2\\sqrt{t - d + \\alpha} \\cdot r_F + \\frac{r_p^\\ell}{2}\\right)^2 \\geq 2^{128}$$ Corresponding equivalent condition after transformation: $$\\left(1 + 2\\sqrt{t - d + \\alpha} \\cdot r_F + \\frac{r_p^\\ell}{2}\\right) \\geq 2^{64}$$ Test Case 1 We selected the $t=8$ row from the first part of Table 3 ($p \\approx 2^{96}, d=3$).\nTable 3 target value: $r_p = 2 - 5$\nKnown parameters:\n$t = 8, d = 3, \\alpha = 3$ $R_F = 6 \\rightarrow r_F = 3$ According to Table3\u0026rsquo;s sponge mode setting, $c = d = 3$ Derived parameters:\n$r = t - c = 8 - 3 = 5$ $\\ell = r - d = 5 - 3 = 2$ Calculation Process: Substitute the parameters into our inequality condition:\n$$\\left( \\frac{1 + 2\\sqrt{8 \\cdot 3 \\cdot 3^{r_p / 2}}}{8 + 3} \\right) \\geq 2^{64}$$$$\\left( \\frac{1 + 2\\sqrt{24 \\cdot 3^{2 + 0.5 r_p}}}{11} \\right) \\geq 2^{64}$$We solve this inequality for $r_p$. Let $N$ such that $(N) \\geq 2^{64}$.\nUsing approximation $N^{11} \\geq 2^{64}$, we get $N \\geq (11! \\cdot 2^{64})^{1/11}$.\nAfter a series of logarithmic operations and algebraic simplifications (process similar to the previous calculation): $$1+9.798 \\cdot 3^{2+0.5r_P} \\ge 4.9 \\cdot 2^{5.818}$$$$0.7925 \\cdot r_P \\ge 1.658$$$$r_P \\ge 2.09$$Conclusion: The calculation requires $r_P$ to be greater than or equal to 2.09, which rounds up to $r_P = 3$. Table 3 gives the range as $2-5$. Our calculation result 3 falls within this range.\nTest Case 2 We selected the parameters corresponding to the $t=16$ row in Table3 with $p ≈ 2^{128}$, $d=2$.\nTable 3 target value: $r_p = 12$\nKnown parameters:\n$t = 16, d = 2, \\alpha = 3$ $R_F = 6 \\rightarrow r_F = 3$ According to Table3\u0026rsquo;s sponge mode setting, $c = d = 2$ Derived parameters:\n$r = t - c = 16 - 2 = 14$ $\\ell = r - d = 14 - 2 = 12$ The calculation process is the same as Test Case 1. The calculation result is $r_p = 11$, which differs from the target value $r_p = 12$ by only one round.\nTest Case 3 We selected the parameters corresponding to the $t=24$ row in Table3 with $p ≈ 2^{256}$, $d=1$.\nTable 3 target value: $r_p = 22$\nKnown parameters:\n$t = 24, d = 1, \\alpha = 3$ $R_F = 6 \\rightarrow r_F = 3$ According to Table3\u0026rsquo;s sponge mode setting, $c = d = 1$ Derived parameters:\n$r = t - c = 24 - 1 = 23$ $\\ell = r - d = 23 - 1 = 22$ The calculation process is the same as Test Case 1. The calculation result is $r_p = 21$, which also differs from the target value $r_p = 22$ by only one round.\nTest Case 4 We selected the parameters corresponding to the $t=12$ row in Table3 with $p ≈ 2^{256}$, $d=1$.\nTable 3 target value: $r_p = 10$\nKnown parameters:\n$t = 12, d = 1, \\alpha = 5$ $R_F = 6 \\rightarrow r_F = 3$ According to Table3\u0026rsquo;s sponge mode setting, $c = d = 1$ Derived parameters:\n$r = t - c = 12 - 1 = 11$ $\\ell = r - d = 11 - 1 = 10$ The calculation process is the same as Test Case 1. The calculation result is $r_p = 9$, which still differs from the target value $r_p = 10$ by only one round.\nParameter Analysis: After verification, we can now use the above analysis method as an analysis tool to conduct security assessments for any set of analysis data. For example, below we perform a security assessment for the following set of parameters.\nParameters to Calculate: State width: $t = 2$ Mode: compress, $c = 0$ Rate: $r = t - c = 2$ Security requirement: $\\kappa=128$ Modulus: $p=381$ Output length: $d = 1$ Full rounds: $R_F = 6 \\implies$ half full rounds $r_F = 3$ S-box: $\\alpha = 5$ Known constants: Attack complexity exponent: $\\omega = 2$ Intermediate variables: Subspace trail length: $\\ell = r - d = 2 - 1 = 1$ Target formula: The attack cost $C_{GB}$ must reach the security target of $2^{128}$.\n$$ C_{GB} \\approx \\left( \\binom{ 1 + 2\\sqrt{t \\cdot d} \\cdot \\alpha^{r_F + \\frac{r_P - \\ell}{2}} }{ t + d } \\right)^{\\omega} \\geq 2^{128} $$Substitute parameters and establish inequality: Substituting the above parameters into the formula:\n$$ \\left( \\binom{ 1 + 2\\sqrt{2 \\cdot 1} \\cdot 5^{3 + \\frac{r_P - 1}{2}} }{ 2 + 1 } \\right)^2 \\geq 2^{128} $$Solve for $r_P$ First, take the square root of both sides: $$ \\left( \\frac{1 + 2\\sqrt{2} \\cdot 5^{2.5+0.5\\cdot r_P}}{3} \\right) \\geq 2^{64} $$ For convenience, let the upper term of the binomial coefficient (that longer expression) be $N$: $$ N = 1 + 2\\sqrt{2} \\cdot 5^{2.5+0.5\\cdot r_P} $$ Then the inequality becomes: $$ \\binom{N}{3} \\geq 2^{64} $$ When $N$ is much larger than $k$, the binomial coefficient can be approximated as: $$ \\binom{N}{k} \\approx \\frac{N^k}{k!} $$ Therefore, $$ \\frac{N^3}{3!} \\geq 2^{64} \\implies \\frac{N^3}{6} \\geq 2^{64} $$ Rearranging: $$ N^3 \\geq 6 \\cdot 2^{64} $$ Taking the cube root of both sides: $$ N \\geq \\sqrt[3]{6 \\cdot 2^{64}} = \\sqrt[3]{6} \\cdot 2^{64/3} $$ Calculate the approximate value of the right side: $$ \\sqrt[3]{6} \\approx 1.817, \\quad 2^{64/3} \\approx 2^{21.333} $$ So, $$ N \\geq 1.817 \\cdot 2^{21.333} $$ Now, we substitute back the expression for $N$ to solve for $r_P$: $$ 1 + 2\\sqrt{2} \\cdot 5^{2.5+0.5\\cdot r_P} \\geq 1.817 \\cdot 2^{21.333} $$ The +1 on the left side can be ignored in the face of such large numbers. Taking the base-2 logarithm $\\log_2$ of both sides: $$ \\log_2 (2\\sqrt{2}) + (2.5 + 0.5 \\cdot r_P) \\log_2 (5) \\geq \\log_2 (1.817) + 21.333 $$ Substituting common logarithm values: $$ \\log_2 (2\\sqrt{2}) = 1.5, \\quad \\log_2 (5) \\approx 2.322, \\quad \\log_2 (1.817) \\approx 0.862 $$ $$ 1.5 + (2.5 + 0.5 \\cdot r_P) \\cdot 2.322 \\geq 0.862 + 21.333 $$ $$ 1.5 + 5.805 + 1.161 \\cdot r_P \\geq 22.195 $$ $$ 7.305 + 1.161 \\cdot r_P \\geq 22.195 $$ $$ 1.161 \\cdot r_P \\geq 14.89 $$ $$ \\lceil r_P \\rceil \\geq \\Bigg\\lceil \\frac{14.89}{1.161} \\Bigg\\rceil \\approx \\lceil 12.825 \\rceil = 13 $$ Under the parameter settings we provided, after correctly interpreting the FW-GB attack cost formula, to achieve 128-bit security, the required partial rounds value is: $$r_{P}=13$$ ","permalink":"https://sidereushu.github.io/posts/poseidon-hash-algebraic-attacks-calculation-tools/","summary":"\u003cbr\u003e\n\u003cp\u003eIn recent years, \u0026ldquo;ZK-friendly\u0026rdquo; hash functions designed for zero-knowledge proof (ZK) scenarios have received widespread attention. They are typically based on carefully constructed algebraic structures to achieve lower constraint counts in arithmetic circuits, thereby enabling higher efficiency in proof systems. However, it is precisely this algebraic friendliness that exposes potential attack surfaces under certain analysis models. Particularly in recent years, algebraic analysis methods—such as Gröbner basis attacks and polynomial degree reduction techniques using subspace trails—have gained significant research interest in the cryptanalysis field and have gradually become one of the core tools for evaluating the security of such hash functions.\u003c/p\u003e","title":"Poseidon Hash Algebraic Attacks (Calculation Tools)"},{"content":"\nWith the rapid development of quantum computing technology, blockchain systems face unprecedented security challenges. Particularly for the consensus layers of mainstream blockchains like Ethereum, the BLS signature schemes they rely on will become vulnerable in the face of quantum computers. This article delves into the considerations for choosing consensus layer signature schemes, focusing on analyzing the trade-offs between hash-based XMSS schemes and lattice-based Falcon schemes in different scenarios, and exploring new signature paradigms for ZK-native chains.\n1. Quantum Threats and Special Requirements of Consensus Layers 1.1 Current Challenges Since Ethereum\u0026rsquo;s The Merge upgrade in 2022, it has adopted a PoS consensus mechanism where validators use BLS signatures to confirm blocks and votes. BLS signatures have excellent aggregation properties—thousands of signatures can be compressed into a single proof, greatly reducing storage and bandwidth requirements. However, these signature schemes based on elliptic curve cryptography are vulnerable to quantum computers running Shor\u0026rsquo;s algorithm.\nIt is estimated that within 10-15 years, computers with millions of qubits may be able to break ECDSA/BLS. Therefore, finding signature schemes that can both resist quantum attacks and meet the special needs of consensus layers has become an urgent priority.\n1.2 Special Requirements of Consensus Layers Consensus layer signature schemes must meet the following key requirements:\nPost-quantum security: Ability to resist attacks from Grover\u0026rsquo;s and Shor\u0026rsquo;s algorithms Efficient aggregation capability: Support for non-interactive aggregation of thousands of validator signatures Low verification cost: On-chain verification gas costs must be controllable Bandwidth-friendly: Aggregated signature sizes should be reasonable Implementation simplicity: Easy to audit, reducing the risk of implementation errors 2. Main Candidates for Post-Quantum Signature Schemes 2.1 XMSS: The Conservative Hash-Based Choice Core Principles XMSS (eXtended Merkle Signature Scheme) is a stateful hash signature scheme that organizes multiple one-time signatures (WOTS+) through a Merkle tree. Its security relies entirely on the one-way property of the underlying hash function, which is the most conservative and widely accepted assumption in cryptography.\nAdvantages in Consensus Layers Minimal security assumptions: Only relies on hash functions, no complex algebraic structures needed Aggregation-friendly: While XMSS signatures don\u0026rsquo;t support native aggregation, their verification process mainly involves hash operations, making them very suitable for \u0026ldquo;proof-based aggregation\u0026rdquo; through ZK-SNARKs/STARKs Forward security: Each signing key can only be used once; even if keys are compromised, past signatures remain secure Practical Parameters and Optimizations In Ethereum proposals, through optimizations like Target Sum Winternitz (TSW) encoding, XMSS signature sizes can be controlled within the 2-4 KB range. Combined with pqSNARKs, thousands of signatures can be aggregated into a single compact proof:\nOriginal: 4,096 validator signatures ≈ 16 MB After aggregation: Single proof \u0026lt; 1 MB 2.2 Falcon: The Compact Lattice-Based Scheme Core Characteristics Falcon is based on the NTRU lattice problem and is one of the NIST-standardized post-quantum signature schemes. Its greatest advantage is extremely compact signatures:\nFalcon-512: Signature ~666 bytes Falcon-1024: Signature ~1.3 KB Challenges in Consensus Layers Aggregation complexity: Falcon doesn\u0026rsquo;t support native aggregation and faces serious challenges when combined with ZK proof systems Arithmetization difficulties: Verifying Falcon signatures involves complex polynomial operations and norm calculations, making conversion to ZK circuits extremely costly Theoretical uncertainties: Under the Quantum Random Oracle Model (QROM), security proofs rely on \u0026ldquo;rewinding\u0026rdquo; techniques that fail. Existing \u0026ldquo;Falcon aggregation\u0026rdquo; schemes (like LaBRADOR) require additional algebraic structures and rewinding techniques, but their security proofs are questionable under quantum models (random oracle paradox, infeasibility of quantum rewinding). 2.3 Why Choose XMSS Over Falcon? Let\u0026rsquo;s understand this choice through detailed comparison:\nDimension XMSS Falcon Consensus Layer Impact Security Assumptions Hash functions (minimal) NTRU lattice (more complex) XMSS more conservative, lower long-term risk Single Signature Size 2-4 KB \u0026lt; 1 KB Falcon better individually, but gap narrows after aggregation Aggregation Scheme ZK-friendly (hash circuits) ZK-difficult (algebraic circuits) Decisive factor: XMSS aggregation efficiency far exceeds Falcon Implementation Complexity Low (hash only) High (floating-point, FFT, side-channels) XMSS easier to audit and implement securely State Management Required (stateful) Not required (stateless) Falcon more convenient in user scenarios Key insight: The core need of consensus layers is aggregation, and XMSS\u0026rsquo;s hash structure is naturally suited for ZK proof systems, while Falcon\u0026rsquo;s lattice-based structure leads to explosive proof generation costs.\n3. Proof-Based Aggregation: The Synergy of XMSS + ZK 3.1 Workflow 1. Collection phase: Aggregator collects thousands of XMSS signatures 2. Proof generation: Run verification program, generate ZK proof for \u0026#34;all signatures are valid\u0026#34; 3. On-chain verification: Only need to verify a single compact ZK proof 3.2 Why Does XMSS Excel in This Model? The core operation for verifying XMSS signatures is hash computation. \u0026ldquo;Circuit-friendly\u0026rdquo; hash functions like Poseidon and Rescue are extremely efficient in the arithmetization process. In contrast, Falcon verification involves:\nComplex operations on polynomial rings High-precision norm calculations Lattice basis reduction related operations The circuit representation of these operations may be several orders of magnitude larger than hashes, directly causing explosive growth in proof generation time and costs.\n3.3 Deep Considerations of Quantum Security Under the Quantum Random Oracle Model (QROM), many classical security proof techniques fail. Particularly, the Falcon + ZK combination faces the \u0026ldquo;rewinding paradox\u0026rdquo;:\nClassical proofs rely on repeatedly \u0026ldquo;rewinding\u0026rdquo; queries to extract keys Quantum attackers query in superposition states, breaking the foundation of rewinding arguments XMSS\u0026rsquo;s simple hash structure avoids these theoretical pitfalls 4. New Paradigms for ZK-Native Chains: Poseidon Preimage Proofs 4.1 Conceptual Innovation In ZK-native chains, a new signature paradigm is emerging: using \u0026ldquo;proof of knowing preimage x such that Poseidon(x) = Y\u0026rdquo; as identity authentication. This essentially uses the ZK proof system itself as the signature mechanism.\n4.2 Advantage Analysis Circuit nativeness: Poseidon is designed specifically for ZK circuits with minimal constraint counts Flexible composition: Can bind arbitrary context (timestamps, chain state, etc.) in proofs Privacy enhancement: Only reveals the proof, leaking no information about the preimage 4.3 Practical Challenges Traditional signature generation: Millisecond level ZK proof generation: Second to minute level (depending on circuit scale) Verification comparison: Traditional signature verification: O(1) simple operations SNARK verification: O(1) but larger constants STARK verification: O(log n) or higher Key trade-off: While proof generation is much slower, if the entire system is ZK-native, this approach can bring unprecedented composability and privacy features.\n5. Practical Recommendations: Layered Strategy Based on the above analysis, we recommend adopting a layered strategy:\n5.1 Consensus Layer Choice: XMSS + pqSNARKs\nRationale:\nFixed validator set, manageable state management Aggregation is core requirement, XMSS + ZK combination is optimal Conservative security assumptions, suitable for long-term critical infrastructure 5.2 Execution Layer Choice: Falcon or other compact schemes\nRationale:\nDiverse users, stateless is more practical Single signature size directly affects gas costs No need for large-scale aggregation 5.3 ZK-Native Application Layer Choice: Poseidon preimage proofs\nRationale:\nDeep integration with application logic Clear advantages in composability and privacy Proof costs can be amortized through batch processing Conclusion The choice of consensus layer signature schemes is not just a technical decision, but a security commitment for the next decade or even longer. XMSS represents a conservative yet pragmatic choice—through the simplest cryptographic assumptions, combined with innovative proof aggregation technology, it lays a solid foundation for blockchain consensus in the post-quantum era.\nWhile lattice-based schemes like Falcon are superior in some metrics, the special needs of consensus layers—particularly large-scale signature aggregation—make XMSS a more suitable choice. Meanwhile, the rise of ZK-native chains brings entirely new design spaces for signature schemes, and innovative approaches like Poseidon preimage proofs demonstrate the enormous potential of deep integration of cryptographic primitives.\nUltimately, there is no \u0026ldquo;optimal\u0026rdquo; signature scheme, only the most appropriate choice for specific scenarios. Through a layered strategy, we can adopt the most suitable schemes at the consensus layer, execution layer, and application layer respectively, building a post-quantum blockchain system that is both secure and efficient.\nReferences RFC 8391 - XMSS: eXtended Merkle Signature Scheme NIST Post-Quantum Cryptography Standardization Ethereum Research Forum - Consensus Layer PQC Discussions Hash-Based Multi-Signatures for Post-Quantum Ethereum Poseidon: A New Hash Function for Zero-Knowledge Proof Systems Falcon: Fast-Fourier Lattice-based Compact Signatures over NTRU The Lean Consensus Roadmap Target Sum Winternitz: An Improvement to the Winternitz Signature Scheme ","permalink":"https://sidereushu.github.io/posts/consensus-layer-signature-scheme-considerations-evolution-from-bls-to-the-post-quantum-era/","summary":"\u003cbr\u003e\n\u003cp\u003eWith the rapid development of quantum computing technology, blockchain systems face unprecedented security challenges. Particularly for the consensus layers of mainstream blockchains like Ethereum, the BLS signature schemes they rely on will become vulnerable in the face of quantum computers. This article delves into the considerations for choosing consensus layer signature schemes, focusing on analyzing the trade-offs between hash-based XMSS schemes and lattice-based Falcon schemes in different scenarios, and exploring new signature paradigms for ZK-native chains.\u003c/p\u003e","title":"Consensus Layer Signature Scheme Considerations: Evolution from BLS to the Post-Quantum Era"},{"content":"\nPoseidon is a ZK-friendly hash function, and its security evaluation largely relies on theoretical modeling of the complexity of Gröbner basis (GB) attacks. Early security analyses (such as [GKR+19], [GLR+20]) primarily employed input–output modeling (directly converting the input-output relationship of the entire function into a polynomial equation system) and calculated the so-called degree of regularity on this model to estimate the complexity of GB attacks.\nIn [GLR+20], another approach was mentioned—round-level modeling: introducing new variables for each S-Box in each round, thereby decomposing the entire function into equation systems for multiple rounds.\nNotably, recent work [ABM24] revisited the feasibility of round-by-round modeling. They directly referenced the approach from [GLR+20] but further used practical experiments to validate the effectiveness of round-by-round modeling in GB attacks, rather than merely relying on theoretical speculation.\nABM24\u0026rsquo;s Method Under the round-by-round modeling model used in [ABM24]:\nThe input and output of each S-Box are independent variables; Polynomial equations for each round only involve the S-Box inputs/outputs and linear layer of the current round; This results in lower degree equations in the system (especially when some rounds are linearized), facilitating faster solving by GB algorithms. Under this modeling, one key indicator of the attack is $D_{\\mathrm{reg}}$ (degree of regularity), which affects the complexity estimation of Gröbner basis solving.\n[ABM24] argued that $D_{\\mathrm{reg}}$ was overestimated, meaning that while the system was previously considered more difficult to solve, it might actually not be that hard.\n[ABM24] discovered an assumption error in previous $D_{\\mathrm{reg}}$ calculations:\nOriginally assumed the equation system was a regular system, but it actually wasn\u0026rsquo;t.\nTherefore, the correct number of equations and variables should be: $$(R_{F}−1)⋅t+R_{P}+r$$ where:\n$R_{F}$ = number of full rounds\n$R_{P}​$ = number of partial rounds.\n$t$ = state size - $r$ = rate, with $r≥t/3$\nUnder this correction, the more precise $D_{\\mathrm{reg}}$ estimation formula becomes: $$D_{\\mathrm{reg}} ≈ \\frac {r⋅R_{F}} {2} + R_{P} + α$$(where $α$ is a small constant related to system structure)\nABM24\u0026rsquo;s Conclusions Main Findings\n[ABM24] analyzed Poseidon\u0026rsquo;s security assumptions and found that the degree growth rate (degree of regularity) is slower than originally assumed by the designers.\nThis means that under certain parameters, the recommended number of rounds is insufficient to achieve the claimed security level.\nSpecific Impact\n1024-bit security level: Clearly demonstrated a complete attack instance (i.e., security was significantly overestimated).\n512-bit and 384-bit security levels: Security analysis is also invalid, but they did not successfully provide complete attacks—the authors believe other factors in the design may have hindered the final step of the attack.\n128-bit and 256-bit security levels: No vulnerable parameter sets were found; the designers\u0026rsquo; original security estimates remain valid at these levels.\nScope Limitations\nTheir conclusions lean toward asymptotic analysis, not claiming all parameters are problematic, but pointing out that under certain high-security-level parameters, theoretical resistance is worse than expected. Poseidon Authors\u0026rsquo; Evaluation of [ABM24] Poseidon authors accepted [ABM24]\u0026rsquo;s correction in their updated paper, incorporating the new $D_{\\mathrm{reg}}$ formula.\nHowever, Poseidon authors stated that this correction has no impact on currently used parameters, as commonly used instances still satisfy security conditions. Specifically:\nSecurity level at 128-bit Modulus size around 64 bits or 256 bits Common SNARK parameters $t∈{3,5}$ Common FRI parameters $t∈{12,24}$ All of these remain unaffected. Therefore, Poseidon authors continued with their original security analysis framework: $$\\Big(\\frac {V+D_{\\mathrm{reg}}}{D_{\\mathrm{reg}}}\\Big)^2 \\geq 2^{\\kappa}$$ where:\nTotal number of variables $V=(R_{F}-1)\\cdot t + R_{P} +r$ $\\kappa$ is the target security strength Summary of Poseidon authors\u0026rsquo; evaluation of [ABM24]:\n[ABM24] pointed out that $D_{\\mathrm{reg}}$ estimates in early round-by-round modeling were too high and provided a more accurate calculation formula. Poseidon authors acknowledged this correction and incorporated it into their analysis framework, but found it has no security impact on currently deployed Poseidon instances.\nPoseidon2 Authors\u0026rsquo; Evaluation of [ABM24] The Poseidon2 paper acknowledged including ABM24\u0026rsquo;s results in their security analysis references (i.e., they at least considered its conclusions).\nPoseidon2 believes [ABM24]\u0026rsquo;s results will not affect the number of rounds for any $\\text{Poseidon}^{π}$ function instance, because the parameter space of $\\text{Poseidon}^{π}$ (especially modulus size $p$, state width $t$, security target $κ$) does not fall within the region where [ABM24] found reduced security margins.\nPoseidon2 authors believe [ABM24]\u0026rsquo;s results have no practical impact, so they do not adjust the number of rounds in Poseidon2\u0026rsquo;s parameter selection and security analysis due to this, nor do they specifically discuss [ABM24]\u0026rsquo;s attack model.\n[GKR25] Authors\u0026rsquo; Evaluation of [ABM24] [GKR25] (a new paper by the Poseidon author team) did not adopt this round-by-round modeling approach.\nReasoning: Poseidon\u0026rsquo;s designers had already argued in early work ([GKR+19] Appendix C.2.2) that round-by-round modeling is weaker than directly modeling the entire input-output relationship (i.e., less favorable to attackers).\nIn other words, if your attack is based on input–output modeling, you might find lower complexity Gröbner basis attacks, while the round-by-round decomposition adopted by [ABM24] would actually increase complexity.\nAdditionally, the affected instances given by [ABM24] (1024-bit security) have very uncommon parameter choices, far higher than security levels used in actual deployments (typically 128-bit or 256-bit).\nOpen Problems\nAlthough Poseidon authors believe round-by-round modeling is weaker, they also acknowledge that there is currently no strict mathematical proof. So [GKR25] treats this as an open problem—future researchers could attempt to formally verify whether \u0026ldquo;round-by-round modeling\u0026rdquo; is indeed always weaker than input–output modeling under all parameters and conditions. ","permalink":"https://sidereushu.github.io/posts/overview-and-evaluation-of-round-by-round-modeling-analysis-for-poseidon-hash/","summary":"\u003cbr\u003e\n\u003cp\u003ePoseidon is a \u003cstrong\u003eZK-friendly hash function\u003c/strong\u003e, and its security evaluation largely relies on theoretical modeling of the complexity of \u003cstrong\u003eGröbner basis (GB) attacks\u003c/strong\u003e. Early security analyses (such as [GKR+19], [GLR+20]) primarily employed \u003cstrong\u003einput–output modeling\u003c/strong\u003e (directly converting the input-output relationship of the entire function into a polynomial equation system) and calculated the so-called \u003cstrong\u003edegree of regularity\u003c/strong\u003e on this model to estimate the complexity of GB attacks.\u003c/p\u003e\n\u003cp\u003eIn [GLR+20], another approach was mentioned—\u003cstrong\u003eround-level modeling\u003c/strong\u003e: introducing new variables for each S-Box in each round, thereby decomposing the entire function into equation systems for multiple rounds.\u003c/p\u003e","title":"Overview and Evaluation of Round-by-Round Modeling Analysis for Poseidon Hash"},{"content":"\nIn recent years, \u0026ldquo;ZK-friendly\u0026rdquo; hash functions designed for zero-knowledge proof (ZK) scenarios have gained widespread attention. They are typically based on carefully constructed algebraic structures to exhibit lower constraint counts in arithmetic circuits, thereby achieving higher efficiency in proof systems. However, it is precisely this algebraic friendliness that exposes them to potential attack surfaces under certain analytical models. Particularly in recent years, algebraic analysis methods—such as Gröbner basis attacks and polynomial degree reduction techniques utilizing subspace trajectories—have seen significant research interest in the cryptanalysis field and have gradually become one of the core tools for evaluating the security of such hash functions.\nThis article approaches from a designer\u0026rsquo;s perspective, based on a recent paper [GKR25], presents some factual conclusions from that work, and provides supplementary analysis for certain conclusions.\nRelated Work Core Method Effectiveness Analysis for Poseidon [FP20] Closed-form degree expressions Helpful for modeling Gröbner basis attacks [BBLP22] Skipping (multiple) full rounds Reduced actual nonlinear depth [ABM24] Round-level Gröbner basis modeling Shows underestimated vulnerabilities at κ = 1024 [BBL+24] FreeLunch Gröbner basis attack Ineffective due to low S-box degree [KLR24] \u0026ldquo;Six Worlds\u0026rdquo; analysis framework Not yet applied to Poseidon, has potential research value [GKR25] Forward Gröbner basis attack utilizing subspace trajectories Original analysis shows overestimation or underestimation of required security rounds [BBB+25] Iterated Resultants Reducible to simple univariate cases [GKR25] is the latest work proposed by the Poseidon designers. This work provides the minimum partial rounds required for the Poseidon2 hash function to achieve 128-bit security in Sponge mode implementation, as shown in the figure below. The black line in the figure represents the parameter settings recommended by the Poseidon2 designers (where the dashed line indicates security redundancy of $+7.5\\% \\cdot r_{P}$), while other colored lines represent the authors\u0026rsquo; test data for current mainstream attacks.\nFrom the black line in the figure, we can see that the Poseidon authors recommend partial rounds $r_{P}$ values in the $52-53$ range. This data is calculated from the following theoretical lower bound formula:\n$$ r_F = 6 ; (r_f = 3, ; r_f' = 3) , , $$$$ r_P = \\left\\lceil \\max \\left\\{ \\underbrace{1 + \\frac{\\min\\{\\kappa, \\log_2(p)\\}}{\\log_2(\\alpha)} + \\log_{\\alpha}(t) - r_F}_{\\text{Term1}}, \\; r_{GB} \\right\\} \\right\\rceil , $$$$ r_{GB} = \\max \\left\\{ \\frac{\\min\\{\\log_2(p), \\kappa\\}}{\\log_2(\\alpha)} - r_F, \\; t - 1 + \\log_{\\alpha}(2) \\cdot \\min \\left\\{ \\frac{\\kappa}{t+1}, \\frac{\\log_2(p)}{2} \\right\\} - r_F' \\right\\} .$$ Verification of Poseidon Authors\u0026rsquo; results: We first select a parameter set to verify whether we can confirm the authors\u0026rsquo; results shown in the figure:\nParameters:\n$p \\approx 2^{256}$ (therefore $\\log_2(p) = 256$) $\\alpha = 5$ $t = 8$ $\\kappa = 128$ $r_F = 6$ (we first ignore the security margin) Calculate intermediate values:\n$\\log_2(\\alpha) = \\log_2(5) \\approx 2.322$\n$\\log_\\alpha(t) = \\log_5(8) = \\frac{\\log_2(8)}{\\log_2(5)} = \\frac{3}{2.322} \\approx 1.292$\nStep 1: Compute Term 1 (defense against interpolation attacks) $$ \\mathrm{Term} 1 = 1 + \\frac{\\min\\{128, 256\\}}{2.322} + 1.292 - 6 $$ $$ = 1 + \\frac{128}{2.322} + 1.292 - 6 $$ $$ \\approx 1 + 55.125 + 1.292 - 6 $$ $$ \\approx 51.417 $$ Step 2: Compute Term 2 ($r_{GB}$) The formula for $r_{GB}$ also contains a $\\max$ operation. We\u0026rsquo;ll calculate its main components separately (temporarily ignoring the unknown $r'_{GB}$ in the formula):\nFirst part of $r_{GB}$: $$ \\frac{\\min{\\log_2(p), \\kappa}}{\\log_2(\\alpha)} - r_F = \\frac{\\min\\{256, 128\\}}{2.322} - 6 \\approx 55.125 - 6 = 49.125 $$ Second part of $r_{GB}$:\n$\\log_{\\alpha}(2) = \\frac{\\log_2(2)}{\\log_2(5)} = \\frac{1}{2.322} \\approx 0.431$\n$\\min\\left\\{\\frac{\\kappa}{t+1}, \\frac{\\log_2(p)}{2}\\right\\} = \\min\\left\\{\\frac{128}{8+1}, \\frac{256}{2}\\right\\} = \\min\\{14.22, 128\\} = 14.22$\n$t - 1 + \\log_{\\alpha}(2) \\cdot (14.22) - r_F = 8 - 1 + 0.431 \\cdot 14.22 - 6 \\approx 7 + 6.13 - 6 = 7.13$\nTherefore, $r_{GB} = \\max{\\{49.125, 7.13\\}}$. We can see that $r_{GB}$ is at least $49.125$.\nStep 3: Final $r_P$ Calculation $$ r_P = \\max{\\{\\text{Term } 1\\},~~ r_{GB}} = \\max{\\{51.417, r_{GB}\\}} $$Since Term 1\u0026rsquo;s value ($51.417$) is greater than the calculated part of $r_{GB}$ ($49.125$), the final $r_P$ is determined by the defense requirements against interpolation attacks.\n$$ r_P \\approx 51.417 $$After rounding, we get $r_P = 52$.\nParameter Analysis: After verification, we can use the above calculation method as an assessment tool to evaluate the security of any set of analytical data. Below, we conduct security evaluations for the following two parameter sets.\nParameters: $t=2$, $d=1$, $c=0$, $α=5$, $\\kappa=128$, $p=381$\nPre-calculated common values:\n$\\min\\{\\kappa, \\log_2(p)\\} = \\min\\{128, 381\\} = 128$\n$\\log_2(\\alpha) = \\log_2(5) ≈ 2.322$\n$\\log_α(t) = \\log_5(2) = \\frac{\\log_2(2)} {\\log_2(5)} = 1 / 2.322 ≈ 0.4307$\n$\\log_α(2) = \\log_5(2) ≈ 0.4307$\nStep 1: Calculate $r_{GB}$ Calculation process:\nFirst sub-term of $r_{GB}$: $$ \\frac{\\min\\{\\log_2(p), \\kappa\\}}{\\log_2(\\alpha)} - r_F $$ $$ = \\frac{128}{2.322} - 6 $$ $$ \\quad\\quad\\quad~~~ \\approx 55.125 - 6 = 49.125 $$ Second sub-term of $r_{GB}$: $$t - 1 + \\log_{\\alpha}(2) \\cdot \\min\\left\\{ \\frac{\\kappa}{t+1}, \\frac{\\log_2(p)}{2} \\right\\} - r'_F$$First compute the $\\min{\\ldots}$ part:\n$$\\min\\left\\{ \\frac{128}{2+1}, \\frac{381}{2} \\right\\} = \\min\\{42.67, 190.5\\} = 42.67$$Then substitute into the full expression:\n$$ = 2 - 1 + 0.4307 \\cdot 42.67 - 3 $$ $$ \\approx 1 + 18.38 - 3 = 16.38 $$ Determine the Value of $r_{GB}$: $$ r_{GB} = \\max{\\{49.125, 16.38\\}} = 49.125 $$\nStep 2: Calculate the Final $r_P$ Now, $r_P$ is determined by taking the maximum of two other sub-terms and then rounding up.\nFirst sub-term of $r_P$ (resistance against algebraic attacks): $$ 1 + \\frac{\\min\\{\\kappa, \\log_2(p)\\}}{\\log_2(\\alpha)} + \\log_\\alpha(t) - r_F $$ $$ \\approx 1 + \\frac{128}{2.322} + 0.4307 - 6 $$ $$ \\approx 1 + 55.125 + 0.4307 - 6 $$ $$ = 56.5557 - 6 = 50.556 $$ Second sub-term of $r_P$ (resistance against GB attacks): This value is the $r_{GB}$ we calculated in the previous step. $$ r_{GB} = 49.125 $$ Determine the value of $r_P$: $$ r_P = \\lceil \\max\\{50.556, 49.125\\} \\rceil $$ $$ r_P = \\lceil 50.556 \\rceil $$ $$ r_P = 51 $$","permalink":"https://sidereushu.github.io/posts/poseidon-hash-algebraic-attack-analysis-designers-perspective/","summary":"\u003cbr\u003e\n\u003cp\u003eIn recent years, \u0026ldquo;ZK-friendly\u0026rdquo; hash functions designed for zero-knowledge proof (ZK) scenarios have gained widespread attention. They are typically based on carefully constructed algebraic structures to exhibit lower constraint counts in arithmetic circuits, thereby achieving higher efficiency in proof systems. However, it is precisely this algebraic friendliness that exposes them to potential attack surfaces under certain analytical models. Particularly in recent years, algebraic analysis methods—such as Gröbner basis attacks and polynomial degree reduction techniques utilizing subspace trajectories—have seen significant research interest in the cryptanalysis field and have gradually become one of the core tools for evaluating the security of such hash functions.\u003c/p\u003e","title":"Poseidon Hash Algebraic Attack Analysis (Designer's Perspective)"},{"content":"\nIn recent years, \u0026ldquo;ZK-friendly\u0026rdquo; hash functions designed for zero-knowledge proof (ZK) scenarios have gained widespread attention. They are typically based on carefully constructed algebraic structures to exhibit lower constraint counts in arithmetic circuits, thereby achieving higher efficiency in proof systems. However, it is precisely this algebraic friendliness that exposes them to potential attack surfaces under certain analytical models. Particularly in recent years, algebraic analysis methods—such as Gröbner basis attacks and polynomial degree reduction techniques utilizing subspace trajectories—have seen significant research interest in the cryptanalysis field and have gradually become one of the core tools for evaluating the security of such hash functions.\nThis article approaches from an analyst\u0026rsquo;s perspective, based on a recent paper [GKR25], presents some factual conclusions from that work, and provides supplementary analysis for certain conclusions.\nRelated Work Core Method Effectiveness Analysis for Poseidon [FP20] Closed-form degree expressions Helpful for modeling Gröbner basis attacks [BBLP22] Skipping (multiple) full rounds Reduced actual nonlinear depth [ABM24] Round-level Gröbner basis modeling Shows underestimated vulnerabilities at κ = 1024 [BBL+24] FreeLunch Gröbner basis attack Ineffective due to low S-box degree [KLR24] \u0026ldquo;Six Worlds\u0026rdquo; analysis framework Not yet applied to Poseidon, has potential research value [GKR25] Forward Gröbner basis attack utilizing subspace trajectories Original analysis shows overestimation or underestimation of required security rounds [BBB+25] Iterated Resultants Reducible to simple univariate cases [GKR25] is the latest work proposed by the Poseidon designers. This work presents a series of analytical data for Gröbner basis attacks on Poseidon2 in Sponge mode in Table 5, specifically based on a two-step analysis of GB steps (Macaulay bound) and FGLM steps (conjectured dI). The authors then derive the minimum partial rounds value $r_P$ that can be set in Table 3 based on this data.\nThe authors provide a series of analytical data for Poseidon2 in both Sponge mode and Compress mode (Table 5 and Table 6 respectively), but do not provide partial rounds settings similar to Table 3. Based on Table 6 data, combined with the Gröbner basis attack complexity formula $(d_{MAC})^{\\omega}$, and setting $\\omega$ to 2 as in Table 3, we perform the following calculation:\nTo achieve 128-bit security, we need:\n$$(d_{MAC})^2 \\geq 2^{128}$$Taking the square root of both sides, we get the requirement for the Macaulay Bound $d_{MAC}$:\n$$d_{MAC} \\geq 2^{64}$$Taking the logarithm, we get the security threshold we need:\n$$\\log_2(d_{MAC}) \\geq 64$$As long as $\\log_2(d_{MAC})$ under a parameter configuration is greater than or equal to 64, we can consider it secure under this attack.\nLet\u0026rsquo;s put this into practice and verify this result.\nVerification of Poseidon Authors\u0026rsquo; result Now, we select a set of parameters from the analytical data table (Table 5) provided by the authors, calculate the required minimum $r_{P}$ value using the \u0026ldquo;security threshold\u0026rdquo; method above, and verify whether this value matches the corresponding entry in Table 3. We select the second row of data from Table 5 as our example:\nState width: $t=4$ Digest length: $d=1$ Capacity: $c=1$ S-box: $α=3$ Security objective:\nThe security level we need to achieve is 128 bits. The attack complexity formula is $C_{GB}​≈(d_{MAC}​)^2$. Therefore, our security threshold is $\\log_{2}​(d_{MAC}​)≥64$. Calculation process:\nWhen $r_P = 1$: $\\log_2(d_{MAC}) = 17$ (\u0026lt; 64, insecure) When $r_P = 2$: $\\log_2(d_{MAC}) = 35$ (\u0026lt; 64, insecure) When $r_P = 3$: $\\log_2(d_{MAC}) = 41$ (\u0026lt; 64, insecure) When $r_P = 4$: $\\log_2(d_{MAC}) = 59$ (\u0026lt; 64, insecure) When $r_P = 5$: $\\log_2(d_{MAC}) = 113$ (\u0026gt; 64, secure) Conclusion: For the parameter set $t=4$, $d=1$, $c=1$, $α=3$, the minimum number of internal rounds $r_P$ required to resist the subspace attack described in the paper and achieve 128-bit security is 5. This calculated result matches the conclusion in Table 3.\nParameter Analysis for Compress Mode After verifying the calculation from Table 5, we can use the \u0026ldquo;security threshold\u0026rdquo; method as an assessment tool to evaluate the security of any set of analytical data. Below, we conduct security evaluations for two parameter sets in Compress mode based on the analytical data in Table 6.\nFirst Parameter Set: $t=2$, $d=1$, $c=0$, $α=5$ Setting security threshold:\nThe security level we need to achieve is 128 bits. The attack complexity formula is $C_{GB}​≈(d_{MAC}​)^2$. Therefore, our security threshold is $\\log_{2}​(d_{MAC}​)≥64$. Calculation process:\nAfter selecting the corresponding data from Table 6, we perform the following calculations:\nWhen $r_P = 1$: $\\log_2(d_{MAC}) = 13$ (\u0026lt; 64, insecure) When $r_P = 2$: $\\log_2(d_{MAC}) = 19$ (\u0026lt; 64, insecure) When $r_P = 3$: $\\log_2(d_{MAC}) = 25$ (\u0026lt; 64, insecure) When $r_P = 4$: $\\log_2(d_{MAC}) = 43$ (\u0026lt; 64, insecure) When $r_P = 5$: $\\log_2(d_{MAC}) = 61$ (\u0026lt; 64, insecure) When $r_P = 6$: $\\log_2(d_{MAC}) = 79$ (\u0026gt; 64, secure) Conclusion: For the parameter set $t=2$, $d=1$, $c=0$, $α=5$, 6 partial rounds are needed to elevate the complexity of the strongest Gröbner basis subspace optimization attack above the 128-bit security level.\nSecond Parameter Set: $t=3$, $d=1$, $c=0$, $α=5$ Setting security threshold:\nThe security level we need to achieve is 128 bits. The attack complexity formula is $C_{GB}​≈(d_{MAC}​)^2$. Therefore, our security threshold is $\\log_{2}​(d_{MAC}​)≥64$. Calculation process:\nAfter selecting the corresponding data from Table 6, we perform the following calculations:\nWhen $r_P = 1$: $\\log_2(d_{MAC}) = 9$ (\u0026lt; 64, insecure) When $r_P = 2$: $\\log_2(d_{MAC}) = 15$ (\u0026lt; 64, insecure) When $r_P = 3$: $\\log_2(d_{MAC}) = 27$ (\u0026lt; 64, insecure) When $r_P = 4$: $\\log_2(d_{MAC}) = 59$ (\u0026lt; 64, insecure) When $r_P = 5$: $\\log_2(d_{MAC}) = 87$ (\u0026gt; 64, secure) Conclusion: For the parameter set $t=3$, $d=1$, $c=0$, $α=5$, 5 partial rounds are needed to elevate the complexity of the strongest Gröbner basis subspace optimization attack above the 128-bit security level.\n","permalink":"https://sidereushu.github.io/posts/poseidon-hash-algebraic-attack-analysis-analysts-perspective/","summary":"\u003cbr\u003e\n\u003cp\u003eIn recent years, \u0026ldquo;ZK-friendly\u0026rdquo; hash functions designed for zero-knowledge proof (ZK) scenarios have gained widespread attention. They are typically based on carefully constructed algebraic structures to exhibit lower constraint counts in arithmetic circuits, thereby achieving higher efficiency in proof systems. However, it is precisely this algebraic friendliness that exposes them to potential attack surfaces under certain analytical models. Particularly in recent years, algebraic analysis methods—such as Gröbner basis attacks and polynomial degree reduction techniques utilizing subspace trajectories—have seen significant research interest in the cryptanalysis field and have gradually become one of the core tools for evaluating the security of such hash functions.\u003c/p\u003e","title":"Poseidon Hash Algebraic Attack Analysis (Analyst's Perspective)"},{"content":"\nWork Key Idea Effectiveness on Poseidon [FP20] Closed-form degree expression Useful for modeling GB attacks [BBLP22] Skipping (multiple) full rounds Reduces effective non-linear depth [ABM24] Round-level GB modeling Shows underestimated vulnerability at κ = 1024 [BBL+24] FreeLunch GB attacks Not effective due to low S-box degree [KLR24] \u0026ldquo;Six Worlds\u0026rdquo; framework Not yet applied; potential for future work [GKR25] Forward GB Attack Exploiting Subspace Trails original analysis under- or overestimates the number of rounds needed for security. [BBB+25] Iterated resultants Reduces to simple univariate case Key Points Evolution of Attack Methods: Beyond traditional analytical approaches such as statistical analysis, algebraic attacks (particularly Gröbner basis attacks) have been recognized by the academic community as significantly more effective and have become the primary research focus in recent years. Parameter Customization Challenges: Poseidon hash offers extensive customizable parameter space, which leads to issues of conservative security assumptions and potential overestimation. Current Research Status: The results provided by [ABM24] are excellent, but their acceptance in the community remains limited at present. In [GKR25], the authors present a comprehensive analysis of Gröbner basis attacks against Poseidon2 in Sponge mode through Table 5. This analysis is based on a two-step approach: the GB step (Macaulay bound) and the FGLM step (conjectured dI). Subsequently, the authors derive the minimum partial rounds values $r_P$ that can be configured, as shown in Table 3.\nNote: While the authors provide analysis data for both Sponge mode (Table 5) and Compress mode (Table 6), they only provide partial rounds settings similar to Table 3 for the Sponge mode. Based on the data from Table 6 and combining the Gröbner basis attack complexity formula $(d_{MAC})^{\\omega}$ with $\\omega = 2$ as used in Table 3, we perform the following calculations:\nSecurity Threshold Calculation To achieve 128-bit security, we need to satisfy:\n$$(d_{MAC})^2 \\geq 2^{128}$$Taking the square root of both sides, we get the requirement for the Macaulay Bound $d_{MAC}$:\n$$d_{MAC} \\geq 2^{64}$$Taking the logarithm, we obtain our required security threshold:\n$$\\log_2(d_{MAC}) \\geq 64$$Any parameter configuration where $\\log_2(d_{MAC})$ is greater than or equal to 64 can be considered secure against this attack.\nAnalysis from Table 5 Now, let\u0026rsquo;s examine Table 5 (a) GB step. This table demonstrates how various complexity indicators change when the number of partial rounds $r_P$ increases from 1 to 5, for fixed parameters $t$, $d$, $c$. We focus particularly on the Macaulay bound d_MAC row, where the values in parentheses represent the pre-calculated $\\log_2(d_{MAC})$ values.\nTaking the first row of Table 5 as an example, with parameters $t=3$, $d=1$, $c=1$, $\\ell=1$:\nWhen $r_P = 1$: $\\log_2(d_{MAC}) = 27$ (\u0026lt; 64, not secure) When $r_P = 2$: $\\log_2(d_{MAC}) = 33$ (\u0026lt; 64, not secure) When $r_P = 3$: $\\log_2(d_{MAC}) = 51$ (\u0026lt; 64, not secure) When $r_P = 4$: $\\log_2(d_{MAC}) = 87$ (\u0026gt; 64, secure) When $r_P = 5$: $\\log_2(d_{MAC}) = 105$ (\u0026gt; 64, secure) Conclusion For the parameter set $t=3$, $d=1$, $c=1$, $\\ell=1$, only 4 partial rounds are required to elevate the complexity of the most powerful Gröbner basis subspace optimization attack above the 128-bit security level.\n","permalink":"https://sidereushu.github.io/posts/poseidon2-hash-security-analysis--gr%C3%B6bner-basis-attack-evaluation/","summary":"\u003cbr\u003e\n\u003ctable\u003e\n  \u003cthead\u003e\n      \u003ctr\u003e\n          \u003cth\u003eWork\u003c/th\u003e\n          \u003cth\u003eKey Idea\u003c/th\u003e\n          \u003cth\u003eEffectiveness on Poseidon\u003c/th\u003e\n      \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e\u003cstrong\u003e[FP20]\u003c/strong\u003e\u003c/td\u003e\n          \u003ctd\u003eClosed-form degree expression\u003c/td\u003e\n          \u003ctd\u003eUseful for modeling GB attacks\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e\u003cstrong\u003e[BBLP22]\u003c/strong\u003e\u003c/td\u003e\n          \u003ctd\u003eSkipping (multiple) full rounds\u003c/td\u003e\n          \u003ctd\u003eReduces effective non-linear depth\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e\u003cstrong\u003e[ABM24]\u003c/strong\u003e\u003c/td\u003e\n          \u003ctd\u003eRound-level GB modeling\u003c/td\u003e\n          \u003ctd\u003eShows underestimated vulnerability at κ = 1024\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e\u003cstrong\u003e[BBL+24]\u003c/strong\u003e\u003c/td\u003e\n          \u003ctd\u003eFreeLunch GB attacks\u003c/td\u003e\n          \u003ctd\u003eNot effective due to low S-box degree\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e\u003cstrong\u003e[KLR24]\u003c/strong\u003e\u003c/td\u003e\n          \u003ctd\u003e\u0026ldquo;Six Worlds\u0026rdquo; framework\u003c/td\u003e\n          \u003ctd\u003eNot yet applied; potential for future work\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e\u003cstrong\u003e[GKR25]\u003c/strong\u003e\u003c/td\u003e\n          \u003ctd\u003eForward GB Attack Exploiting Subspace Trails\u003c/td\u003e\n          \u003ctd\u003eoriginal analysis under- or overestimates the number of rounds needed for security.\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e\u003cstrong\u003e[BBB+25]\u003c/strong\u003e\u003c/td\u003e\n          \u003ctd\u003eIterated resultants\u003c/td\u003e\n          \u003ctd\u003eReduces to simple univariate case\u003c/td\u003e\n      \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e\n\u003cbr\u003e\n\u003ch2 id=\"key-points\"\u003eKey Points\u003c/h2\u003e\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003eEvolution of Attack Methods\u003c/strong\u003e: Beyond traditional analytical approaches such as statistical analysis, algebraic attacks (particularly Gröbner basis attacks) have been recognized by the academic community as significantly more effective and have become the primary research focus in recent years.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eParameter Customization Challenges\u003c/strong\u003e: Poseidon hash offers extensive customizable parameter space, which \u003cstrong\u003eleads to issues of conservative security assumptions and potential overestimation\u003c/strong\u003e.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eCurrent Research Status\u003c/strong\u003e: The results provided by [ABM24] are excellent, but their acceptance in the community remains limited at present.\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eIn [GKR25], the authors present a comprehensive analysis of Gröbner basis attacks against Poseidon2 in Sponge mode through Table 5. This analysis is based on a two-step approach: the GB step (Macaulay bound) and the FGLM step (\u003ccode\u003econjectured dI\u003c/code\u003e). Subsequently, the authors derive the minimum partial rounds values $r_P$ that can be configured, as shown in Table 3.\u003c/p\u003e","title":"Poseidon2 Hash Security Analysis: Gröbner Basis Attack Evaluation"},{"content":"\nSuggested Parameters:\nParameter Suggested (Width 2) Suggested (Width 3) Notes Prime Field BLS12-381 BLS12-381 Newly supported field in Gnark Width t 2 3 Width 3 is preferred if Gnark supports it S-box $x^5$ $x^5$ Common and secure choice over prime fields Full Rounds 8 8 Avoid using fewer than 8 rounds Partial Rounds 22 14–17 Based on updated recommendations Mode Compress Mode Compress Mode Used for input compression in UTXO models Security Level ≥128 bits ≥128 bits Default setting meets the requirement Additional Notes Security Analysis: Considers recent advances in algebraic attacks such as Gröbner basis methods, [KR21], [BCD+20], [ABM23], and [GKR25].\nSimplified and updated design guidance: No longer uses $ x \\mapsto x^{-1} $ or the STARKAD family.\nClearer round number recommendations: Based on the latest cryptanalysis.\nEfficiency emphasis: Prime fields like BLS12-381 offer superior performance in ZKP systems such as Gnark.\n","permalink":"https://sidereushu.github.io/posts/practical-recommendations-for-poseidon2-+-bls12-381-+-compress-mode/","summary":"\u003cbr\u003e\n\u003cp\u003e\u003cstrong\u003eSuggested Parameters:\u003c/strong\u003e\u003c/p\u003e\n\u003ctable\u003e\n  \u003cthead\u003e\n      \u003ctr\u003e\n          \u003cth\u003eParameter\u003c/th\u003e\n          \u003cth\u003eSuggested (Width 2)\u003c/th\u003e\n          \u003cth\u003eSuggested (Width 3)\u003c/th\u003e\n          \u003cth\u003eNotes\u003c/th\u003e\n      \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n      \u003ctr\u003e\n          \u003ctd\u003ePrime Field\u003c/td\u003e\n          \u003ctd\u003eBLS12-381\u003c/td\u003e\n          \u003ctd\u003eBLS12-381\u003c/td\u003e\n          \u003ctd\u003eNewly supported field in Gnark\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eWidth \u003cem\u003et\u003c/em\u003e\u003c/td\u003e\n          \u003ctd\u003e2\u003c/td\u003e\n          \u003ctd\u003e3\u003c/td\u003e\n          \u003ctd\u003eWidth 3 is preferred if Gnark supports it\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eS-box\u003c/td\u003e\n          \u003ctd\u003e$x^5$\u003c/td\u003e\n          \u003ctd\u003e$x^5$\u003c/td\u003e\n          \u003ctd\u003eCommon and secure choice over prime fields\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eFull Rounds\u003c/td\u003e\n          \u003ctd\u003e8\u003c/td\u003e\n          \u003ctd\u003e8\u003c/td\u003e\n          \u003ctd\u003eAvoid using fewer than 8 rounds\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003ePartial Rounds\u003c/td\u003e\n          \u003ctd\u003e22\u003c/td\u003e\n          \u003ctd\u003e14–17\u003c/td\u003e\n          \u003ctd\u003eBased on updated recommendations\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eMode\u003c/td\u003e\n          \u003ctd\u003eCompress Mode\u003c/td\u003e\n          \u003ctd\u003eCompress Mode\u003c/td\u003e\n          \u003ctd\u003eUsed for input compression in UTXO models\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eSecurity Level\u003c/td\u003e\n          \u003ctd\u003e≥128 bits\u003c/td\u003e\n          \u003ctd\u003e≥128 bits\u003c/td\u003e\n          \u003ctd\u003eDefault setting meets the requirement\u003c/td\u003e\n      \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e\n\u003chr\u003e\n\u003ch3 id=\"additional-notes\"\u003eAdditional Notes\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eSecurity Analysis\u003c/strong\u003e: Considers recent advances in algebraic attacks such as Gröbner basis methods, [KR21], [BCD+20], [ABM23], and [GKR25].\u003c/p\u003e","title":"Practical Recommendations (for Poseidon2 + BLS12-381 + Compress Mode)"},{"content":"\nIn modern cryptography, particularly in zero-knowledge proofs and high-performance hash designs (such as Poseidon, Rescue, and Griffin), one fundamental building block appears repeatedly: the cryptographic sponge function.\nIt not only offers an elegant absorb-and-squeeze paradigm but also strikes a principled balance between security and efficiency.\nI. What is a Sponge Function? A sponge is a flexible cryptographic structure used to absorb an input of arbitrary length and squeeze out a fixed or extensible output. Its power lies in its simplicity, relying only on a single state and a permutation function.\nThe internal state size is $b = r + c$, where:\n$r$: the rate, the portion of the state used for input/output;\n$c$: the capacity, the hidden part that ensures security.\nThe execution process consists of two phases:\nAbsorbing Phase\nThe input message is split into $r$-bit blocks. Each block is XORed into the first $r$ bits of the state, followed by applying a permutation $f$.\nSqueezing Phase\nOutput is read from the first $r$ bits. After each extraction, $f$ is applied again until the desired output length is achieved.\nThis process mimics the idea of \u0026ldquo;soaking a sponge with water and squeezing it out\u0026rdquo; — the internal state acts as a container that processes and diffuses the message before producing the final output.\nII. Generic Attacks and Indifferentiability In cryptographic analysis, we often distinguish between:\nGeneric attacks: leverage only the sponge framework itself, not the structure of the permutation.\nStructural attacks: exploit specific algebraic properties or implementation details of the permutation $f$.\nSponge constructions are notable for offering formal guarantees against generic attacks. To argue this formally, we use:\nThe Indifferentiability Framework This is a simulation-based model aiming to prove:\n“If an attacker can distinguish the sponge from a random oracle, then they must also be able to distinguish the internal permutation $f$ from a truly random one.”\nThus, if the permutation $f$ behaves like a random permutation, the entire sponge behaves like a random oracle.\nIII. What is an Externally Visible Weakness? In the indifferentiability model, only input-output behavior is visible to an attacker. Any internal details — including the state — are hidden. Therefore, only the following constitute valid attacks:\nOutput collisions: distinct inputs lead to the same output.\nSecond preimage attacks: given a hash value, find another message mapping to it.\nLength extension attacks: predict $H(M | X)$ based on $H(M)$.\n✅ Internal collisions or state coincidences are not considered attacks — unless they become externally observable.\nThis principle significantly simplifies security analysis: we only need to reason about observable behavior.\nIV. How to Claim Security in the Random Sponge Model? Just as we use idealized models like random oracles or ideal ciphers for conventional primitives, we can model sponges via:\nThe Random Sponge — a theoretical sponge behaving like a random function.\nThus, a sponge construction is secure if it is indifferentiable from a random sponge — i.e., if no adversary can distinguish it through queries alone.\nExample of a Security Claim: “We assert that our sponge-based design is secure if it does not exhibit any externally visible weaknesses that would not appear in a random sponge.”\nIn other words:\nIf an attacker can distinguish the implementation from an ideal sponge via I/O interactions alone, the construction is insecure. V. Hermetic Sponge Strategy: Black-Box Security The original Keccak paper described a sponge design philosophy that underpins its provable security:\n“Build a sponge on top of a permutation $f$ which itself is cryptographically strong, and claim that any attack better than $2^{c/2}$ implies a structural weakness in $f$.”\nThis is called the:\nHermetic Sponge Strategy The core idea is to seal the permutation inside a black box, and reduce the sponge’s security to whether the box behaves like a random oracle. Specifically:\nOnly the rate part ($r$ bits) is ever exposed;\nThe capacity ($c$ bits) remains fully hidden;\nIf an attack works better than $2^{c/2}$, it must exploit the permutation\u0026rsquo;s structure.\nThis is in contrast to classic constructions like Merkle–Damgård, which build security through incremental compression. The sponge instead assumes non-observability and indistinguishability as its defense mechanism.\nNote: Hermetic Sponge assumes an ideal world with no side-channel visibility. It doesn\u0026rsquo;t cover power, timing, or cache attacks.\nVI. Collisions and Second Preimage Attacks Sponge functions offer different security bounds depending on how they\u0026rsquo;re parameterized:\n1. Output Collisions The goal is to find $M_1 \\ne M_2$ such that:\n$H(M_1)=H(M_2)$\nIf the output length is $n \u003c c$, the attack complexity is approximately $2^{n/2}$.\n2. Internal Collisions The attacker aims to create two different messages that end in the same internal state after absorption;\nThis leads to the same output when squeezed;\nComplexity is roughly $2^{c/2}$.\nComparison: If $n \u003c c$, output collisions are easier;\nIf $n \\geq c$, it’s better to aim for inner collisions.\nThis is why in practice, designers often ensure:\nCapacity $c ≥ 2$ × output length $n$\nVII. Throughput vs. Security: Balancing r and c The sponge’s performance and security depend on how we divide the state:\n$b = r + c$: total internal state is fixed;\nIncreasing $r$: improves throughput;\nIncreasing $c$: improves security (resistance to generic attacks).\nTypical settings:\nGoal Recommended Setting Explanation 128-bit security $c = 256$, $r = b - 256$ Collision resistance ≈ $2^{128}$ High throughput Increase $r$, reduce $c$ Favor speed over generic security High security Use $c \\gg n$ Avoid inner collisions and extension This trade-off is critical in the parameterization and round-count design of sponge-based hashes like Poseidon and Griffin.\nVIII. Conclusion: Sponge as a Philosophy Sponge functions are not merely algorithmic tricks — they embody a design philosophy rooted in observability and compositional soundness.\nSecurity isn\u0026rsquo;t built from compression chaining, but from black-box indistinguishability;\nThe sponge doesn\u0026rsquo;t try to block every attack, but instead limits what the attacker can see;\nProvable security derives from indifferentiability, not ad-hoc resistance mechanisms.\nThis is why sponge functions underpin Keccak (SHA-3), Poseidon (ZK-friendly hashing), Rescue (algebraic soundness), and other modern cryptographic constructions.\n","permalink":"https://sidereushu.github.io/posts/cryptographic-sponge-functions--the-foundation-of-zk-friendly-hash-constructions/","summary":"\u003cbr\u003e\n\u003cp\u003eIn modern cryptography, particularly in zero-knowledge proofs and high-performance hash designs (such as Poseidon, Rescue, and Griffin), one fundamental building block appears repeatedly: the \u003cstrong\u003ecryptographic sponge function\u003c/strong\u003e.\u003cbr\u003e\nIt not only offers an elegant absorb-and-squeeze paradigm but also strikes a principled balance between \u003cstrong\u003esecurity and efficiency\u003c/strong\u003e.\u003c/p\u003e\n\u003cbr\u003e\n\u003cp\u003e\u003cimg alt=\"SpongeFunc\" loading=\"lazy\" src=\"/images/SpongeFunc.png\"\u003e\u003c/p\u003e\n\u003cbr\u003e\n\u003chr\u003e\n\u003ch3 id=\"i-what-is-a-sponge-function\"\u003eI. What is a Sponge Function?\u003c/h3\u003e\n\u003cp\u003eA \u003cstrong\u003esponge\u003c/strong\u003e is a flexible cryptographic structure used to absorb an input of arbitrary length and squeeze out a fixed or extensible output. Its power lies in its simplicity, relying only on a single state and a permutation function.\u003c/p\u003e","title":"Cryptographic Sponge Functions: The Foundation of ZK-Friendly Hash Constructions"},{"content":"\nIn the competition of Web3 infrastructure, an increasing number of projects claim to be building \u0026ldquo;the TCP/IP of Web3.\u0026rdquo; Among these, Zero-Knowledge (ZK) proof technology has gained significant attention due to its powerful verification capabilities, with many Layer 0 projects positioning ZK proofs as their core competitive advantage. However, we need to think deeply: Are ZK proofs truly the essence of Layer 0?\nLet us approach this question from a more fundamental perspective.\nReimagining the Nature of Layer 0 When we talk about Layer 0, what are we actually discussing? It is not an execution engine, nor is it merely a consensus mechanism. The true value of Layer 0 lies in trust abstraction — providing a dependable trust foundation for upper-layer applications while encapsulating complex cross-chain operations, state synchronization, and verification mechanisms within the protocol layer.\nMuch like the Internet\u0026rsquo;s TCP/IP protocol stack, Layer 0 needs to solve a core problem: how to establish a reliable communication and state transfer mechanism in a network environment filled with uncertainty. The key concepts here are \u0026ldquo;abstraction\u0026rdquo; and \u0026ldquo;trust,\u0026rdquo; rather than specific technical implementations.\nThe Limitations of ZK Proofs ZK proofs indeed possess powerful verification capabilities, enabling the proof of computational correctness without revealing information. However, when we consider them as the sole technical path for Layer 0, we encounter several practical challenges.\nFirst is the computational cost. ZK proof protocols are computationally intensive, requiring substantial processing power and time, which may be impractical for real-time, high-throughput applications. Every state transition requires generating ZK proofs, creating serious performance bottlenecks in scenarios involving high-frequency trading or numerous state updates. While mitigation strategies include efficient implementation techniques, optimized cryptographic algorithms, and advances in hardware acceleration, the computationally intensive nature of ZK proofs means they cannot serve as a universal solution for all scenarios.\nSecond is the flexibility constraint. Pure ZK systems often require predefined rules and circuits for state transitions, which poses a serious limitation for Layer 0 systems that need rapid iteration and adaptation to different application scenarios.\nMost importantly, ZK proofs solve verification problems, not trust problems. Even if we can perfectly prove the correctness of every state transition, we still need to address more fundamental questions: how to enable different chains and applications to trust each other and collaborate effectively.\nUnderstanding the Limitations of TCP/IP and the Complexity of Web3 When we discuss learning design philosophy from TCP/IP, we need to recognize an important reality: the traditional TCP/IP protocol stack was born in the 1970s, primarily solving connectivity problems — how to enable different computer networks to communicate reliably with each other. TCP/IP\u0026rsquo;s design assumptions were based on a relatively trustworthy environment where network nodes were fundamentally trustworthy or at least had clear administrative authority.\nHowever, Web3 faces entirely different challenges. Web3 must not only solve connectivity problems but also address trust issues in a completely decentralized environment without authoritative institutions. This is like being in a room full of strangers where you need not only to communicate but also to verify identities, securely exchange goods, collaborate on tasks, and maintain fair payment mechanisms.\nIf TCP/IP solved the problem of \u0026ldquo;how to build roads,\u0026rdquo; then Web3 needs to solve the problem of \u0026ldquo;how to establish a complete socio-economic system without government.\u0026rdquo; This system requires coordination across multiple dimensions including identity authentication, property rights, contract execution, and dispute resolution. The Multi-dimensional Challenges of Web3 Protocol Stack Let us examine the complexity of Web3 infrastructure from a more comprehensive perspective:\nModule Objective Existing Projects Maturity Level Connectivity Gossip networks, P2P, secure routing libp2p, waku, quic ✅ Relatively mature Decentralized Identity No reliance on CA, no reliance on PKI ENS, DID, Ethereum Name Service ⚠️ Early stage Decentralized Storage Content addressing, censorship resistance, verifiability IPFS, Arweave, Ceramic ⚠️ Semi-mature, high cost Decentralized Computing Verifiable execution, off-chain computation, ZK verification RISC Zero, zkWASM, Aleo ⚠️ Very early stage Decentralized Payments State transitions + settlement + zkPay Lightning, StarkNet, Aztec ✅ Initial framework Proof Protocol Layer (ZK) Composable, recursive, security layer Plonky2, Halo2, Nova ✅ Mature but heavy This table reveals that genuine Web3 infrastructure faces multi-dimensional challenges. The connectivity layer is already relatively mature because it most closely resembles the problems traditional TCP/IP aimed to solve. The decentralized identity layer remains in early stages, reflecting the fundamental challenge of establishing trustworthy identity systems without central authority. The decentralized storage layer is semi-mature but costly, indicating that technical feasibility has been proven but economic models require optimization. The decentralized computing layer is very early stage because verifiable computation involves complex cryptographic proofs. The proof protocol layer is mature but heavy, which perfectly validates our point about ZK proofs — the technology itself is relatively mature, but computational costs remain high.\nThe Hybrid Model of Trust Abstraction Based on our understanding of Web3\u0026rsquo;s complexity, a genuine Layer 0 should adopt a hybrid trust model, providing different trust guarantee mechanisms according to various application scenarios and security requirements. This design approach can draw inspiration from LayerZero V2\u0026rsquo;s architecture.\nLayerZero\u0026rsquo;s architecture is modular at the verification level while remaining static at the transport level. This design achieves a crucial balance between current performance and future-oriented design. Any entity capable of verifying cross-chain data packets can join LayerZero as a Decentralized Verification Network (DVN), thereby avoiding vendor lock-in at the security level.\nThe core principle of this design philosophy is: different messages and state transitions can choose different verification mechanisms. For high-value cross-chain asset transfers, ZK proofs can provide the strongest security guarantees. For low-risk data synchronization or state queries, optimistic verification or multi-signature mechanisms can be used to reduce costs. For applications requiring rapid response, instant confirmation mechanisms can be provided with asynchronous verification running in the background.\nIt is worth noting that practical solutions are being explored to address ZK proof computational costs. For example, Polyhedra\u0026rsquo;s zkLightClient directly addresses LayerZero V2\u0026rsquo;s challenge through proof batching. Polyhedra\u0026rsquo;s zkLightClient significantly reduces on-chain verification costs and latency by compressing multiple transaction verifications into a single ZKP. This batching technique demonstrates how to maintain ZK proof security while solving performance bottlenecks through engineering optimization.\nConcrete Implementation of the Hybrid Model Based on this approach, a genuine Layer 0 system should comprise several layers:\nCore State Layer: For account balances, critical state transitions, and other core data, ZK proofs ensure absolute security. This data has relatively low update frequency but requires the highest security standards.\nExtended Interaction Layer: For inter-application message passing, governance voting, data publishing, and other operations, configurable verification mechanisms are provided. Applications can choose ZK proofs, optimistic verification, or hybrid modes based on their specific needs.\nRouting Coordination Layer: Responsible for coordination between different verification mechanisms, ensuring system-wide consistency and reliability. This layer\u0026rsquo;s design resembles the routing layer in network protocol stacks, handling load balancing and fault recovery.\nDesign Proposal for Hybrid Model When considering a hybrid model, we can reference real hybrid chain design experiences and adopt the following structure:\nZK Core + Optimistic Side Path Architecture: This architecture\u0026rsquo;s core concept is maintaining the strongest security guarantees while providing optimization paths for high-frequency interaction scenarios. The default path enters the state engine through signature verification, account nonce checking, and snapshot mechanisms, enabling rapid response to most transaction requests. Simultaneously, the system asynchronously generates ZK proofs, marking them as \u0026ldquo;strong security state blocks\u0026rdquo; to ensure absolute safety of critical states.\nMore importantly, this architecture provides a flexible query interface through the query_proof(hash) mechanism, allowing users or other chains to verify the correctness of any state transition on demand. This \u0026ldquo;proof on demand\u0026rdquo; design philosophy solves a crucial problem: not all state transitions require real-time ZK proofs, but all state transitions should be verifiable.\nThis design\u0026rsquo;s advantage lies in finding the optimal balance between performance and security. If Layer 0 is positioned as \u0026ldquo;the final trust anchor for consensus, state, and transfers,\u0026rdquo; then more ZK proofs are indeed better because they provide the strongest security guarantees. However, if Layer 0 needs to support high-frequency interactions and low-latency experiences, then hybrid strategies become crucial. By introducing the \u0026ldquo;proof on demand + optimistic path\u0026rdquo; combination mechanism, the system can maintain the highest security standards while meeting real-world application performance requirements.\nUnderstanding Trust Abstraction Evolution Through LayerZero In LayerZero V2, oracles and relayers have been replaced by Decentralized Verification Networks (DVNs) and permissionless executors. DVNs are responsible for verifying message accuracy before message delivery and correct execution on target chains.\nThis evolution embodies an important design philosophy: separating verification from execution. LayerZero addresses the resource-intensive nature of developing and updating external security code by separating verification from execution. Any code not critical to security is moved to separate components — executors — which can run permissionlessly and remain isolated from verification logic.\nThis separation not only improves system flexibility but, more importantly, provides applications with choice. LayerZero endpoints provide stable application-facing interfaces, lossless network channel abstractions, exactly-once delivery guarantees, and manage OApp security stacks. Endpoint immutability ensures long-term channel validity through enforced update isolation, configuration ownership, and channel integrity.\nTrade-offs in Practical Applications Let us return to practical application scenarios to understand the value of this design. Suppose we are building a cross-chain DeFi application:\nFor large-value asset transfers, we need the highest level of security guarantees, making ZK proofs the optimal choice. Even with high computational costs, this is acceptable relative to asset value.\nFor price information synchronization, we need speed and timeliness, allowing us to use optimistic verification mechanisms and initiate ZK proof processes only when disputes arise.\nFor governance voting, we need transparency and tamper-resistance, allowing us to use commit-reveal mechanisms combined with delayed ZK verification.\nFor user behavior data, we need privacy protection and batch processing, allowing us to use recursive ZK proofs to amortize computational costs.\nDesign Philosophy of Modular Protocol Ecosystem The success of traditional TCP/IP protocol stacks lies in providing a layered, modular, and scalable architecture where each layer has clearly defined responsibility boundaries, upper layers need not concern themselves with lower-layer implementations, and lower layers provide abstract service interfaces for upper layers.\nHowever, Web3\u0026rsquo;s complexity far exceeds traditional internet requirements. Web3\u0026rsquo;s trust abstraction encompasses not only transaction verification but also identity authentication, data storage, computational execution, payment settlement, and multiple other dimensions. Each dimension has its own maturity level and challenges requiring coordinated development. Therefore, Web3\u0026rsquo;s Layer 0 needs to adopt modular composition thinking, not merely layered abstraction.\nAs we can see from our protocol stack analysis, the true Web3 TCP/IP is not just \u0026ldquo;one protocol\u0026rdquo; but rather a modular, composable protocol ecosystem. Different application scenarios require different trust guarantees, storage methods, computational models, and payment mechanisms.\nSimilarly, Web3\u0026rsquo;s Layer 0 should follow this design philosophy. LayerZero enables developers to create and configure unified applications, tokens, and data primitives — regardless of chain — through arbitrary message transmission, much like TCP/IP standardized internet communication.\nThe key to this standardization lies not in forcing the use of specific verification mechanisms but in providing unified interfaces and abstractions. Developers can focus on business logic while delegating complex cross-chain communication, state synchronization, and security verification to the protocol layer.\nTechnical Implementation of Hybrid Model From a technical implementation perspective, a hybrid model Layer 0 system needs to address several key issues:\nState Management: How to maintain state consistency across different verification mechanisms? This requires designing a unified state mechanism capable of handling different types of state updates and verification results.\nRouting Mechanism: How to automatically select appropriate verification paths based on message types and security requirements? This requires implementing intelligent routing algorithms that can balance security, cost, and performance.\nDispute Resolution: How to arbitrate and resolve conflicts when different verification mechanisms produce contradictory results? This requires designing layered dispute resolution mechanisms that ultimately fall back to the most secure verification methods.\nPerformance Optimization: How to maximize system throughput and response speed while ensuring security? This requires implementing various optimization techniques such as batching, parallel verification, and precomputation.\nConclusion: The Future of Trust Abstraction ZK proofs are an important technology providing Web3 with powerful verification capabilities. However, treating them as the sole technical path for Layer 0 represents limited thinking. A genuine Layer 0 should be a trust abstraction layercapable of providing the most appropriate trust guarantee mechanisms according to different application needs and scenarios.\nThis hybrid model design can not only fully leverage ZK proof advantages but also provide greater flexibility and scalability for the entire Web3 ecosystem. Just as TCP/IP protocol stacks achieved internet prosperity through layered and modular design, Web3\u0026rsquo;s Layer 0 should also establish a solid foundation for next-generation decentralized applications through trust abstraction and hybrid models.\nIn this process, we need to shift from technical implementation perspectives to protocol design perspectives, from single solutions to ecosystem construction, and from technology-first to user experience-first approaches. Only in this way can we truly build the \u0026ldquo;TCP/IP\u0026rdquo; of the Web3 world.\nComprehensively speaking, building genuine Web3 infrastructure requires us to grasp several key points:\nRecognize that Web3 complexity far exceeds traditional internet requirements: Traditional TCP/IP only solved connectivity problems, while Web3 needs to solve multi-dimensional trust problems including identity, storage, computation, and payments in a completely decentralized environment.\nZK proofs are not the essence of Layer 0; trust abstraction is: True value lies in providing the most appropriate trust guarantee mechanisms according to different application needs, rather than forcing the use of a single technical path.\nBuild a modular, composable protocol ecosystem: Web3\u0026rsquo;s Layer 0 should be a protocol ecosystem capable of flexibly combining different trust mechanisms, storage solutions, computational models, and payment methods.\nLearn from LayerZero V2\u0026rsquo;s practical experience: Through decentralized verification networks and verification-execution separation, a good balance has been achieved between modular security and protocol stability.\nOnly by deeply understanding these complexities can we truly build infrastructure suitable for the Web3 era, establishing a solid foundation for the prosperity of decentralized applications.\n","permalink":"https://sidereushu.github.io/posts/zk-proofs-are-not-the-essence-of-layer-0-trust-abstraction-is/","summary":"\u003cbr\u003e\n\u003cp\u003eIn the competition of Web3 infrastructure, an increasing number of projects claim to be building \u0026ldquo;the TCP/IP of Web3.\u0026rdquo; Among these, Zero-Knowledge (ZK) proof technology has gained significant attention due to its powerful verification capabilities, with many Layer 0 projects positioning ZK proofs as their core competitive advantage. However, we need to think deeply: Are ZK proofs truly the essence of Layer 0?\u003c/p\u003e\n\u003cp\u003eLet us approach this question from a more fundamental perspective.\u003c/p\u003e","title":"ZK Proofs Are Not the Essence of Layer 0, Trust Abstraction Is"},{"content":"\n\u0026ldquo;The internet was designed to be open, but the platforms built on top of it are not.\u0026rdquo;\n——Chris Dixon, Rebooting the Internet\n0. From Open Web1 to Centralized Web2: The Legacy of Missing Trust The Web1 era began with openness. Born out of academic and military collaboration, the TCP/IP protocol stack laid the foundation for global connectivity. TCP/IP was — and remains — an open and permissionless stack: any device following the protocol can join the network. This property of permissionless connectivity created the early decentralized flavor of the Internet.\nBut as the Internet commercialized, new demands emerged: identity, content, value transfer. We soon realized that TCP/IP solved the problem of connectivity, but not the problem of trust.\nFor example:\nHow do users authenticate their identities? Who owns or controls the data? How can cross-site coordination be trusted? How do we prevent double-spending or denial in financial interactions? Because trust was never embedded into the network stack, applications were forced to implement it themselves. This gap was filled by centralized platforms. Thus, Web2 emerged — an architecture where platforms manage identities, content, and transaction integrity, becoming central custodians of user data and trust.\n1. TCP/IP: The Backbone of the Open Internet TCP/IP is the foundational communication protocol suite of the Internet. It is organized as a layered architecture:\nTCP/IP Layer OSI Equivalent Key Protocols Role Application Application HTTP, DNS High-level application logic Transport Transport TCP, UDP End-to-end transmission Network Network IP, ICMP Routing and addressing Link Data/Physical Ethernet, Wi-Fi Physical transmission The stack’s key advantage is abstraction and decoupling: each layer only depends on the layer below. This modularity enabled the Internet’s rapid expansion.\nHowever, the TCP/IP philosophy is based on best-effort delivery:\nThe lower layers do not guarantee delivery; higher layers are responsible for reliability.\nFor example, the IP layer may drop or reorder packets; TCP adds retransmission and reordering to give the illusion of reliability. In Web2, this is acceptable because platforms have centralized control. But Web3 does not have that luxury.\n2. Web3 Demands More Than “Best Effort” — It Demands Cryptographic Guarantees Web3 aims to build not an Internet of information, but an Internet of value. That means communication isn\u0026rsquo;t just about moving bytes, but about moving state, value, commitments — all in a verifiable, tamper-resistant, trust-minimized way.\nWhy TCP/IP Falls Short for Web3 Core Web3 Requirement Can TCP/IP Provide It? Verifiable state transitions ❌ Stateless Decentralized identity ❌ No identity layer Cross-chain communication ❌ No context awareness Zero-knowledge proof transport ❌ No crypto semantics Permissionless composability ❌ No execution model Censorship-resistant P2P messaging ❌ No built-in resistance In short, TCP/IP may be the physical substrate, but it is not Web3-native. Today, much of Web3 still relies on centralized infrastructure for critical operations:\nWallets interact with chains via centralized RPC providers (e.g., Infura) IPFS content often gets routed through centralized gateways Smart contract calls are initiated by centralized frontends Web3 today is still a Web3 emulator running on Web2 protocols.\n3. A Comparative Stack: Web2 vs Web3 Protocol Layers To reframe Web3’s architecture, we can draw an analogy with TCP/IP:\nInformation Internet Protocol Layer Value Internet (Web3) Protocol Layer IP addresses IP (Network Layer) Blockchain accounts Layer 1 state machines Packet transport TCP / UDP (Transport) Rollups / Channels Layer 2 state extensions Website access HTTP / DNS (App Layer) DApps, NFTs, DAOs Application protocols (ERC-20/721) What’s missing from Web3 is not just more dApps — it’s a native backbone layer, a \u0026ldquo;TCP/IP-equivalent\u0026rdquo; for trust, identity, and value. We need a Layer 0 purpose-built for Web3.\n4. Why Didn’t TCP/IP Include This? When TCP/IP was developed in the 1970s, it was designed to solve the problem of reliable connectivity, not trust. Many technologies needed for decentralized trust simply didn’t exist yet:\nCapability Missing in TCP/IP Era Web2 Replacement Decentralized identity No public-key addresses / DID Email + password (OAuth) Decentralized storage No IPFS / Filecoin AWS / GCP Decentralized compute No zkVM / decentralized VMs Serverless / Cloud Decentralized payments No cryptocurrencies / smart contracts Credit card / PayPal So although TCP/IP was open and permissionless, the application layer had no choice but to rely on centralized platforms to provide closed-loop services. The result was Web2 — an ecosystem where:\nUsers generate data Platforms own data Algorithms control users This model has been called data feudalism — users become digital serfs working the land owned by platforms.\n5. Web3 Is a Return to TCP/IP’s Spirit — Not a Rejection Web3 is not trying to overthrow the Internet — it’s trying to complete what TCP/IP never finished: embedding trust into the stack.\nWe can envision a new decentralized protocol stack:\nLayer Web2 Implementation Web3 Equivalent Communication TCP/IP TCP/IP (shared) Structure JSON / SQL Merkle Trees / SMTs Identity OAuth (Google, Facebook) Public-key addresses / DIDs Storage AWS / Cloud IPFS / Arweave / Filecoin Compute Cloud Functions EVM / zkVM / WASM + ZK Consensus Centralized databases Blockchain (PoW / PoS / BFT) Application Web2 platforms DApps / DAOs / DeFi / NFTs Web3’s mission is to use cryptography + distributed systems to build native, verifiable protocols for trust and computation.\n6. So What Should Web3’s TCP/IP Look Like? We need a Web3-native Layer 0 stack that transmits not just packets, but:\nVerifiable state snapshots Censorship-resistant messages Zero-knowledge proofs and public-key identities Deployable, composable contracts Cross-chain state interoperability Projects like Celestia, Eigenlayer, and IBC are exploring this territory — building the \u0026ldquo;TCP/IP for the Internet of Value\u0026rdquo;.\n7. Web3’s “New TCP/IP” Isn’t a Rebellion — It’s a Foundation Web1 gave us protocol freedom\nWeb2 gave us platform convenience\nWeb3 seeks to reclaim the protocol layer — without relying on the platform\nWeb3 doesn’t aim to replace TCP/IP — it aims to augment it with a native, trust-aware state layer.\nThis new \u0026ldquo;connectivity\u0026rdquo; layer won’t just route data packets — it will route state, trust, value, and cryptographic commitments. We are no longer content with “best-effort”. What we demand is proof-driven delivery.\nAnd that is the foundational infrastructure Web3 is still missing.\n","permalink":"https://sidereushu.github.io/posts/web3s-missing-foundation--why-it-needs-a-new-tcp-ip/","summary":"\u003cbr\u003e\n\u003cblockquote\u003e\n\u003cp\u003e\u003cstrong\u003e\u0026ldquo;The internet was designed to be open, but the platforms built on top of it are not.\u0026rdquo;\u003c/strong\u003e\u003cbr\u003e\n——Chris Dixon, \u003cem\u003eRebooting the Internet\u003c/em\u003e\u003c/p\u003e\u003c/blockquote\u003e\n\u003cbr\u003e\n\u003ch3 id=\"0-from-open-web1-to-centralized-web2-the-legacy-of-missing-trust\"\u003e0. From Open Web1 to Centralized Web2: The Legacy of Missing Trust\u003c/h3\u003e\n\u003cp\u003eThe Web1 era began with openness. Born out of academic and military collaboration, the TCP/IP protocol stack laid the foundation for global connectivity. TCP/IP was — and remains — an \u003cstrong\u003eopen and permissionless stack\u003c/strong\u003e: any device following the protocol can join the network. This property of \u003cem\u003epermissionless connectivity\u003c/em\u003e created the early decentralized flavor of the Internet.\u003c/p\u003e","title":"Web3’s Missing Foundation - Why It Needs a New TCP/IP"},{"content":"\nTimeline for Post-Quantum Migration According to analysis by Chaincode Labs, Bitcoin’s transition to post-quantum cryptography (PQC) can follow two main strategies: a short-term contingency plan (cf. Figure 1) and a long-term comprehensive path (cf. Figure 2).\nThe short-term strategy focuses on deploying a basic quantum-resistant option within 1 to 2 years, offering a fallback mechanism in case cryptographically relevant quantum computers (CRQCs) emerge sooner than expected. This involves proposing a minimal PQC signature scheme through a BIP, implementing it in Bitcoin Core, and enabling voluntary migration of vulnerable UTXOs. While not optimized for all use cases, it provides immediate protection for at-risk users and critical institutions. Success depends on close coordination across the technical community and early involvement from major Bitcoin holders.\nThe long-term strategy envisions a broader research and development process to design Bitcoin’s optimal quantum-resistant future. This includes evaluating different PQC algorithms, mitigating their performance trade-offs, and developing robust UTXO migration mechanisms. Drawing from historical upgrades like SegWit and Taproot, which took several years to reach wide adoption, this path could span up to a decade.\nChaincode emphasizes that these two strategies are not mutually exclusive. The short-term plan offers immediate protection and preparedness, while the long-term plan focuses on sustainability, optimization, and ecosystem-wide adoption in the face of evolving quantum threats.\nFigure 1. Timeline of estimate to establish short-term contingency measures.\nFigure 2. Timeline for comprehensive quantum resistance path.\nGovernment Post-Quantum Plans and Timelines Governments worldwide are actively responding to the potential massive disruption posed by quantum computers to existing cryptographic systems by formulating strategies for migration to post-quantum cryptography (PQC).\nAs of mid-2025, more than 15 countries and regions have issued official guidance on PQC transition. Most follow the NIST (U.S. National Institute of Standards and Technology) standardization process, while some develop indigenous algorithms independently. The general timeline for transition completion spans 2027 to 2035, with 2030 considered a critical milestone.\nUnited States The U.S. has established the most comprehensive plan, including:\nNational Security Memorandum 10 (May 2022): Signed by President Biden, titled “Advancing America’s Leadership in Quantum Computing While Mitigating Risks to Vulnerable Cryptographic Systems”.\nThe goal is to mitigate quantum risks as much as possible by 2035.\nNIST Transition Deadlines:\nPhasing out RSA-2048 and ECC-256 by 2030, with complete prohibition by 2035, while allowing hybrid classical-PQC schemes during the transition.\nNSA’s CNSA 2.0 Suite:\nRequires software and network equipment upgrades by 2030, with browsers and operating systems completing upgrades by 2033.\nPresidential Executive Order 14144 (January 2025):\n“Strengthening National Cybersecurity Innovation” emphasizes collaboration with other nations and industry to encourage adoption of NIST’s PQC standards.\nUnited Kingdom The UK’s National Cyber Security Centre (NCSC), part of GCHQ intelligence agency, published guidance in March 2025 outlining a three-phase transition plan:\nBy 2028: Identify cryptographic services needing upgrades and develop migration plans.\n2028–2031: Prioritize upgrading critical systems and adapt according to PQC progress.\n2031–2035: Complete full migration of systems and products to PQC.\nThe UK plan explicitly bases on NIST standards and aligns with IETF protocol standardization efforts.\nEuropean Union The EU drives its PQC roadmap through the European Telecommunications Standards Institute (ETSI) Quantum-Safe Cryptography (QSC) working group, established in 2015:\nPublished quantum-safe migration guidelines [ETS20] and key establishment standards [ETS25];\nProvides technical reports endorsing NIST standards as well.\nChina China pursues a more independent and autonomous PQC development path.\nIn February 2025, the Commercial Cryptography Institute launched the “Next Generation Commercial Cryptography Algorithms Program” (NGCC) 1, developing domestic PQC algorithms aligned with national security requirements.\nChina emphasizes technological self-reliance and has not publicly released a detailed transition timeline.\nIndustry Progress in Post-Quantum Cryptography (PQC) Several well-known companies relying on cryptography in their products or services have already started migrating to post-quantum cryptography (PQC) solutions.\nCloudflare Date: October 2022\nCloudflare announced that all websites and APIs served through Cloudflare support a post-quantum hybrid key exchange mechanism.\nTechnical Details: This mechanism combines classical elliptic curve cryptography (ECC) with the post-quantum Kyber algorithm to form a hybrid scheme.\nSecurity Advantage: Attackers must break both ECC and Kyber algorithms simultaneously to compromise keys, significantly enhancing security.\nGoogle Date: August 2023\nGoogle announced that its Chrome browser started supporting a post-quantum hybrid key generation mechanism similar to Cloudflare’s.\nMotivation and Background: Google emphasized the urgency of updating TLS (Transport Layer Security) to use quantum-resistant session keys to prevent “Harvest Now, Decrypt Later” attacks, i.e.,\nAttackers collect encrypted data now and decrypt it later once universal quantum computers become available.\nSignal Date: September 2023\nSignal, a leading privacy-focused encrypted messaging app, announced adding post-quantum security layers to its communication encryption.\nImplementation: Introduced CRYSTALS-Kyber post-quantum key encapsulation mechanism (KEM) atop its original ECC-based encryption to enhance security.\nApple Date: February 2024\nApple announced a new encryption protocol PQ3 designed for iMessage.\nPQ3 Features:\nUses PQC for initial key agreement and ongoing message exchange;\nSupports automatic recovery mechanisms, enabling rapid restoration of security even if a key is compromised, thereby enhancing overall resilience.\nIndustry Trend Summary With PQC algorithms continuously standardized and maturing, an increasing number of companies will deploy PQC mechanisms, marking a shift from “experimental defense” to “mainstream security requirement.”\nWhy Choose a “Hybrid Encryption” Strategy? No single PQC algorithm is yet proven 100% secure;\nHybrid schemes allow a gradual transition without interrupting existing security guarantees;\nThey prevent a “single point of failure” — even if one algorithm is broken in the future, the other may still provide protection.\nWhy Kyber? Kyber is the first post-quantum key encapsulation algorithm (KEM) standardized by NIST;\nBased on lattice cryptography, currently regarded as highly resilient to quantum attacks;\nHas relatively small key and ciphertext sizes, making it suitable for performance-sensitive applications such as TLS and mobile communications.\nDrivers for Industry Action Clear government timelines (e.g., NIST’s 2030/2035 plans) create regulatory pressure;\nQuantum threats may not be immediate, but data harvesting attacks are already ongoing, requiring early preparation to protect future privacy;\nCompanies gain a first-mover advantage in security and can strengthen market trust.\nPhilosophical Debate If a universal, breakable quantum computer eventually becomes reality, the Bitcoin community will face a difficult decision:\nShould it take action to handle currently exposed public-key UTXOs that are vulnerable to quantum attacks?\nThis question fundamentally touches Bitcoin’s core values, such as property rights, censorship resistance, forward compatibility, and conservatism.\nThe “Burn” Position Core Arguments By “burning” quantum-vulnerable UTXOs, these coins become unusable, thereby protecting property rights and network integrity.\nBitcoin developer Jameson Lopp2 , in his article [“Against Allowing Quantum Recovery of Bitcoin” (Lop25a)] and BDML talks, argues:\nAllowing quantum computers to seize these bitcoins results in a wealth transfer—from those who lost keysto winners of the quantum arms race.\nThe “burn” strategy treats quantum vulnerability as a protocol-level bug that should be conservatively fixed, like previous vulnerabilities.\nEconomic Effects “Burning” reduces total Bitcoin supply, thereby increasing the value of remaining coins.\nCoordinated “burn” events can reduce market uncertainty and volatility.\nThe “Steal” Position Core Arguments Opponents of “burning” argue:\nIt violates user property rights and amounts to confiscation.\nBitcoin’s design intent is complete user sovereignty over their assets, allowing spending at any time.\nSome users may be unaware of quantum risks or unable to migrate assets promptly.\nMoral and Technical Dilemmas Protocol changes that render some UTXOs permanently unspendable introduce third-party control, violating Bitcoin’s decentralization principles.\nWithout intervention, the “steal” path becomes the default option—once quantum computers mature, early attackers could seize all vulnerable assets, causing serious wealth redistribution.\nMigration Pathways Overview Migrating Bitcoin to quantum-resistant signatures is one of the most historically significant undertakings in the Bitcoin ecosystem. Even if a quantum-safe signature scheme gains technical consensus and integrates into Bitcoin Core, migrating millions of UTXOs to quantum-safe scripts requires unprecedented network coordination. This section discusses practical migration pathways, on-chain resource requirements, activation methods, user education, and coordination mechanisms3.\nQuantum-resistant migration is not only a technical challenge but also a network-wide collaboration and governance issue.\nIt involves block space, soft forks, user key management, activation mechanisms, and more.\nAll Bitcoin holders need basic awareness of migration pathways to develop effective risk mitigation strategies against quantum threats.\nUTXO Migration As of May 2025, Bitcoin’s UTXO set contains approximately 190 million UTXOs. Only a portion exposes public keys and is thus vulnerable to long-range attacks. However, all UTXOs are potentially susceptible to short-range attacksduring spending and transaction confirmation windows, as described earlier in “On-Spend Vulnerable Script Types.”\nIdeally, all UTXOs should migrate to quantum-resistant scripts by spending old UTXOs in a transaction and creating new UTXOs secured by post-quantum signatures. In practice, some UTXOs are inaccessible (e.g., lost keys), creating the “philosophical dilemma: burn or steal.”\n2024 research estimates:\nTheoretical fastest migration time: Using 100% block space for migration transactions, migrating all UTXOs would take approximately 76–142 days (model-dependent).\nRealistic scenario: Using only 25% block space, it would take 305–568 days.\nIn reality, only an urgent migration of exposed public-key UTXOs is necessary.\nKey points:\nUTXO migration is resource-intensive and costly.\nWithout on-chain scaling, existing block size and block interval severely limit migration throughput.\nSignature aggregation and batching transactions are potential optimizations.\nMigration Mechanisms Several proposed mechanisms balance user participation, consensus rule changes, and technical complexity.\nCommit-Delay-Reveal Protocol (CDR) and Variants Background:\nInitially proposed in 2018, with variants by Adam Back, Tim Ruffing, and others.\nDesigned to securely migrate old UTXOs to post-quantum scripts even after ECC compromise.\nThree-phase process:\nCommit phase:\nUser creates a commitment transaction hashing the old ECC and new post-quantum public keys.\nThe hash is committed on-chain via OP_RETURN.\nTransaction valid under current consensus rules.\nSoft fork needed to enforce that the UTXO can only be spent by knowledge of both private keys.\nDelay phase:\nIntroduces a mandatory delay period (e.g., 6 months) to prevent reorg attacks by ensuring chain finality.\nFunds are locked and unusable during this period.\nReveal phase:\nUser submits a reveal transaction disclosing the original ECC and new post-quantum keys.\nThe transaction is signed with the post-quantum signature.\nVerifies that the reveal matches the commit hash.\nAdvantages:\nHigh security under CRQC presence; allows safe migration of unexposed UTXOs post-ECC compromise.\nSupports migration after quantum attacks occur.\nDisadvantages:\nCannot migrate UTXOs with already exposed public keys (e.g., P2PK).\nRequires user participation in two transactions.\nRequires users to hold post-quantum coins for fees, creating a bootstrapping problem.\nAt least one, more likely two soft forks needed (one for PQ signatures, one for CDR semantics).\nBlock space constraints remain.\nImproved Scheme: Lifted FawkesCoin Protocol Improvements:\nResolves the “user must already hold PQ coins” problem in original CDR.\nEmploys zero-knowledge proofs (ZKPs) using the PICNIC PQ signature to prove knowledge of a private key without revealing the public key.\nCircumvents the paradox of needing to reveal a public key to commit.\nAdditional capabilities:\nHD wallet users can prove derivation relationships using seeds.\nPartial private key loss is recoverable via master seed.\nCan protect the majority of Bitcoin accumulated since HD wallets became standard.\nCosts and challenges:\nRequires integrating ZKPs into Bitcoin protocol, a more disruptive change than single PQ signature adoption.\nPICNIC and related ZK systems are complex and not yet widely deployed.\nCommentary \u0026amp; Opinion Bitcoin’s quantum-resistant migration is a gradual but urgent process:\nComprehensiveness vs Reality:\nIdeally migrate all UTXOs.\nPractically prioritize active, spendable, high-risk UTXOs.\nInaccessible funds (lost keys) remain a “philosophical problem.”\nTechnical vs Social Coordination:\nTechnical solutions (CDR, ZKP lifting) are early-stage and require extensive validation.\nReal challenges lie in activation, education, consensus coordination, and incentive design.\nFuture research directions:\nDesign quantum-resistant coin creation mechanisms to solve bootstrapping.\nSimplify CDR workflows (e.g., automated wallet support).\nExplore soft-fork-free or incremental transition mechanisms.\nAnalyze Grover’s algorithm effectiveness on various Bitcoin script structures to inform prioritization.\nReferences Institute of Commercial Cryptography Standards, China. Announcement on Launching the Next-generation Commercial Cryptographic Algorithms Program (NGCC). February 5, 2025.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nJ. Lopp. Against Allowing Quantum Recovery of Bitcoin. Cypherpunk Cogitations. March 16, 2025.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nChaincode Labs. Bitcoin and Quantum Computing: Current Status and Future Directions.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://sidereushu.github.io/posts/post-quantum-readiness-in-blockchain---threats-roadmaps-and-migration-strategy-iii/","summary":"\u003cbr\u003e\n\u003ch2 id=\"timeline-for-post-quantum-migration\"\u003eTimeline for Post-Quantum Migration\u003c/h2\u003e\n\u003cp\u003eAccording to analysis by Chaincode Labs, Bitcoin’s transition to post-quantum cryptography (PQC) can follow two main strategies: a \u003cstrong\u003eshort-term contingency plan\u003c/strong\u003e (cf. Figure 1) and a \u003cstrong\u003elong-term comprehensive path\u003c/strong\u003e (cf. Figure 2).\u003c/p\u003e\n\u003cp\u003eThe \u003cstrong\u003eshort-term strategy\u003c/strong\u003e focuses on deploying a basic quantum-resistant option within \u003cstrong\u003e1 to 2 years\u003c/strong\u003e, offering a fallback mechanism in case cryptographically relevant quantum computers (CRQCs) emerge sooner than expected. This involves proposing a minimal PQC signature scheme through a BIP, implementing it in Bitcoin Core, and enabling voluntary migration of vulnerable UTXOs. While not optimized for all use cases, it provides immediate protection for at-risk users and critical institutions. Success depends on close coordination across the technical community and early involvement from major Bitcoin holders.\u003c/p\u003e","title":"Post-Quantum Readiness in Blockchain: Threats, Roadmaps, and Migration Strategy III"},{"content":"\nPost-Quantum Cryptography (PQC) Post-Quantum Cryptography has become a critical solution to counter the threat posed by scalable, controllable quantum computers to current cryptographic systems.\nUrgency of PQC: Originates from Peter Shor’s 1995 algorithm, which can factor integers and compute discrete logarithms in polynomial time, effectively breaking mainstream schemes like RSA, DH, and ECC.\nPQC is not a single algorithm, but a set of parallel technical approaches, including:\nLattice-based cryptography: The most promising category with well-established theoretical foundations;\nHash-based signatures: Such as SPHINCS+, offering strong security but large signature sizes;\nCode-based cryptography: Examples include BIKE and McEliece, characterized by very large keys;\nIsogeny-based cryptography: Such as SIKE, featuring compact structures but recently subject to severe cryptanalytic attacks;\nMultivariate polynomial cryptography: Like Rainbow, which is vulnerable to algebraic attacks.\nThese schemes differ greatly in practical terms, especially regarding:\nKey and signature sizes;\nSigning and verification efficiency;\nMaturity of design and security evaluation;\nSensitivity to block size and verification efficiency in applications like Bitcoin. PQC Algorithm Analysis in the Bitcoin Context To evaluate the suitability of PQC algorithms for Bitcoin, we compare their public key/signature sizes and computational efficiency against the current ECDSA and Schnorr signature schemes[^ChainCodeReport].\nBased on data aggregated from BIP-360 1and the PQ Signatures Zoo, the summary is as follows:\nHash-based signatures (e.g., SPHINCS+): Very small public keys but the largest signatures;\nLattice-based signatures (e.g., FALCON and CRYSTALS-Dilithium): Achieve a good balance between public key and signature sizes;\nIsogeny-based signatures (e.g., SQIsign): Smallest signatures and relatively small public keys, but slow and not yet widely vetted.\nThe overall trend indicates PQC technologies are still evolving, with room for improvement in signature size, public key size, and signing/verification efficiency.\nTherefore, locking in a specific scheme too early is suboptimal; a more rational strategy is to monitor developments continuously and maintain compatibility in design.\nMetric ECC (Schnorr) SPHINCS+ CRYSTALS-Dilithium FALCON SQIsign Signature Size ~64 bytes ~8–17 KB ~2–3 KB ~0.5–1 KB ~100–300 bytes Public Key Size ~32 bytes ~32 bytes ~1.3 KB ~800 bytes ~200 bytes Signing Time Fast Slow Medium Fast Slow Security Maturity High High High Medium Low Suitable for Bitcoin? ✅ ❌ ⚠️ ✅ ❌ NIST Post-Quantum Standardization Process To address the long-term threat posed by quantum computing to existing cryptographic standards, NIST launched the Post-Quantum Cryptography (PQC) standardization process in December 2016. This initiative builds on foundational research from PQCrypto conferences (since 2006) and multiple projects in the EU, Japan (e.g., SAFEcrypto, CREST) 2.\nThe process is globally open, collecting algorithm submissions, conducting public testing and academic review, with the goal to publish algorithms resistant to both classical and quantum attacks. As of March 2025, NIST has officially released the following standards:\nFIPS 203: Encryption algorithm standards (non-signature);\nFIPS 204: Module Lattice Signature Standard (ML-DSA) based on CRYSTALS-Dilithium;\nFIPS 205: Stateless Hash-Based Signature Standard (SLH-DSA) based on SPHINCS+;\nFIPS 206: NTRU-based Fast Fourier Lattice Signature (FN-DSA) based on FALCON, draft expected in 2025.\nAdditionally, another candidate, the LPN-based HQC key encapsulation algorithm, is expected to complete review by 2026 before draft release NIS25.\nNIST’s Main Signature Schemes (Current Status) FIPS # Name Underlying Algorithm Features Suitable for Bitcoin? FIPS 204 ML-DSA Dilithium Secure, balanced, easy to deploy ✅ FIPS 205 SLH-DSA SPHINCS+ Extremely secure but bulky ❌ FIPS 206 FN-DSA FALCON Compact, efficient but numerically unstable ✅ (requires floating-point stability improvements) Dilithium is currently the most balanced scheme and is widely adopted by Google and Cloudflare;\nFALCON is attractive for Bitcoin due to its compactness and speed, but its floating-point implementation complexity introduces side-channel risks;\nSPHINCS+ is mainly used for ultra-high security scenarios (e.g., backup signatures, critical firmware signing), and is unsuitable for high-frequency signing.\nReferences H. Beast. BIP-360: QuBit - Pay to Quantum Resistant Hash. Bitcoin Improvement Proposal (BIP) Pull Request 1670. September 2024.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nChaincode Labs. Bitcoin and Quantum Computing: Current Status and Future Directions.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://sidereushu.github.io/posts/post-quantum-readiness-in-blockchain---threats-roadmaps-and-migration-strategy-ii/","summary":"\u003cbr\u003e\n\u003ch2 id=\"post-quantum-cryptography-pqc\"\u003ePost-Quantum Cryptography (PQC)\u003c/h2\u003e\n\u003cp\u003ePost-Quantum Cryptography has become a critical solution to counter the threat posed by scalable, controllable quantum computers to current cryptographic systems.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eUrgency of PQC\u003c/strong\u003e: Originates from \u003cstrong\u003ePeter Shor’s 1995 algorithm\u003c/strong\u003e, which can factor integers and compute discrete logarithms in polynomial time, effectively breaking mainstream schemes like RSA, DH, and ECC.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003ePQC is \u003cstrong\u003enot a single algorithm\u003c/strong\u003e, but a set of \u003cstrong\u003eparallel technical approaches\u003c/strong\u003e, including:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eLattice-based cryptography\u003c/strong\u003e: The most promising category with well-established theoretical foundations;\u003c/p\u003e","title":"Post-Quantum Readiness in Blockchain: Threats, Roadmaps, and Migration Strategy II"},{"content":"\nBitcoin\u0026rsquo;s Security and the Threat from Quantum Computing Bitcoin\u0026rsquo;s security relies on a cryptographic assumption that has long been considered unbreakable under current technological conditions. However, the emergence of quantum computers could undermine this assumption within the next decade.\nAt the core of Bitcoin’s cryptographic foundation are:\nthe Elliptic Curve Digital Signature Algorithm (ECDSA);\nand, since 2021, the introduction of Schnorr signatures.\nBoth schemes are based on the Elliptic Curve Discrete Logarithm Problem (ECDLP), which is asymmetric in nature: deriving the public key from a private key is easy, but reversing the process is believed to require trillions of years even on today’s most powerful supercomputers.\nHowever, in the face of cryptographically relevant quantum computers (CRQCs), this asymmetry may collapse — deriving the private key from the public key could take only hours or days.\nInstitutions such as the U.S. National Institute of Standards and Technology (NIST), the U.S. government, and other international agencies have recommended phasing out elliptic curve cryptography by 2030 and fully deprecating it by 2035.\nPotential Impact of Quantum Computers on Bitcoin Once practical quantum computers become available, their impact on Bitcoin could be profound. According to an analysis by 1:\nApproximately 20% to 50% of all circulating Bitcoin (4 to 10 million BTC) could be at risk of theft;\nThis risk primarily stems from the ability to derive private keys from exposed public keys.\nThe most vulnerable assets include:\nUTXOs that use insecure script types;\nFunds associated with “address reuse”, where the public key has already been revealed;\nExchange and institutional addresses that frequently reuse addresses;\nEarly-era addresses from the Satoshi period, such as those using raw public key scripts;\n“Lost coins”, which remain unmoved and are locked under outdated or unsafe scripts.\nAmong these, high-value exchange and institutional accounts are expected to be top targets for quantum-enabled attackers.\nQuantum Threats to Bitcoin Mining and Ecosystem Security On the other hand, while quantum computing may also impact Bitcoin mining, its threat is considered limited in the foreseeable future:\nCurrent mining performance is primarily driven by clock frequency;\nQuantum miners would have to compete with highly optimized ASIC machines;\nEven with a quadratic speedup from algorithms like Grover’s, it would still be difficult to outperform classical miners;\nThe hardware performance gap remains significant.\nIf quantum miners were to dominate in the future, potential risks could include:\nDouble-spending attacks by small or solo miners;\nIncreased centralization due to disproportionate mining power.\nAlthough these scenarios remain distant, exploring post-quantum mining models still holds theoretical value and can help prepare for long-term contingencies.\nBeyond Signatures and Mining: Ecosystem-Level Risks In addition to the direct threats to digital signatures and mining, a broader set of ecosystem-level vulnerabilities must also be considered in the post-quantum era:\nFor example: SSL/TLS communication protocols, hardware wallets, and firmware update mechanisms may all be compromised by quantum-capable adversaries. Two Types of Quantum Attack Vectors (Long-Range Attack vs. Short-Range Attack)\nLong-Range Attack: Targets addresses whose public keys are already exposed on-chain, such as P2PK, P2MS, and P2TR. These addresses are permanently vulnerable, as their public keys are visible at all times.\nShort-Range Attack: Targets addresses that only reveal their public keys when spending, such as P2PKH. In this case, the attacker must act within a very short window — between the time a transaction is broadcast (and the public key is exposed) and when it gets confirmed on the blockchain.\nIn short, if the public key of a UTXO is known, quantum adversaries may derive the private key and steal the funds. The difference lies in how and when the public key is exposed:\nSome addresses are “natively exposed”, meaning their public keys are visible from the beginning, and thus remain under long-term attack risk.\nOthers (e.g., P2PKH) only reveal the public key when the coin is spent, offering a very narrow attack window, also known as a \u0026ldquo;preemptive\u0026rdquo; or \u0026ldquo;racing\u0026rdquo; attack.\nQuantum Vulnerabilities by Script Type The feasibility of quantum attacks depends not only on the existence of CRQCs (Cryptographically Relevant Quantum Computers), but also on whether the corresponding public key is accessible. Therefore, different script types present different levels of vulnerability. Below is a classification by script type:\n🔴 Immediate Vulnerability P2PK (Pay to Public Key): The earliest script type, which directly exposes the ECDSA public key in the locking script. Though it accounts for only ~0.025% of UTXOs, it secures ~8.68% of all BTC (~1.72 million BTC) — largely associated with early Satoshi-era wallets.\nP2MS (Pay to MultiSig): A script type that exposes multiple public keys. Now rarely used and secures a relatively small amount of BTC.\nP2TR (Pay to Taproot): Despite being a modern script, its key-path spending still reveals a tweaked public key, making it vulnerable to quantum attacks. P2TR makes up ~32.5% of UTXOs but secures only ~0.74% of BTC.\n🟡 On-Spend Vulnerable Script Types P2PK, P2MS, and P2TR are classified as “immediately vulnerable” because they expose public keys directly in the output script (scriptPubKey).\nHowever, some script types do not reveal the public key until the coin is spent. These are vulnerable only during the short window between transaction broadcast and confirmation.\n🟠 Address Reuse Vulnerability Script types such as P2PKH, P2SH, P2WPKH, and P2WSH only expose public keys at the time of spending, meaning the key is normally revealed once.\nBut if the same address (i.e., the same public key) is reused, the associated public key remains permanently exposed on-chain, turning a short-range attack surface into a long-range attack vector.\n🟣 Other Avenues for Public Key Exposure 1. Fork Chain Exposure If a user spends an unspent UTXO on a Bitcoin fork chain (e.g., Bitcoin Cash, Bitcoin Gold), the public key becomes exposed on the fork — even though the UTXO remains unspent on the Bitcoin mainnet.\nThus, the corresponding UTXO on the mainnet becomes a long-term quantum target.\n2. Leaked Extended Public Keys (xpubs) xpubs are used in Hierarchical Deterministic (HD) wallets, enabling address generation without revealing private keys.\nIf an xpub is leaked, all non-hardened child public keys derived from it can have their private keys recovered by quantum computers.\n3. Public Key Exposure in Multi-Sig \u0026amp; Lightning Network In multi-signature wallets and Lightning Network channels, participants must share their public keys to construct transactions or payment channels.\nAlthough these public keys are not broadly public, they are shared within limited parties, and thus may be exploited by insiders or malicious actors.\nEcosystem Vulnerabilities Once elliptic curve cryptography (ECC) is broken by quantum computers, the impact will go far beyond Bitcoin — potentially compromising the entire Internet cryptographic infrastructure.\nCore protocols like SSL/TLS could become insecure, leading to ecosystem-level attacks such as man-in-the-middle (MITM) attacks, where adversaries intercept and redirect user requests:\nMITM attacks: By compromising SSL/TLS, attackers can forge connections to exchanges and hijack funds.\nHardware wallet updates: Firmware updates may be tampered with, silently embedding backdoors.\nMining pool impersonation: Attackers may masquerade as miners to infiltrate mining pools.\nDNS attacks: Redirecting users to malicious nodes through DNS manipulation.\nAPI attacks: Tampering with third-party APIs widely used by wallets and exchanges.\nThese threats are stealthier than direct cryptographic breakage, potentially remaining undetected for extended periods due to their ecosystem-level obfuscation.\nBitcoin Hash Functions and Grover’s Algorithm Bitcoin relies on two hash functions: SHA-256 and RIPEMD-160, used for:\nMining (double SHA-256 on block headers)\nGenerating transaction IDs\nSigning messages\nDeriving addresses from public keys (Hash160)\nQuantum Resistance of Hash Functions The fundamental property of hash functions is irreversibility.\nEven the world’s most powerful supercomputers (e.g., El Capitan, as of Nov 2024) would require far more time to break them than to crack ECC. 2\nGrover’s algorithm offers a quadratic speedup for brute-forcing hash functions:\nClassical attack complexity: O(N)O(N)\nQuantum attack complexity: O(N)O(N​)\nFor example, finding a SHA-256 collision classically requires 22562256 operations.\nWith Grover’s algorithm, this is reduced to 21282128, which is still computationally infeasible without extremely powerful quantum computer.\nImpact on Bitcoin Mining The essence of Bitcoin mining is to find a nonce value that makes the block header\u0026rsquo;s hash lower than the network difficulty target.\nThis process involves repeated SHA-256 computations, currently performed almost exclusively by ASIC hardware.\n1. Limitations of Quantum Mining Grover’s algorithm provides sequential, not parallel, speedup, which leads to key limitations:\nIt cannot scale in parallel like classical mining does; adding more quantum hardware does not linearly increasehash power.\nCompeting with ASICs would require a large number of high-speed quantum machines, making deployment extremely costly.\n2. Network Difficulty Adjustment Bitcoin dynamically adjusts its mining difficulty every 2016 blocks to maintain a block interval of ~10 minutes.\nIf quantum mining is introduced, the protocol will increase the difficulty accordingly, which would further offset any advantage from Grover’s algorithm.\nForks and the 51% Attack 1. Quantum-Induced Forking Risk Research shows that quantum mining could increase the frequency of blockchain forks:\nWhen one miner finds a valid block, other quantum miners must decide whether to continue or restart their computation.\nSimultaneous quantum measurements by multiple miners increase the chance that multiple valid blocks are found at the same time, causing forks.\n2. Security Concerns In a high-fork environment:\nHonest miners\u0026rsquo; hash power gets split across multiple chains;\nAdversaries can concentrate their resources on attacking one chain, potentially gaining dominance without owning 51% of total hash power.\nThis reduces finality and consensus stability, and significantly increases the attack surface.\nComprehensive Quantum Vulnerability Assessment of Bitcoin Component Quantum Threat Resistance Level ECC Signatures Critical risk (Shor\u0026rsquo;s algorithm) Very Low Hash Functions Moderate risk (Grover\u0026rsquo;s algorithm) Relatively High Mining Mechanism Efficiency impacted but adjustable Medium Ecosystem Attacks High risk (SSL, DNS, APIs) Depends on external mitigation Fork Security Potentially severe (more frequent forks) Low References Chaincode Labs. Bitcoin and Quantum Computing: Current Status and Future Directions.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nE. Strohmaier, J. Dongarra, H. Simon, H. Meuer. TOP500 List - November 2024 (64th Edition). TOP500.org. November 19, 2024.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://sidereushu.github.io/posts/post-quantum-readiness-in-blockchain---threats-roadmaps-and-migration-strategy-i/","summary":"\u003cbr\u003e\n\u003ch3 id=\"bitcoins-security-and-the-threat-from-quantum-computing\"\u003eBitcoin\u0026rsquo;s Security and the Threat from Quantum Computing\u003c/h3\u003e\n\u003cp\u003eBitcoin\u0026rsquo;s security relies on a \u003cstrong\u003ecryptographic assumption that has long been considered unbreakable under current technological conditions\u003c/strong\u003e. However, the emergence of quantum computers \u003cstrong\u003ecould undermine this assumption within the next decade\u003c/strong\u003e.\u003c/p\u003e\n\u003cp\u003eAt the core of Bitcoin’s cryptographic foundation are:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003ethe \u003cstrong\u003eElliptic Curve Digital Signature Algorithm (ECDSA)\u003c/strong\u003e;\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eand, since 2021, the introduction of \u003cstrong\u003eSchnorr signatures\u003c/strong\u003e.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eBoth schemes are based on the \u003cstrong\u003eElliptic Curve Discrete Logarithm Problem (ECDLP)\u003c/strong\u003e, which is \u003cstrong\u003easymmetric\u003c/strong\u003e in nature: deriving the public key from a private key is easy, but reversing the process is believed to require trillions of years even on today’s most powerful supercomputers.\u003cbr\u003e\nHowever, in the face of \u003cstrong\u003ecryptographically relevant quantum computers (CRQCs)\u003c/strong\u003e, this asymmetry may collapse — \u003cstrong\u003ederiving the private key from the public key could take only hours or days\u003c/strong\u003e.\u003c/p\u003e","title":"Post-Quantum Readiness in Blockchain: Threats, Roadmaps, and Migration Strategy I"},{"content":"\n❓What is Quantum Computing? Quantum computing harnesses the quantum states of microscopic particles—such as photons, electrons, or atoms—to process information. It is fundamentally based on several key principles: superposition, entanglement, coherence, and the no-cloning theorem.\nAs early as 1982, Professor Richard Feynman famously stated:\n\u0026ldquo;Nature isn’t classical, damn it, and if you want to make a simulation of nature, you’d better make it quantum mechanical.\u0026rdquo;\nThis insight laid the conceptual foundation for building computational tools using quantum systems themselves, rather than relying on classical approximations of quantum behavior.\nWhy Classical Computers Are Not Enough Many fundamental systems in nature do not follow classical physics but are governed by quantum mechanics. Therefore, if we want to “simulate nature,” classical computation is not sufficient — we must use quantum systems themselves to build computational tools.\nMore specifically, simulating quantum systems with classical computers faces exponential difficulty. A quantum system composed of $n$ particles requires $2^n$ complex numbers to describe its state, which quickly exceeds the processing capability of classical machines. Furthermore, particles in a quantum system are often not independent — they form complex correlations known as quantum entanglement.\nQuantum computing, on the other hand, leverages the superposition of quantum states to perform parallel computation. For example, a 4-qubit quantum register can represent all 16 values from 0 to 15 simultaneously, and any operation on the register acts on all 16 values at once.\nThis is why Feynman proposed a groundbreaking idea: instead of simulating quantum systems with classical computers, we should use quantum systems themselves to compute. His insight was remarkably accurate — as we now know, there are computations that can only be carried out through quantum experiments and are beyond the reach of traditional computers.\nWhat Does Quantum Computing Mean for Mainstream Cryptosystems Like RSA and ECC? In 1994, Peter Shor proposed a quantum algorithm—Shor\u0026rsquo;s Algorithm—that can break mainstream cryptosystems such as RSA and ECC, drawing widespread attention. The foundation of Shor\u0026rsquo;s Algorithm is its ability to perform fast Fourier transforms (FFT) on state vectors of exponential size, making it easier to identify the period of a function.\nQuantum computing leverages the superposition effect of quantum states, allowing parallel computation across all possible states. For example, Schrödinger’s cat can be simultaneously ‘alive’ and ‘dead’—a superposition of two states. If there are 100 cats, each in a superposition of ‘alive’ and ‘dead’ there are ($2^{100}$) possible states. Each state has its own amplitude, positive or negative, and quantum computing can manipulate this exponentially large vector of states simultaneously.\nOn the surface, this exponential parallelism might seem to allow brute-force solutions to any computationally hard problem. However, the output of a quantum computation is itself a superposition, making it difficult to extract a definitive answer.\nCurrently, only Shor’s Algorithm can use quantum Fourier transforms to efficiently extract the period of a function, thereby solving the integer factorization and discrete logarithm problems upon which modern public-key cryptosystems RSA and ECC are based. For other hard problems that cannot be reduced to period finding, no polynomial-time quantum algorithms are known yet.\nHow Close Are We to Practical Quantum Computers? Yes and no. To break a 1024-bit RSA key, a universal quantum computer with millions of qubits would be needed—far beyond our current manufacturing capabilities (the largest quantum computers today have only a few hundred qubits):\nIn 2017, IBM introduced the world’s first 16-qubit quantum computer. The latest fault-tolerant universal quantum machines, such as Google’s Willow chip (105 qubits by the end of 2024), now achieve error correction threshold capabilities. The Oxford team has also created single-qubit operations with gate error rates as low as 0.000015%. Developing public-key cryptographic algorithms that can resist quantum attacks (post-quantum cryptography) has become an urgent task. To address this, the U.S. National Institute of Standards and Technology (NIST) launched the Post-Quantum Cryptography (PQC) standardization project back in 2016. After years of evaluation, the leading quantum-resistant public-key algorithms—such as CRYSTALS-Kyber, Dilithium, and Falcon—are now essentially standardized and are set to become the new benchmarks for federal and enterprise security infrastructure.\nD-Wave’s Progress The Canadian quantum computing company D-Wave has long provided quantum annealers, which are well-suited for solving optimization problems, rather than problems like period-finding as required by Shor’s algorithm.\nIn 2025, D-Wave released its Advantage2 system, featuring over 4,400+ qubits and a 20-node connectivity topology. It is currently offering cloud-based services to clients such as Mastercard, NTT, and the Jülich Supercomputing Centre.\nMeanwhile, D-Wave’s team published a paper in Science claiming “quantum advantage” for the first time: in simulating complex magnetic systems, their machine outperformed the world’s top classical supercomputers by what they claim would be millions of years.\nAlthough this result has sparked debates over classical simulators (such as MPS and t-VMC), it still suggests that quantum annealing is showing practical value in certain scientific and engineering domains.\nD-Wave has built and sold several quantum computers; however, these machines cannot run Shor’s algorithm and therefore do not pose a direct threat to cryptographic schemes based on factoring or discrete logarithms.\nStrictly speaking, D-Wave builds quantum annealers, which are machines that can only run quantum annealing algorithms. Their fundamental working principle differs from that of universal quantum computers, which rely on quantum superposition to achieve powerful computational capabilities. In contrast, quantum annealers rely on quantum tunneling to simulate the annealing process.\nCurrently, there is research underway that attempts to transform integer factorization into an annealing-compatible problem and solve it using D-Wave machines — but so far, it has only managed to factor integers up to 90 bits.\nEmerging Track: Topological Quantum Computing and Fault-Tolerance February 2025: Microsoft launched its first Majorana-based topological quantum processor, Majorana 1, featuring highly stable topological qubits and aiming for million-qubit scalability.\nMay 2025: Quantinuum announced that its H2 system had achieved a record-breaking quantum volume $2^{23}=8388608$, a key metric for assessing a system’s practical computational power.\nMay 2025: Oxford, Google’s Willow, and other teams continued advancing toward fault-tolerant quantum computing by optimizing single-qubit gate fidelities and improving two-qubit error correction schemes.\n","permalink":"https://sidereushu.github.io/posts/notes---what-is-quantum-computing-implications-for-rsa--ecc/","summary":"\u003cbr\u003e\n\u003ch3 id=\"what-is-quantum-computing\"\u003e❓What is Quantum Computing?\u003c/h3\u003e\n\u003cp\u003eQuantum computing harnesses the quantum states of microscopic particles—such as photons, electrons, or atoms—to process information. It is fundamentally based on several key principles: \u003cstrong\u003esuperposition\u003c/strong\u003e, \u003cstrong\u003eentanglement\u003c/strong\u003e, \u003cstrong\u003ecoherence\u003c/strong\u003e, and the \u003cstrong\u003eno-cloning theorem\u003c/strong\u003e.\u003c/p\u003e\n\u003cp\u003eAs early as \u003cstrong\u003e1982\u003c/strong\u003e, \u003cstrong\u003eProfessor Richard Feynman\u003c/strong\u003e famously stated:\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003e\u003cem\u003e\u0026ldquo;Nature isn’t classical, damn it, and if you want to make a simulation of nature, you’d better make it quantum mechanical.\u0026rdquo;\u003c/em\u003e\u003c/p\u003e\u003c/blockquote\u003e\n\u003cp\u003e\u003cimg alt=\"Feynman\" loading=\"lazy\" src=\"/images/screenshot-Feynman.png\"\u003e\u003c/p\u003e\n\u003cp\u003eThis insight laid the conceptual foundation for building computational tools using quantum systems themselves, rather than relying on classical approximations of quantum behavior.\u003c/p\u003e","title":"Notes | What is Quantum Computing? Implications for RSA \u0026 ECC"},{"content":"Throughout the evolution of human civilization, the pursuit of computational power has never ceased. From $\\underline{mechanical ~computation}$ to $\\underline{electronic ~computation}$, we now stand at the dawn of a new era — $\\mathbf{on ~the ~verge ~of ~quantum ~computing. }$\nThe core breakthrough of quantum computing does not lie in macro-level advancements such as “faster” transistors or “stronger” chips, but rather in a paradigm shift from macro to micro: it no longer relies on the classical binary of 0 and 1 but leverages the peculiar properties of quantum superposition and entanglement at the particle level to build $\\underline{an ~entirely ~new ~computational ~logic}$. This logic cannot be simulated by traditional electronic computers, much like nuclear weapons cannot be replicated with chemical explosives—their energies are simply not on the same scale.\nIn this age of exponentially increasing computational power, modern cryptography stands as the first field to be fundamentally challenged. Encryption algorithms that are rock-solid on classical computers—such as RSA and ECC—become vulnerable in the face of quantum algorithms like Shor’s algorithm.\nJust as electronic computers once marked the end of the mechanical cipher era, quantum computing is now poised to reshape the foundations of modern cryptography. It brings not only challenges, but also an opportunity to redefine what “security” truly means.\n","permalink":"https://sidereushu.github.io/posts/from-macro-to-micro---a-computational-revolution-reshaping-cryptography/","summary":"\u003cp\u003eThroughout the evolution of human civilization, the pursuit of computational power has never ceased. From $\\underline{mechanical ~computation}$ to $\\underline{electronic ~computation}$, we now stand at the dawn of a new era — $\\mathbf{on ~the ~verge ~of ~quantum ~computing. }$\u003c/p\u003e\n\u003cp\u003eThe core breakthrough of quantum computing does not lie in macro-level advancements such as “faster” transistors or “stronger” chips, but rather in \u003cstrong\u003ea paradigm shift from macro to micro\u003c/strong\u003e: it no longer relies on the classical binary of 0 and 1 but leverages the peculiar properties of quantum superposition and entanglement at the particle level to build $\\underline{an ~entirely ~new ~computational ~logic}$. This logic cannot be simulated by traditional electronic computers, much like nuclear weapons cannot be replicated with chemical explosives—their energies are simply not on the same scale.\u003c/p\u003e","title":"From Macro to Micro - A Computational Revolution Reshaping Cryptography"},{"content":" Dr. Sidereus M. Hu A crypto PhD building the future of trust. Research interests include: Post-Quantum Cryptography, Privacy-Preserving Computation, and Provable Security.\nFormer Founder, Crypto \u0026 InfoSec Team (H7US3C)\nHonors: SJTU Top 10 Outstanding Students of the Year Academic Star Award Outstanding Dissertation Award 📧 crypto.sidereus@gmail.com 📧 mxhu@teamtps.org ","permalink":"https://sidereushu.github.io/about/","summary":"\u003cstyle\u003e\n.card {\n  max-width: 960px;\n  margin: 2rem auto;\n  padding: 2rem;\n  display: flex;\n  align-items: flex-start;\n  gap: 2.5rem;\n  background: #f9f9f9;\n  border-radius: 16px;\n  box-shadow: 0 4px 24px rgba(0, 0, 0, 0.08);\n  font-family: -apple-system, BlinkMacSystemFont, \"Helvetica Neue\", \"PingFang SC\", \"Microsoft YaHei\", sans-serif;\n}\n.card img {\n  width: 320px;\n  height: auto;\n  object-fit: cover;\n  border-radius: 12px;\n  box-shadow: 0 4px 12px rgba(0, 0, 0, 0.1);\n  margin: 0;\n  display: block;\n}\n.card-content {\n  flex: 1;\n  margin: 0;\n  padding: 0;\n  display: flex;\n  flex-direction: column;\n  justify-content: space-between; /* 邮箱与图片底部对齐关键 */\n  min-height: 100%; /* 保证内容高度撑满父容器 */\n}\n.card-content-inner {\n  display: flex;\n  flex-direction: column;\n}\n.card-content h1 {\n  font-size: 2rem;\n  margin-top: 0;\n  margin-bottom: 0.4rem;\n  word-break: keep-all;\n  text-align: left;\n  white-space: pre;         /* 空格生效，确保左对齐 */\n  font-weight: bold;\n  padding-left: 0;\n}\n.card-content p {\n  margin: 0.35rem 0;\n  font-size: 1.05rem;\n  color: #333;\n  line-height: 1.75;\n}\n.card-content p.intro {\n  margin-bottom: 0.12rem;\n}\n.card-content .founder {\n  margin-bottom: 1.8rem;   /* 增加与Honors间距，邮箱能到底部 */\n}\n.card-content a {\n  color: #1a73e8;\n  text-decoration: none;\n}\n.honor-list {\n  list-style: disc;\n  padding-left: 1.25em;\n  margin: 0.2em 0 1em 0;\n}\n.honor-list li {\n  margin: 0.12em 0;\n  padding-left: 0;\n  text-indent: 0;\n  line-height: 1.3;\n}\n.email-list {\n  display: flex;\n  flex-direction: column;\n  gap: 0.13em;\n  margin-top: 0.18em;\n}\n.email-list a {\n  font-size: 1.03rem;\n  text-decoration: none;    /* 去掉下划线 */\n}\n\u003c/style\u003e\n\u003cdiv class=\"card\"\u003e\n  \u003cimg src=\"/images/me.jpg\" alt=\"我的头像\"\u003e\n  \u003cdiv class=\"card-content\"\u003e\n    \u003cdiv class=\"card-content-inner\"\u003e\n      \u003ch1\u003eDr. Sidereus M. Hu        \u003c/h1\u003e\n      \u003cp class=\"intro\"\u003eA crypto PhD building the future of trust. Research interests include: Post-Quantum Cryptography, Privacy-Preserving Computation, and Provable Security.\u003c/p\u003e","title":"About Sidereus"},{"content":"","permalink":"https://sidereushu.github.io/posts/images/short-term/","summary":"","title":""}]